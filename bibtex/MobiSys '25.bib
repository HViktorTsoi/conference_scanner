@inbook{10.1145/3711875.3729135,
author = {Li, Zhiying and Aziz, Abdul and Do, Patrick Phuoc and Nguyen, Phuc and Li, Tianxing},
title = {MobiChem: A Ubiquitous Smartphone-Based Toolkit for Practical Fruit Monitoring and Analysis},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3729135},
abstract = {This paper introduces MobiChem, a low-cost, portable, practical, and ubiquitous smartphone-based toolkit for fruit monitoring. The key idea is to leverage the light emitted from a smartphone's screen and front camera, coupled with a custom-built screen cover, to perform comprehensive hyperspectral analysis on targeted objects. Specifically, we designed a zero-powered screen cover that selectively filters wavelengths essential for hyperspectral sensing. We then incorporate a CNN-based algorithm and a novel ranking-based learning technique that manipulates the latent space to classify maturity stages and characterize their chemical and physical factors. To demonstrate MobiChem's feasibility, robustness, and practicality, we showcase its application in tomato, banana, and avocado sensing. Our system examines the maturity, chlorophyll, lycopene content, free sugar levels, and firmness, enabling various dietary assessments and food safety applications. Experimental results using 117 tomatoes, 98 bananas, and 73 avocados show MobiChem achieved 95.67\% accuracy in chlorophyll concentration measurement, 98.76\% for lycopene detection, 93.53\% for sugar concentrations analysis, and 91.34\% average accuracy in classifying maturity (96.64\% for tomato, 86.37\% for banana, and 91.03\% for avocado).},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {1–14},
numpages = {14}
}
@inbook{10.1145/3711875.3729124,
author = {Chen, Kaixin and Xiang, Junfan and Tan, Wanying and Chen, Keyu and Luo, Yaqiong and Ma, Chang and Wu, Kaishun and Wang, Lu},
title = {PIT: A Novel Toothbrush Providing Real-Time and Robust Plaque Indication During Brushing},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3729124},
abstract = {The dental plaque disclosing agent helps guide plaque removal by revealing plaque on the teeth. However, existing toothbrushes cannot provide visualization of the stained plaque beneath the toothpaste foam during brushing. To fill this gap, this paper proposes PIT and uses smartphone to provide real-time plaque visualization during brushing. PIT introduces novel hardware, including a micro camera, four green LEDs, and a mechanical structure, offering a stable camera view of bristle position and bristle rotation. To address the challenge of toothpaste foam obstruction, we derived an optical channel model that guided the design of the light source to enhance plaque visibility. Furthermore, we designed a deep neural network specifically for plaque segmentation under thick foam. Finally, the trained distilled student model was run on a smartphone and evaluated on both denture models and human subjects. The results show that PIT achieved an IoU (Intersection over Union) of 75.22\% and a latency of 29 ms, with robust performance across various conditions. Evaluation by 10 participants revealed that PIT helped reduce plaque coverage to 5.6\% within two minutes of brushing, significantly outperforming existing advanced toothbrushes.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {15–27},
numpages = {13}
}
@inbook{10.1145/3711875.3729159,
author = {Peng, Jinbo and Duan, Junwen and Lin, Zheng and Yuan, Haoxuan and Gao, Yue and Chen, Zhe},
title = {SigChord: Sniffing Wide Non-sparse Multiband Signals for Terrestrial and Non-terrestrial Networks},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3729159},
abstract = {While unencrypted information inspection in physical layer (e.g., open headers) can provide deep insights for optimizing wireless networks, the state-of-the-art (SOTA) methods heavily depend on full sampling rate (a.k.a Nyquist rate), and high-cost radios, due to terrestrial and non-terrestrial networks densely occupying multiple bands across large bandwidth (e.g., from 4G/5G at 0.4–7 GHz to LEO satellite at 4–40 GHz). To this end, we present SigChord, an efficient physical layer inspection system built on low-cost and sub-Nyquist sampling radios. We first design a deep and rule-based interleaving algorithm based on Transformer network to perform spectrum sensing and signal recovery under sub-Nyquist sampling rate, and second, cascade protocol identifier and decoder based on Transformer neural networks to help physical layer packets analysis. We implement SigChord using software-defined radio platforms, and extensively evaluate it on over-the-air terrestrial and non-terrestrial wireless signals. The experiments demonstrate that SigChord delivers over 99\% accuracy in detecting and decoding, while still decreasing 34\% sampling rate, compared with the SOTA approaches.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {28–41},
numpages = {14}
}
@inbook{10.1145/3711875.3729144,
author = {Xu, Jingao and Bala, Mihir and Eiszler, Thomas and Chen, Xiangliang and Dong, Qifei and Chanana, Aditya and Pillai, Padmanabhan and Satyanarayanan, Mahadev},
title = {TerraSLAM: Towards GPS-Denied Localization},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3729144},
abstract = {A long-standing concern with GPS-based location sensing is its vulnerability to satellite signal loss. This may arise from adversarial attacks or natural causes such as urban canyons. Today, there are no real alternatives to GPS for providing absolute global coordinates. We address this concern by introducing TerraSLAM, a new global positioning system that uses a 3D GIS model to bridge relative and absolute coordinate systems, thereby enhancing visual SLAM to function as a global positioning solution. TerraSLAM offers localization accuracy and efficiency comparable to GPS-RTK even in GPS-denied settings. Extensive evaluation on drone localization scenarios shows that TerraSLAM achieves an average global positioning accuracy of 0.21m and a 99th percentile within 0.67m, outperforming advanced GPS solutions by over 70\% (0.72m) and 80\% (3.62m). Additionally, when integrated with ORB-SLAM3, the localization latency per frame is 16.7ms, achieving a 60\% reduction compared to the baseline of 41.3ms. Code is available at https://github.com/cmusatyalab/TerraSLAM.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {42–55},
numpages = {14}
}
@inbook{10.1145/3711875.3729125,
author = {Yu, Shiming and Zhang, Ziyue and Xia, Xianjin and Zheng, Yuanqing and Wang, Jiliang},
title = {Are LoRa Logical Channels Really Orthogonal? Practically Orthogonalizing Massive Logical Channels},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3729125},
abstract = {LoRaWANs are envisioned to connect billions of IoT devices through thousands of physically overlapping yet logically orthogonal channels (termed logical channels). These logical channels hold significant potential for enabling highly concurrent scalable IoT connectivity. Large-scale deployments however face strong interference between logical channels. This practical issue has been largely overlooked by existing works but becomes increasingly prominent as LoRaWAN scales up. To address this issue, we introduce Canas, an innovative gateway design that is poised to orthogonalize the logical channels by eliminating mutual interference. To this end, Canas develops a series of novel solutions to accurately extract the meta-information of individual ultra-weak LoRa signals from the received overlapping channels. The meta-information is then leveraged to accurately reconstruct and subtract the LoRa signals over thousands of logical channels iteratively. Real-world evaluations demonstrate that Canas can enhance concurrent transmissions across overlapping logical channels by 2.3\texttimes{} compared to the best known related works.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {56–69},
numpages = {14}
}
@inbook{10.1145/3711875.3729136,
author = {Gong, Kaijie and Dong, Wei and Wang, Hao and Peng, Yingqi and Gao, Yi},
title = {Programming Embedded IoT Applications in Natural Language with IoTPilot},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3729136},
abstract = {In recent years, the swift expansion of Internet of Things (IoT) applications has been notable. However, developing a comprehensive IoT application is highly challenging for non-expert developers due to the highly diverse characteristics of embedded operating systems. LLM-based methods provide a paradigm for code generation through natural language, which can greatly simplify and accelerate the development of IoT applications. While promising, existing works have failed to account for the specific characteristics of embedded operating system, resulting in the lower quality of generated IoT code. In this paper, we present IoTPilot, an LLM-driven embedded IoT programming tool. We have observed that the conflicts between LLM internal APIs/headers and external OS-specific APIs/headers are key factors leading to the low quality of generated embedded IoT applications. Thus, we introduce two effective self-thinking chains to integrate internal LLM knowledge with external documentation, addressing conflicts in APIs and headers. We provide embedded IoT benchmarks (IoTEval), which are built on RIOT, Zephyr, Contiki and FreeRTOS. Results show that IoTPilot can improve the performance of IoT code generation on all the three embedded OSes compared with existing state-of-the-art (SOTA) methods.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {70–82},
numpages = {13}
}
@inbook{10.1145/3711875.3729153,
author = {Zou, Yang and Na, Xin and Sun, Yimiao and Chen, Yande and He, Yuan},
title = {Satori: In-band Analog Backscatter for Audio Transmission},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3729153},
abstract = {In IoT applications such as environmental monitoring and industrial security surveillance, audio sensors are increasingly used, among which wireless sensors are preferred. In order to achieve a sustained transmission, low-power wireless technology such as backscatter has been widely considered. However, existing backscatter systems encounter difficulties in audio transmissions due to the high power consumption from the complicated digital processing and fast frequency-shifting clocks. In this paper, we propose Satori, the first-of-its-kind in-band analog backscatter system for audio transmission with ultra-low power consumption. Satori eliminates the need for in-place digital processing by directly embedding analog audio voltages into backscattered WiFi symbols through analog modulation. It also avoids the power consumption of the frequency-shifting clock by transmitting the audio within the excitation WiFi signal's band. We implement the Satori prototype and evaluate it under various settings. The results indicate that Satori can transmit audio at a sampling rate of 41.67 kHz and achieve a SNR exceeding 18 dB.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {83–95},
numpages = {13}
}
@inproceedings{10.1145/3711875.3729164,
author = {Medaranga, Sooriya Patabandige Pramuka and Chinthalapani, Rajashekar Reddy and Yan, Wenqing and Dutta, Prabal and Varshney, Ambuj},
title = {Unraveling the Missing Link in Low-power Communication: An Autodyning Receiver Architecture that Achieves a Long Range},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3729164},
doi = {10.1145/3711875.3729164},
abstract = {Wireless communication remains the most power-intensive operation in embedded systems. Decades of research have enabled radio transmitters to operate at power levels as low as tens of μWs while maintaining practical communication ranges. However, achieving power-efficient reception over similarly useful distances has received significantly less attention. State-of-the-art low-power receivers typically rely on Schottky diode-based envelope detectors, which are inherently limited in sensitivity and unable to support complex modulation schemes. We introduce SoMix, the Single Oscillator Mixer receiver, a novel architecture that uses tunnel diode oscillators to overcome these limitations. Specifically, we demonstrate the autodyning property of tunnel diode oscillators, allowing a single circuit to generate both a carrier signal and perform signal downconversion, thus merging two traditionally power-hungry analog tasks into one energy-efficient step. The SoMix front-end consumes less than 100 μW while supporting high-sensitivity reception. Through injection-locking, SoMix stabilizes its tunnel diode oscillator using even a weak external carrier signal, allowing it to receive frequency-modulated transmissions from distances greater than 100 meters in line-of-sight environments. We also demonstrate that the SoMix exhibits robustness in complex real-world scenarios. SoMix outperforms state-of-the-art receivers in power, range, and functionality.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {96–109},
numpages = {14},
keywords = {embedded systems, backscatter, sensors, tunnel diodes, receivers},
location = {Hilton Anaheim, Anaheim, CA, USA},
series = {MobiSys '25}
}
@inbook{10.1145/3711875.3729137,
author = {Chen, Lili and Zhao, Yizhe and Wang, Shuning and Zhong, Linghui and Fu, Yongjian and Yue, Sheng and Ren, Ju and Zhang, Yaoxue},
title = {Towards Distance-Adaptive Wireless Charging},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3729137},
abstract = {Wireless charging holds significant promise for IoT devices and transportation networks by facilitating convenient and autonomous power supply. Traditional wireless charging technologies have typically adhered to a singular approach, choosing between near-field coupling or far-field radiation. However, our investigations uncover that each method outperforms the other at specific distances. This insight leads us to integrating the advantages of both to enable rapid wireless charging across any distance within the charging range. For this vision, we poses an intriguing question: "Can we develop a system that supports both near-field and far-field charging simultaneously?"We give an affirmative answer by introducing ChargeM, a novel wireless charging system that explores frequency diversity at the transceiver to support different modes. By controlling the transceiver to operate at disparate frequency bands, ChargeM can automatically and safely switch the charging mode according to the location of charging devices, thus enabling continuous fast charging. Furthermore, the smart metasurface is involved to perform beam-forming for high-efficient power transfer with a compact receiver. We propose new hardware component design methods to support multi-mode power transfer, and tailored a fast receiver localization strategy to guide metasurface and transmitter control. Finally, we developed a prototype of ChargeM for charging small mobile devices, and evaluated its performance through controlled experiments and real-world charging. Extensive experiments demonstrate that ChargeM supports both near-field and far-field charging, achieving charging distance of up to 2 m with a low transmit power of 20W.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {110–123},
numpages = {14}
}
@inbook{10.1145/3711875.3729128,
author = {Deng, Yongheng and Qiao, Ziqing and Zhang, Ye and Ma, Zhenya and Liu, Yang and Ren, Ju},
title = {CrossLM: A Data-Free Collaborative Fine-Tuning Framework for Large and Small Language Models},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3729128},
abstract = {While large language models (LLMs) are endowed with broad knowledge, their task-specific performance is often suboptimal. Fine-tuning LLMs with task-specific data from diverse nodes is necessary, but this data is typically safeguarded and not shared publicly due to privacy concerns. A common solution involves downstream nodes downloading the LLM locally and fine-tuning it with their proprietary data. However, owners often regard pre-trained LLMs as valuable assets and are reluctant to share them. Additionally, the significant computational resources required by LLMs make local fine-tuning impractical for many nodes. To mitigate these problems, this paper proposes CrossLM, a data-free collaborative fine-tuning framework for large and small language models. CrossLM enables resource-constrained nodes to train smaller language models (SLMs) using their private task-specific data. These SLMs are subsequently leveraged to promote the task-specific natural language generation and understanding capabilities of the LLMs. Simultaneously, the SLMs of nodes also benefit from enhancement by the fine-tuned LLMs. In this way, CrossLM avoids sharing private data and proprietary LLMs, and also reduces the resource requirements of nodes. Through extensive experiments across a range of benchmark tasks and popular language models, we demonstrate that CrossLM significantly boosts the task-specific performance of both LLMs and SLMs while preserving the generalization capabilities of LLMs.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {124–137},
numpages = {14}
}
@inbook{10.1145/3711875.3729141,
author = {Shen, Zheyu and He, Yexiao and Wang, Ziyao and Zhang, Yuning and Sun, Guoheng and Ye, Wanghao and Li, Ang},
title = {EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3729141},
abstract = {Large Language Models (LLMs) have gained significant attention due to their versatility across a wide array of applications. Fine-tuning LLMs with parameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these models to efficiently adapt to downstream tasks without extensive retraining. Deploying fine-tuned LLMs on multi-tenant edge devices offers substantial benefits, such as reduced latency, enhanced privacy, and personalized responses. However, serving LLMs efficiently on resource-constrained edge devices presents critical challenges, including the complexity of adapter selection for different tasks, memory overhead from frequent adapter swapping. Moreover, given the multiple requests in the multi-tenant settings, processing requests sequentially will result in underutilization of computational resources and significant latency. This paper introduces EdgeLoRA, an efficient system for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA incorporates three key innovations: (1) an adaptive adapter selection mechanism to streamline the adapter configuration process; (2) heterogeneous memory management, leveraging intelligent adapter caching and pooling to mitigate memory operation overhead; and (3) batch LoRA inference, which enables efficient batch processing to significantly reduce computational latency. Comprehensive evaluations using the Llama3.1-8B model demonstrates that EdgeLoRA significantly outperforms the status quo (i.e., llama.cpp) in terms of both latency and throughput. The results demonstrates EdgeLoRA could achieve up to 4\texttimes{} boost in throughput with less energy consumption. Even more impressively, it manages to serve several orders of magnitude more adapters simultaneously without sacrificing inference performance. These results highlight EdgeLoRA's potential to transform edge deployment of LLMs in multi-tenant scenarios, offering a scalable and efficient solution for resource-constrained environments.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {138–153},
numpages = {16}
}
@inbook{10.1145/3711875.3729132,
author = {Wang, Haoming and Yang, Boyuan and Yin, Xiangyu and Gao, Wei},
title = {Never Start from Scratch: Expediting On-Device LLM Personalization via Explainable Model Selection},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3729132},
abstract = {Personalization of Large Language Models (LLMs) is important in practical applications to accommodate the individual needs of different mobile users. Due to data privacy concerns, LLM personalization often needs to be locally done at the user's mobile device, but such on-device personalization is constrained by both the limitation of on-device compute power and insufficiency of user's personal data. In this paper, we address these constraints by fine-tuning an already personalized LLM with user's personal data, and present XPerT, a new technique that ensure proper selection of such already personalized LLMs based on explainability about how they were being fine-tuned. We implemented and evaluated XPerT on various smartphone models with mainstream LLMs, and experiment results show that XPerT reduces the computation costs of on-device LLM personalization by 83\%, and improves its data efficiency by 51\%.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {154–168},
numpages = {15}
}
@inbook{10.1145/3711875.3729151,
author = {Wang, Rongxiang and Xu, Zhiming and Lin, Felix Xiaozhu},
title = {WhisperFlow: speech foundation models in real time},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3729151},
abstract = {Speech foundation models, such as OpenAI's Whisper, become the state of the art in speech understanding due to their strong accuracy and generalizability. Yet, their applications are mostly limited to processing pre-recorded speech, whereas processing of streaming speech, in particular doing it efficiently, remains rudimentary. Behind this inefficiency are multiple fundamental reasons: (1) speech foundation models are trained to process long, fixed-length voice inputs (often 30 seconds); (2) encoding each voice input requires encoding as many as 1,500 tokens with tens of transformer layers; (3) decoding each output entails an irregular, complex beam search. As such, streaming speech processing on resource-constrained client devices is more expensive than other AI tasks, e.g., text generation.To this end, we present a novel framework, WhisperFlow, which embodies both model and system optimizations. (1) Hush word as a short, learnable audio segment; appended to a voice input, a hush word gracefully stops the speech model from processing more input without hallucination; (2) Beam pruning, which aligns streaming audio buffers over time and reuses results from earlier decoding rounds, therefore significantly accelerating decoding; and (3) CPU/GPU pipelining, which not only maps to the encoding/decoding stages dynamically, but also tunes to an optimal resource ratio, respecting the encoding/decoding speed that varies across voice inputs, models, and hardware.We test WhisperFlow on commodity ARM platforms with 4–12 CPU cores and 10–30 GPU cores. It reduces per-word latency by 1.6\texttimes{}–4.7\texttimes{} to as low as 0.5 second, while seeing negligible accuracy degradation. On an entry-level MacBook Air, WhisperFlow can keep the per-word latency around 1 second, with the whole device drawing only 7 Watts in total.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {169–182},
numpages = {14}
}
@inproceedings{10.1145/3711875.3729163,
author = {Yen, James and Wang, Jiarui and Huang, Zhibai and Wei, Zhixiang and Zhang, Ziyang and Chen, Chen and Yu, Senhao and Wang, Yun and Wang, Hao and Qi, Zhengwei},
title = {ARMing x86 Games: Accelerating Binary Translation Using Software-Only Validated Flag Speculation},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3729163},
doi = {10.1145/3711875.3729163},
abstract = {As ARM architecture becomes more prevalent in personal computers, users transitioning from x86-based Windows platforms face compatibility issues, particularly with x86 applications like games. Existing solutions, such as QEMU, Box64, and Apple's Rosetta 2, either incur high latency, face performance bottlenecks, or are limited to specific ecosystems. A key challenge remains the efficient translation of x86 status flags, which impacts performance.We propose a novel optimization method that enhances compatibility and performance by leveraging software-only strategies tailored to ARM hardware features. Using data flow analysis, our approach identifies when ARM's hardware flags can replace x86 flags, reducing reliance on software emulation and lowering translation overhead. This results in improved speed and compatibility for x86 applications on ARM, supporting demanding applications like games across x86 and ARM platforms without specialized hardware. Experimental results show significant performance gains, with computational tasks improving by up to 18\%, and graphics rendering (FPS) also increasing by up to 18\%. In particular, real-world testing on popular Steam titles demonstrates FPS improvements ranging from about 7\% to over 12\%.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {183–195},
numpages = {13},
keywords = {binary translation, x86, ARM, EFLAGS},
location = {Hilton Anaheim, Anaheim, CA, USA},
series = {MobiSys '25}
}
@inbook{10.1145/3711875.3729145,
author = {Liu, Renyuan and Leng, Yuyang and Liu, Kaiyan and Hu, Shaohan and Chen, Chun-Fu (Richard) and Zhao, Peijun and Yun, Heechul and Yao, Shuochao},
title = {DAF: An Efficient End-to-End Dynamic Activation Framework for on-Device DNN Training},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3729145},
abstract = {Recent advancements in on-device training for deep neural networks have underscored the critical need for efficient activation compression to overcome the memory constraints of mobile and edge devices. As activations dominate memory usage during training and are essential for gradient computation, compressing them without compromising accuracy remains a key research challenge. While existing methods for dynamic activation quantization promise theoretical memory savings, their practical deployment is impeded by system-level challenges such as computational overhead and memory fragmentation.To address these challenges, we introduce DAF, a Dynamic Activation Framework that enables scalable and efficient on-device training through system-level optimizations. DAF achieves both memory- and time-efficient dynamic quantization training by addressing key system bottlenecks. It develops hybrid reduction operations tailored to the memory hierarchies of mobile and edge SoCs, leverages collaborative CPU-GPU bit-packing for efficient dynamic quantization, and implements an importance-aware paging memory management scheme to reduce fragmentation and support dynamic memory adjustments. These optimizations collectively enable DAF to achieve substantial memory savings and speedup without compromising model training accuracy. Evaluations on various deep learning models across embedded and mobile platforms demonstrate up to a 22.9\texttimes{} reduction in memory usage and a 3.2\texttimes{} speedup, making DAF a scalable and practical solution for resource-constrained environments.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {196–208},
numpages = {13}
}
@inbook{10.1145/3711875.3729123,
author = {Gong, Chen and Liang, Bo and Gao, Wei and Xu, Chenren},
title = {Data Can Speak for Itself: Quality-guided Utilization of Wireless Synthetic Data},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3729123},
abstract = {Generative models have gained significant attention for their ability to produce realistic synthetic data that supplements the quantity of real-world datasets. While recent studies show performance improvements in wireless sensing tasks by incorporating all synthetic data into training sets, the quality of synthetic data remains unpredictable and the resulting performance gains are not guaranteed. To address this gap, we propose tractable and generalizable metrics to quantify quality attributes of synthetic data—affinity and diversity. Our assessment reveals prevalent affinity limitation in current wireless synthetic data, leading to mislabeled data and degraded task performance. We attribute the quality limitation to generative models' lack of awareness of untrained conditions and domain-specific processing. To mitigate these issues, we introduce SynCheck, a quality-guided synthetic data utilization scheme that refines synthetic data quality during task model training. Our evaluation demonstrates that SynCheck consistently outperforms quality-oblivious utilization of synthetic data, and achieves 4.3\% performance improvement even when the previous utilization degrades performance by 13.4\%.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {209–222},
numpages = {14}
}
@inbook{10.1145/3711875.3729134,
author = {Wen, Hao and Tian, Shizuo and Pavlov, Borislav and Du, Wenjie and Li, Yixuan and Chang, Ge and Zhao, Shanhui and Liu, Jiacheng and Liu, Yunxin and Zhang, Ya-Qin and Li, Yuanchun},
title = {AutoDroid-V2: Boosting SLM-based GUI Agents via Code Generation},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3729134},
abstract = {Large language models (LLMs) have brought exciting new advances to mobile UI agents, a long-standing research field that aims to complete arbitrary natural language tasks through mobile UI interactions. However, existing UI agents usually demand powerful large language models that are difficult to be deployed locally on end-users' devices, raising huge concerns about user privacy and centralized serving cost. Inspired by the remarkable coding abilities of recent small language models (SLMs), we propose to convert the UI task automation problem to a code generation problem, which can be effectively solved by an on-device SLM and efficiently executed with an on-device code interpreter. Unlike normal coding tasks that can be extensively pre-trained with public datasets, generating UI automation code is challenging due to the diversity, complexity, and variability of target apps. Therefore, we adopt a document-centered approach that automatically builds fine-grained API documentation for each app and generates diverse task samples based on this documentation. By guiding the agent with the synthetic documents and task samples, it learns to generate precise and efficient scripts to complete unseen tasks. Based on detailed comparisons with state-of-the-art mobile UI agents, our approach effectively improves the mobile task automation with significantly higher success rates and lower latency/token consumption. Code is open-sourced at https://github.com/MobileLLM/AutoDroid-V2.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {223–235},
numpages = {13}
}
@inbook{10.1145/3711875.3729146,
author = {Geissdoerfer, Kai and Splitt, Ingmar and Sokolowski, Matthias and Herrmann, Carsten and Kubicki, Jonas and de Winkel, Jasper and Zimmerling, Marco},
title = {Shepherd Nova: A Public Testbed for Rigorous Experiments Under Repeatable Energy-Harvesting Conditions},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3729146},
abstract = {Public testbeds are essential for replicable experiments and meaningful comparisons on shared physical infrastructure. While many testbeds exist for battery-powered Internet of Things (IoT) systems, there is a lack of public testbeds for observing and profiling the distributed operation of energy-harvesting IoT systems, including battery-free devices. We fill this gap and present Shepherd Nova, the first public testbed designed to support experiments under repeatable energy-harvesting conditions. Shepherd Nova uses field-recorded harvesting data to supply power to devices, consistently replicating real-world spatio-temporal energy availability across multiple experiments. Its virtual power source supports diverse ambient energy sources, harvesting circuitry, and energy storage devices. Moreover, Shepherd Nova provides services like general-purpose input/output (GPIO) tracing, power profiling, and serial output logging, all of which can run synchronously and with high resolution. Sub-microsecond synchronization enables precise correlation between these observations and emulated energy-harvesting conditions, offering unprecedented insights into distributed energy-harvesting IoT systems. In this paper, we describe Shepherd Nova's design, characterize its performance, and demonstrate its capabilities through controlled experiments and an example test case. To access the testbed, documentation as well as open-source harvesting data, hardware designs, and code, visit https://testbed.nes-lab.org/.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {236–248},
numpages = {13}
}
@inproceedings{10.1145/3711875.3729161,
author = {Jung, Chanyoung and Lee, Jeho and Kim, Gunjoong and Kim, Jiwon and Park, Seonghoon and Cha, Hojung},
title = {ARIA: Optimizing Vision Foundation Model Inference on Heterogeneous Mobile Processors for Augmented Reality},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3729161},
doi = {10.1145/3711875.3729161},
abstract = {Mobile Augmented Reality (AR) applications demand high-quality, real-time visual prediction, including pixel-level depth and semantics, to enable immersive and context-aware user experiences. Recently, Vision Foundation Models (VFMs) offer strong generalization capabilities on diverse and unseen data, supporting scalable mobile AR experiences. However, deploying VFMs on mobile devices is challenging due to computational limitations, particularly in maintaining both prediction accuracy and real-time performance. In this paper, we present ARIA, the first system that enables on-device inference acceleration of a VFM. ARIA employs the heterogeneity of mobile processors through a parallel and selective inference scheme: full-frame prediction is periodically offloaded to a processor with high parallelism capability like GPU, while low-latency updates on dynamic regions are conducted via a specialized accelerator like NPU. Implemented and evaluated using mobile devices, ARIA achieved significant improvements in accuracy and deadline success rate on diverse real-world mobile AR scenarios.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {249–262},
numpages = {14},
keywords = {mobile augmented reality, vision foundation model, on-device AI, heterogeneous mobile processors},
location = {Hilton Anaheim, Anaheim, CA, USA},
series = {MobiSys '25}
}
@inbook{10.1145/3711875.3729133,
author = {Chen, Xingyu and Fang, Xinmin and Zhang, Shuting and Zhang, Xinyu and He, Liang and Li, Zhengxiong},
title = {You Only Render Once: Enhancing Energy and Computation Efficiency of Mobile Virtual Reality},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3729133},
abstract = {Mobile Virtual Reality (VR) is essential for achieving convenient and immersive human-computer interaction and realizing emerging applications such as Metaverse and spatial computing. However, existing VR technologies require two separate renderings of binocular images, thereby causing a significant bottleneck for mobile devices with limited computing and battery capacity. This paper proposes a new approach to optimizing mobile VR rendering called YORO. By utilizing the per-pixel attribute, YORO can generate binocular VR images from the monocular image through genuinely one rendering, saving half the computation over conventional approaches. Our experimental evaluation and detailed user study indicate that, YORO can save 27\% power consumption on average and increase frame rate by 115.2\%, while maintaining similar binocular image quality compared with state-of-the-art mobile VR rendering solutions. YORO is production-ready and has already been tested in real VR applications. The source code, demo video, prototype android app, video game engine plugins, and more are released anonymously at YORO-VR.github.io.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {263–276},
numpages = {14}
}
@inbook{10.1145/3711875.3729148,
author = {Chai, Eugene and Apicharttrisorn, Kittipat and Wang, Limin and Chang, Hyunseok and Mukherjee, Sarit},
title = {PointPresence: An Online Habitat for Multi-User Mixed Reality Telepresence},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3729148},
abstract = {Mixed reality (MR) telepresence provides a shared common 3D space for networked users to enjoy real-time immersive experiences with natural interactivity and movement. Due to its high hardware and compute demands, a large-scale cloud-edge MR solution is necessary for achieving critical mass adoption, where users engage in the MR environment using low-cost cameras at the edge and complex 3D world processing is offloaded to the cloud servers. However, cloud-edge MR solutions have distinct challenges such as concurrent multiple camera support, unpredictable compute demands, and CPU/GPU contention. In this paper, we present PointPresence, an edge-compute framework for large-scale MR applications. PointPresence is deployed at an edge node, such as an O-RAN RIC or a local enterprise server, and provides low-latency MR experiences to a large number of users by incorporating intelligent camera selection for compute reduction, reactive pipeline design for adaptation to compute demand changes, and context-aware GPU sharing. Our comprehensive evaluation on an MR testbed shows that PointPresence reduces end-to-end latency by up to 3.5\texttimes{} and improves end user perceived visual quality by 30\%.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {277–290},
numpages = {14}
}
@inbook{10.1145/3711875.3729150,
author = {Hu, Dongyin and Yang, Xin and Yuh, Ahhyun and Wang, Zihao and Al-Aswad, Lama A. and Lee, Insup and Zhao, Mingmin},
title = {Tracking Blink Dynamics and Mental States on Glasses},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3729150},
abstract = {Eye blink dynamics offer crucial insights into physiological and cognitive states. Existing solutions either require specialized equipment for detailed measurements or sacrifice temporal resolution for accessibility. We present BlinkWise, the first system that transforms everyday eyewear into a wise device for detailed blink dynamics tracking as a tiny add-on. BlinkWise measures eye openness in real-time on resource-constrained edge devices by leveraging both RF modality's inherent efficiency and novel computational optimizations—including recurrentization of convolutional network operations, quantization-aware normalization, and a lightweight proposal algorithm. Evaluation with 20 subjects and more than 18,000 blink measurements demonstrated BlinkWise's high accuracy in capturing subtle blink dynamics at millisecond resolution, achieving a Pearson correlation of 0.981 with ground truth. Through three real-world application studies, we demonstrate BlinkWise's potential for monitoring cognitive states and ocular health. Code, datasets, and demo videos are available on our website.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {291–304},
numpages = {14}
}
@inbook{10.1145/3711875.3729152,
author = {Ruiz, Rafael and Lacruz, Jesus O. and Bloessl, Bastian and Hollick, Matthias and Widmer, Joerg},
title = {HELIX: High-speed Real-Time Experimentation Platform for 6G Wireless Networks},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3729152},
abstract = {Mobile networks are evolving rapidly, with 6G promising unprecedented capabilities in terms of data rates and ultra-low latencies. However, the development of testbed platforms for wireless experimentation has not kept pace. Existing platforms typically offer either end-to-end capabilities with low bandwidth or high bandwidth with limited or no real-time functionality. In this paper, we introduce HELIX, an experimentation platform with 6G scalable real-time capabilities. HELIX integrates a comprehensive physical layer subsystem with multi-numerology support alongside an advanced mixed software-hardware control unit responsible for interacting with the fronthaul network and dynamically configuring the functional split in real time. On the server side, we implement the necessary drivers and routines to enable seamless integration with O-RAN systems, thus facilitating open and end-to-end experimentation. We demonstrate the capabilities of HELIX through a variety of experiments at sub-6 GHz, 28 GHz, and 60 GHz frequencies. Notably, HELIX achieves data rates of up to 1200 Mbps using 256-QAM modulation with over 417 MHz of bandwidth, and end-to-end bidirectional latencies of 500 μs. We show advanced features, including the implementation of Integrated Sensing And Communication (ISAC), and discuss how the platform could be extended to support bandwidths of up to 1670 MHz.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {305–318},
numpages = {14}
}
@inproceedings{10.1145/3711875.3729162,
author = {Wang, Yida and Lu, Yu and Zhou, Yuxuan and Shen, Yifei and Qiu, Lili and Lai, Zeyuan and Chen, Yi-Chao and Pan, Hao and Zhou, Juntao and Ding, Dian and Wang, Mei and Xue, Guangtao and Zhang, Qian},
title = {High-resolution mmWave Imaging using Metasurface and Diffusion},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3729162},
doi = {10.1145/3711875.3729162},
abstract = {In this paper, we propose a novel high-resolution mmWave imaging technique that operates with a small, off-the-shelf mmWave module and eliminates the need for any mechanical movement, offering a streamlined, portable solution. Our approach tackles two primary challenges: 1) mmWave commodity hardware is constrained by a limited number of antennas, limiting imaging resolution, and 2) most wireless imaging algorithms rely on compressive sensing to overcome the physical constraints, which assumes sparsity - a condition that may not always apply. To address these challenges, we first design an optimized mmWave metasurface specifically tailored for high-resolution imaging. This involves deriving a unit cell pattern that achieves high signal penetration and near-2π phase control, followed by joint optimization of both the metasurface and the codebook to further refine the signal quality and imaging resolution. We further propose a diffusion-based neural network model that transforms mmWave signals into high-quality images by directly exploiting the inherent features of target images, providing a robust alternative to conventional compressive sensing approaches. Our method encodes mmWave signals into physical representations and employs conditional generation through stable diffusion, effectively enhancing image quality. Through comprehensive implementation and rigorous testbed experiments, we demonstrate the feasibility and effectiveness of our approach.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {319–332},
numpages = {14},
keywords = {mmWave imaging, mmWave metasurface, diffusion model},
location = {Hilton Anaheim, Anaheim, CA, USA},
series = {MobiSys '25}
}
@inbook{10.1145/3711875.3729129,
author = {Zhao, Yizhe and Chen, Lili and Li, Xinyi and Dong, Yue-Jiang and Ren, Ju and Zhang, Yaoxue},
title = {MetaGen: LLM-Driven Generative Framework for Intelligent Metasurface Element},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3729129},
abstract = {Metasurfaces are a transformative class of artificial electromagnetic materials with significant potential in communication, sensing, and security. However, existing design methods require detailed physical properties as input and lack flexibility under complex constraints, limiting their applicability. In this paper, we propose MetaGen, a general, efficient, and user-friendly generation framework for intelligent metasurface elements. MetaGen employs a fine-tuned large language model to translate natural language instructions into formatted physical properties and integrates a diffusion-based model to generate metasurface elements. Furthermore, we develop a metasurface element dataset with granular frequency sampling and extended geometric parameters to enable MetaGen to learn the complex relationships between metasurface element geometries and electromagnetic responses. Experimental results demonstrate that MetaGen effectively satisfies complex constraints, achieving electromagnetic responses closely aligned with target specifications.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {333–346},
numpages = {14}
}
@inbook{10.1145/3711875.3729154,
author = {Deram, Sai Pavan and Rossanese, Marco and Garcia-Saavedra, Andres and Shah, Syed Waqas Haider and Sciancalepore, Vincenzo and Widmer, Joerg and Costa-Perez, Xavier},
title = {RISENSE: Long-Range In-Band Wireless Control of Passive Reconfigurable Intelligent Surfaces},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3729154},
abstract = {Reconfigurable Intelligent Surfaces (RIS) are a promising technology for creating smart radio environments by controlling wireless propagation. However, several factors hinder the integration of RIS technology into existing cellular networks, including the incompatibility of RIS control interfaces with 5G PHY/MAC procedures for synchronizing radio scheduling decisions and RIS operation, and the cost and energy limitations of passive RIS technology. This paper presents RISENSE, a system for practical RIS integration in cellular networks. First, we propose a novel, low-cost, and low-power RIS design capable of decoding control messages without complex baseband operations or additional RF chains, utilizing a power sensor and a network of microstrip lines and couplers. Second, we design an effective in-band wireless RIS control interface, compatible with 5G PHY/MAC procedures, that embeds amplitude-modulated (AM) RIS control commands directly into standard OFDM-modulated 5G data channels. Finally, we propose a low-overhead protocol that supports swift on-demand RIS re-configurability, making it adaptable to varying channel conditions and user mobility, while minimizing the wastage of 5G OFDM symbols. Our experiments validate the design of RISENSE and our evaluation shows that our system can re-configure a RIS at the same pace as users move, boosting 5G coverage where static or slow RIS controllers cannot.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {347–360},
numpages = {14}
}
@inbook{10.1145/3711875.3729131,
author = {Lee, Hojeong and Kim, Yu Hong and Ryu, Sangwoo and Hong, James Won-Ki and Ha, Sangtae and Kim, Seyeon},
title = {DeltaStream: 2D-Inferred Delta Encoding for Live Volumetric Video Streaming},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3729131},
abstract = {Live volumetric video streaming enables immersive user experiences but poses significant challenges due to the high bandwidth requirement that 3D representations entail. Recent research has focused on reducing volumetric video bandwidth, but it struggles to effectively address temporal redundancy under real-time constraints, limiting its applicability for live streaming scenarios. To address these challenges, we present DeltaStream, a novel live volumetric video streaming system that efficiently encodes 3D point clouds by leveraging 2D information. By utilizing 2D RGB and depth frames, DeltaStream efficiently infers inter-frame changes to reduce the streaming bandwidth of volumetric video. Furthermore, DeltaStream introduces an adaptive block-based approach that can reduce the client-side decoding load. Through extensive evaluations, our results demonstrate that DeltaStream reduces bandwidth by up to 71\% with 1.63\texttimes{} faster decoding speed while maintaining visual quality compared to state-of-the-art systems.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {361–373},
numpages = {13}
}
@inbook{10.1145/3711875.3729140,
author = {Yang, Donggyu and Nam, Wooseung and Kang, Byunggu and Lee, Kyunghan},
title = {Empilo: Realizing Immersive Mobile 3D Video Conferencing through Parameterized Communication},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3729140},
abstract = {In this work, we explore a new communication paradigm for immersive 3D video conferencing, termed parameterized communication, which dramatically reduces bandwidth usage by eliminating the need to exchange excessive volumetric data. Instead, this approach extracts a compact set of informative parameters representing key elements in the 3D space, transmits only these parameters, and reconstructs the scene on the receiving end. Translating this concept into practice, we present Empilo, a mobile 3D conferencing system composed of a face parameter extractor and a neural rendering-based scene generator. However, while neural rendering excels at synthesizing arbitrary views of objects without explicit 3D models, its heavy computational demands present a major obstacle for mobile deployment. To overcome this challenge, we propose a novel technique called truncated ray marching, which significantly reduces computational overhead by replacing iterative MLP inferences with a single-pass of a shallow neural network. Furthermore, to ensure a consistently immersive experience, we structure the neural-free lightweight renderer as a decoupled component, dedicated to delivering rapid responsiveness to dynamic viewpoint changes. These breakthroughs on computation together enable Empilo to rely entirely on mobile resources, achieving real-time performance with a frame generation time of 30.3 ms and a re-rendering latency of just 6.6 ms—all while operating at an exceptionally low bitrate of 24 kbps. Our approach provides valuable guidance for the practical deployment of 3D conferencing, envisioning accessibility on par with platforms like FaceTime and Zoom.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {374–387},
numpages = {14}
}
@inproceedings{10.1145/3711875.3729160,
author = {Zhang, Rui-Xiao and Huang, Tianchi and Chen, Bo and Nahrstedt, Klara},
title = {NeRFlow: Towards Adaptive Streaming for NeRF Videos},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3729160},
doi = {10.1145/3711875.3729160},
abstract = {Neural Radiance Field (NeRF) has emerged as a powerful technique for 3D scene representation due to its high rendering quality. Among its applications, mobile NeRF video-on-demand (VoD) is especially promising, benefiting from both the scalability of the mobile devices and the immersive experience offered by NeRF. However, streaming NeRF videos over real-world networks presents significant challenges, particularly due to limited bandwidth and temporal dynamics. To address these challenges, we propose NeRFlow, a novel framework that enables adaptive streaming for NeRF videos through both bitrate and viewpoint adaptation. NeRFlow solves three fundamental problems: first, it employs a rendering-adaptive pruning technique to determine voxel importance, selectively reducing data size without sacrificing rendering quality. Second, it introduces a viewpoint-aware adaptation module that efficiently compensates for uncovered regions in real time by combining pre-encoded master and sub-frames. Third, it incorporates a QoE-aware bitrate ladder generation framework, leveraging a genetic algorithm to optimize the number and configuration of bitrates while accounting for bandwidth dynamics and ABR algorithms. Through extensive experiments, NeRFlow is demonstrated to effectively improve user Quality of Experience (QoE) by 31.3\% to 41.2\%, making it an efficient solution for NeRF video streaming.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {388–401},
numpages = {14},
keywords = {content delivering, neural radiance field, video streaming},
location = {Hilton Anaheim, Anaheim, CA, USA},
series = {MobiSys '25}
}
@inbook{10.1145/3711875.3729139,
author = {Yi, Juheon and Lee, Goodsol and Jeong, Minkyung and Shin, Seokgyeong and Kim, Daehyeok and Lee, Youngki},
title = {Towards End-to-End Latency Guarantee in MEC Live Video Analytics with App-RAN Mutual Awareness},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3729139},
abstract = {While mobile live video analytics apps require end-to-end latency guarantee for responsiveness and immersiveness, achieving consistent low latency is challenging due to complex fluctuations of wireless channel and scene complexity; for example, latency SLO satisfaction rate drops to as low as 26\% in commercial 5G MEC platforms. Prior works mostly focus on either app-only (bitrate, DNN adaptation, or GPU allocation) or RAN-only (radio resource allocation) scheduling, with mutual ignorance of the other side resulting in mismatched scheduling decisions and frequent SLO violations. Coordinating the two schedulers is also challenging, as they are run separately by network and cloud operators with disjoint control. We present ARMA, an end-to-end live video analytics system with app-RAN mutual-awareness for high end-to-end latency SLO satisfaction in MEC. We design a mutually-aware decoupled scheduling mechanism on top of RAN Intelligent Controller (RIC) in Open-RAN architecture that fosters cooperative interaction between the two operators' schedulers while preserving operational proprietaries. We prototype an Open RAN-enabled 5G MEC testbed and evaluate ARMA, showing that ARMA achieves 97\% SLO satisfaction rate.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {402–416},
numpages = {15}
}
@inbook{10.1145/3711875.3729155,
author = {Qian, Kun and Pathak, Parth},
title = {Toward Spoofing-Resilient and Communication-Integrated MmWave Radar Sensing},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3729155},
abstract = {MmWave FMCW radars are integrated into many sensing systems for robust sensing. However, their sensing functions are vulnerable to spoofing attacks and interfered with by backscatter communications, both of which can cause sensor malfunction and system failure. Noticing that radar spoofing and communication share similar signal modulation mechanisms, in this paper, we present SCR, a new Spoofing-resilient and Communication-integrated Radar sensing scheme. SCR is based on the rigorous analysis of the radar sensing model that highlights the differences between modulated spoofing and communication signals and normal sensing signals reflected by natural objects. The key designs of SCR are a novel chirp configuration scheme and signal processing pipeline, which signify different patterns between modulated and normal signals in radar spectra, for reliable detection of spoofing and communication. We have developed SCR and tested it with actual 77 GHz mmWave radar sensors and backscatter prototypes. Our field tests show that SCR can reliably detect fake objects created by modulated signals in both velocity and distance radar sensing domains.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {417–430},
numpages = {14}
}
@inbook{10.1145/3711875.3729157,
author = {Xu, Yihan and Guo, Dongfang and Song, Qun and Lou, Yang and Zhu, Yi and Wang, Jianping and Qiao, Chunming and Tan, Rui},
title = {Dynamic Defense for Car-Borne LiDAR Vehicle Detection},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3729157},
abstract = {Adversarial attacks with real objects or lasers on car-borne LiDAR-based object detection are concerning. The existing defense approaches are often designed to address specific attacks and short of considering adaptive attackers who may adapt based on all available information about the deployed defense to maximize attack effect. This paper proposes Hyper3Def, a new defense for the function of detecting vehicle objects, which uses a Hypernet to generate an ensemble of multiple new detection models when needed at run time. The detection results of these models are fused to give the final result. As a dynamic defense, Hyper3Def revokes an important basis of the adaptive attack, i.e., the object detection model is needed to plan effective adversarial perturbations. Evaluation based on open data and real-world experiments with embedded system implementation show that, when confronting adaptive attacks, Hyper3Def outperforms various baseline defenses including the adversarial training, which is often cited as the state of the art.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {431–444},
numpages = {14}
}
@inbook{10.1145/3711875.3729138,
author = {Dodds, Laura and Boroushaki, Tara and Zhou, Kaichen and Adib, Fadel},
title = {Non-Line-of-Sight 3D Object Reconstruction via mmWave Surface Normal Estimation},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3729138},
abstract = {This paper presents the design, implementation, and evaluation of mmNorm, a new and highly-accurate method for non-line-of-sight 3D object reconstruction using millimeter wave (mmWave) signals. In contrast to past approaches for millimeter-wave-based imaging that perform backprojection for 3D object reconstruction, mmNorm reconstructs the surface by estimating the object's surface normals. To do this, it introduces a novel algorithm that directly estimates the surface normal vector field from mmWave reflections. By then inverting the normal field, it can reconstruct structural isosurfaces, then solve for the exact surface through a novel mmWave optimization framework.We built an end-to-end prototype of mmNorm using a TI IWR1443 Boost mmWave radar and a UR5e Robotic Arm, and evaluated it in over 110 real-world experiments across more than 60 different everyday objects. In a head-to-head comparison with state-of-the-art baselines, mmNorm achieves 96\% reconstruction accuracy (3D F-score) compared to 78\% for the best-performing baseline. These results show that mmNorm is capable of high-accuracy mmWave object reconstruction. The codebase and a video demonstration are available here: https://github.com/signalkinetics/mmNorm},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {445–458},
numpages = {14}
}
@inbook{10.1145/3711875.3729130,
author = {Ren, Haojie and Zhang, Wuyang and Shi, Shuyao and Zhang, Xinran and Zhang, Lu and Zhang, Yanyong},
title = {UniSense: Spatial-Uncertainty-Aware Collaborative Sensing for Autonomous Driving},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3729130},
abstract = {Vehicle-to-vehicle collaborative perception faces fundamental deployment barriers: raw LiDAR data sharing requires over 300 Mbps per vehicle - far exceeding V2X network capacities, while network delays of 80-200ms create dangerous temporal misalignments at highway speeds. We present UniSense, a distributed collaborative perception system that enables efficient and reliable multi-vehicle perception through uncertainty-driven sensor data exchange. Instead of sharing raw sensor data, vehicles exchange compact uncertainty maps that identify regions requiring additional perceptual information. Our key innovations include: (1) a lightweight uncertainty quantification pipeline that runs in real-time on automotive hardware, identifying perception-critical regions while reducing bandwidth requirements by more than 10\texttimes{}, (2) a bandwidth-aware protocol that dynamically adapts data sharing based on network conditions and perception uncertainty, and (3) a selective motion compensation scheme that maintains temporal consistency. We evaluate UniSense through a year-long deployment with 16 roadside LiDAR nodes and autonomous vehicles across our campus. Our experimental results show that UniSense extends reliable perception range from local 80m to 140m, improving accuracy by 1.33\texttimes{} on average, up to 1.73\texttimes{}, over the state-of-the-art baselines, under communication constraints. The code and dataset are available at https://github.com/LetStarFly/UniSense.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {459–472},
numpages = {14}
}
@inbook{10.1145/3711875.3729158,
author = {Sun, Bangjie and Chan, Mun Choon and Han, Jun},
title = {CAMPrints: Leveraging the "Fingerprints" of Digital Cameras to Combat Image Theft},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3729158},
abstract = {Photo sharing is increasingly popular, driven by social media platforms like Instagram and services such as Flickr and Google Photos. However, this growth has been accompanied by significant issues, particularly image theft. To address this issue, we introduce CAMPrints, a robust system for detecting image theft. CAMPrints verifies whether edited images found online contain camera fingerprints matching those of user-provided reference images. The system overcomes the challenges of identifying images altered by diverse image processing operations. We select a small yet representative set of operations by categorizing them based on their impact on pixel values and locations. A deep-learning model is trained to recognize and compare camera noise patterns pre- and post-editing. We conduct real-world evaluations involving 36 cameras across eight make-and-model combinations, along with over 40 image processing operations applied to more than 4,000 images. CAMPrints achieves an average AUC of 0.92, significantly outperforming the state-of-the-art methods by up to 1.8 times.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {473–486},
numpages = {14}
}
@inbook{10.1145/3711875.3729143,
author = {Jiang, Nan and Zhou, Qihang and Qian, Feifan and Chen, Jiayun and Huang, Heqing and Jia, Xiaoqi and Du, Haichao},
title = {Chameleon: Towards Building Least-privileged TEE via Functionality-based Resource Re-grouping},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3729143},
abstract = {TrustZone-assisted Trusted Execution Environment (TEE) has been widely employed in mobile devices to protect sensitive applications. With increased customization demands, Trusted Applications (TAs) have become more flexible and complex, exposing numerous vulnerabilities within the TEE. Furthermore, due to the unrestricted Trusted Operating System (TOS) services provided to TA, an attacker can exploit vulnerabilities to compromise the whole TEE system. In this paper, we propose a novel customized TOS partition approach, called Chameleon, to enhance the security of the TrustZone-assisted TEE system. Inspired by the principle of least privilege and our TEE vulnerability analysis, we first categorize the TOS into TOS service modules and basic kernel modules. Then, we selectively encapsulate these modules into distinct Capsules based on the TA's functional requirements, providing each TA with a separate execution environment (TA-entity). To enforce access control and confine vulnerable modules within a TA-entity, we introduce T-Visor, which serves as our Trusted Computing Base. Our prototype implementation, built upon Linaro's OP-TEE, requires only 2.9K Lines of Code (LoC) modifications. Evaluation on a Hikey960 board demonstrates that Chameleon reduces the attack surface of TOS services to 51\% and mitigates 122 out of 138 CVEs (88.41\%) with negligible performance overhead.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {487–499},
numpages = {13}
}
@inbook{10.1145/3711875.3729156,
author = {Abir, Tasnim Azad and Le, Vu and Kuantama, Endrowednes and Gupta, Pranjol Sen and Copley, Austin and Dawes, Judith and Islam, Mohammad and Han, Richard and Nguyen, Phuc},
title = {Detection and Tracking of Drone Swarms using LiDAR},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3729156},
abstract = {This paper introduces LiSWARM, a low-cost LiDAR system to detect and track individual drones in a large swarm. LiSWARM provides robust and precise localization and recognition of drones in 3D space, which is not possible with state-of-the-art drone tracking systems that rely on radio-frequency (RF), acoustic, or RGB image signatures. It includes (1) an efficient data processing pipeline to process the point clouds, (2) robust priority-aware clustering algorithms to isolate swarm data from the background, (3) a reliable neural network-based algorithm to recognize the drones, and (4) a technique to track the trajectory of every drone in the swarm. We develop the LiSWARM prototype and validate it through both in-lab and field experiments. Notably, we measure its performance during two drone light shows involving 150 and 500 drones and confirm that the system achieves up to 98\% accuracy in recognizing drones and reliably tracking drone trajectories. To evaluate the scalability of LiSWARM, we conduct a thorough analysis to benchmark the system's performance with a swarm consisting of 15,000 drones. The results demonstrate the potential to leverage LiSWARM for other applications, such as battlefield operations, errant drone detection, and securing sensitive areas such as airports and prisons.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {500–514},
numpages = {15}
}
@inbook{10.1145/3711875.3729142,
author = {Huang, Peng and Pan, Kun and Wang, Qinglong and Cheng, Peng and Lu, Li and Ba, Zhongjie and Ren, Kui},
title = {SecHeadset: A Practical Privacy Protection System for Real-time Voice Communication},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3729142},
abstract = {Voice communication is convenient while also poses risks of privacy leakage, due to potential interception or eavesdropping during voice transmission. Current protections of voice privacy are almost entirely controlled by communication service providers (CSPs), which operate as a black-box to users thus hard to fully trust. To take back the control of user privacy, in this paper, we introduce SecHeadset, an end-to-end solution for secure voice communication based on voice obfuscation, which is plug-and-play and compatible with various CSPs. Our solution involves two parts. First, we design a voice-like noise masking scheme for voice obfuscation. The noise, mimicking voice characteristics, could effectively obscure users' voices while demonstrating resilience against noise reduction methods. Second, we develop a protocol that enables efficient channel state estimation and secure information exchange between two communication entities. Based on this information, we propose a lightweight algorithm for voice retrieval during communication. We develop a prototype of SecHeadset and evaluate its performance with 8 widely-used applications, including Telegram and Skype. It reduces the voice recognition accuracy of various adversaries to below 15\% while maintaining communication quality. We also integrate SecHeadset with off-the-shelf portable devices and verify its real-world effectiveness.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {515–527},
numpages = {13}
}
@inbook{10.1145/3711875.3729147,
author = {Wang, Pengcheng and Liu, Zhuoming and Bagchi, Shayok and Xu, Ran and Bagchi, Saurabh and Li, Yin and Chaterji, Somali},
title = {Agile3D: Adaptive Contention- and Content-Aware 3D Object Detection for Embedded GPUs},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3729147},
abstract = {Efficient 3D perception is critical for autonomous systems—self-driving vehicles, drones—to navigate safely in dynamic environments. Accurate 3D object detection from LiDAR data must handle irregular, high-volume point clouds, variable latency from contention and scene complexity, and tight embedded GPU constraints. Balancing accuracy and latency under dynamic conditions is crucial, yet existing frameworks like Chanakya [NeurIPS '23], LiteReconfig [EuroSys '22], and AdaScale [MLSys '19] struggle with the unique demands of 3D detection. We present Agile3D, the first adaptive 3D system integrating a cross-model Multi-branch Execution Framework (MEF) and a Contention- and Content-Aware Reinforcement Learning-based controller (CARL). CARL dynamically selects the optimal execution branch using five novel MEF control knobs: encoding format, spatial resolution, spatial encoding, 3D feature extractor, and detection head. CARL uses supervised training for stable initial policies, then Direct Preference Optimization (DPO) to finetune branch selection without hand-crafted rewards, presenting the first application of DPO to branch scheduling in 3D detection. Comprehensive evaluations show that Agile3D achieves state-of-the-art performance, maintaining high accuracy across varying hardware contention levels and 100-500 ms latency budgets. On NVIDIA Orin and Xavier GPUs, it consistently leads the Pareto frontier, outperforming existing methods for efficient 3D detection.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {528–541},
numpages = {14}
}
@inbook{10.1145/3711875.3729127,
author = {Dai, Yubin and Qian, Bin and Yan, Yuxuan and Zhao, Minglei and Liu, Yangkun and Shu, Yuanchao},
title = {EAR-Mapping: Edge-Assisted Real-Time Dense Mapping with Low Bandwidth Requirements},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3729127},
abstract = {3D reconstruction plays a critical role in applications such as augmented reality (AR) and robotic systems. However, implicit neural representations (INRs), widely used in modern 3D reconstruction systems, demand substantial communication and computational resources, making the reconstruction process excessively slow and costly. In this paper, we introduce EAR-Mapping, a novel edge-assisted online 3D reconstruction framework designed for latency-sensitive mobile applications. EAR-Mapping incorporates an innovative sampling mechanism that seamlessly integrates explicit and implicit methods, enabling selective processing of camera data to maximize reconstruction performance. In addition, we utilize a value-based representation module to maximize computation resource efficiency. Finally, we design a framework that minimizes communication overhead through ROI-based data transmission. Our prototype implementation on a mobile-edge testbed demonstrates that EAR-Mapping achieves up to a 1.2x reduction in reconstruction latency and a 3.5x reduction in bandwidth usage, offering a significant advancement in the efficiency of 3D reconstruction for mobile-edge systems.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {542–555},
numpages = {14}
}
@inbook{10.1145/3711875.3729149,
author = {Ma, Zhiyao and Chen, Guojun and Chen, Zhuo and Zhong, Lin},
title = {Hopter: a Safe, Robust, and Responsive Embedded Operating System},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3729149},
abstract = {Microcontroller-based embedded systems are vulnerable to memory safety errors and must be robust and responsive because they are often used in unmanned and mission-critical scenarios. The Rust programming language offers an appealing compile-time solution for memory safety but leaves stack overflows unresolved and foils zero-latency interrupt handling. We present Hopter, a Rust-based embedded operating system (OS) that provides memory safety, system robustness, and interrupt responsiveness to embedded systems while requiring minimal application cooperation. Hopter executes Rust code under a novel finite-stack semantics that converts stack overflows into Rust panics, enabling recovery from fatal errors through stack unwinding and restart. Hopter also employs a novel mechanism called soft-locks so that the OS never disables interrupts. We compare Hopter with other well-known embedded OSes using controlled workloads and report our experience using Hopter to develop a flight control system for a miniature drone and a gateway system for Internet of Things (IoT). We demonstrate that Hopter is well-suited for resource-constrained microcontrollers and supports error recovery for real-time workloads.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {556–569},
numpages = {14}
}
@inbook{10.1145/3711875.3729126,
author = {Zhang, Chong and Wang, Han and Meng, Qianhe and Zhao, Yize and Song, Yihang and Xu, Kanglin and Li, Jinzhe and Lu, Li},
title = {LEGO+: Redefining the Redundancy Removal for IoT Sensing Edge-End Systems},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3729126},
abstract = {The Internet of Things (IoT) can only thrive if IoT sensor nodes can be effortlessly deployed and maintained without compromising their general-purpose nature. However, existing low-power sensor systems fail to strike a balance between these two issues, leaving the widespread of IoT sensor nodes as an open problem. In this paper, we propose LEGO+ as a minimalist yet general-purpose sensing edge-end architecture. Instead of running embedded software on a redundant general-purpose microprocessor, LEGO+ can directly construct the desired control functionality for various IoT sensing applications through hardware-level logic orchestration. To achieve this, we first conduct an in-depth analysis of the underlying unit behaviors within IoT sensor systems and, based on this, abstract a uniform logic orchestration model. Next, to enable sensor nodes to comprehend and execute the generated logic, we devise a hierarchical atomic control circuit with negligible overheads. Finally, we develop a task state prediction scheme to further improve the overall operation efficiency among multiple nodes. We prototype LEGO+ for proof-of-concept and conduct comprehensive experiments, and the results demonstrate that LEGO+ can reduce the overall power consumption of sensor nodes by 86\% and enhance task efficiency by 49\%, thereby facilitating a wider array of IoT sensing applications.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {570–582},
numpages = {13}
}
@inbook{10.1145/3711875.3734545,
author = {Ren, Yili and Zhang, Haopeng and Yuan, Haohan and Zhang, Jingzhe and Shen, Yitong},
title = {Poster: Large Language Model-powered Wi-Fi-based Human Activity Recognition},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3734545},
abstract = {Recent advances in LLMs have shown exceptional reasoning capability. However, their ability to integrate physical model knowledge for real-world signal interpretation remains largely unexplored. We introduce Wi-Chat, an LLM-powered Wi-Fi-based human activity recognition system. By embedding Wi-Fi sensing principles into prompts, we show that LLMs can infer human activities through Wi-Fi signals in a zero-shot manner without complex signal processing.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {583–584},
numpages = {2}
}
@inbook{10.1145/3711875.3734546,
author = {Gao, Bo and Zheng, Tianyue},
title = {Poster: GlueRT: Bridging the Gap Between Ray-tracing and Real-World Wireless Channel},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3734546},
abstract = {Despite ray-tracing's theoretical foundation in radio frequency propagation modeling, significant discrepancies persist between simulated and measured channel characteristics in real environments. This paper presents GlueRT, a hybrid framework leveraging ray-tracing's physical interpretability while using a targeted neural network to correct simulation-reality gaps. GlueRT applies neural network-generated corrections to both amplitude and phase components of ray-tracing channel estimates, maintaining explainability while requiring less training data than pure learning methods. Experiments across various environments demonstrate GlueRT achieves higher channel estimation accuracy than traditional ray-tracing or neural network approaches.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {585–586},
numpages = {2}
}
@inbook{10.1145/3711875.3734547,
author = {Cui, Yanru and Zheng, Tianyue},
title = {Poster: HeteroRF: Heterogeneity-Aware Federated Radio-Frequency Human Activity Recognition},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3734547},
abstract = {Ubiquitous radio-frequency (RF) devices constantly gather environmental and human data, generating substantial information for Human Activity Recognition (HAR). This data remains underutilized due to its distributed nature, privacy concerns, and lack of labels. Federated learning (FL) offers a promising solution for RF-HAR data mining, but conventional FL methods often fail due to divergence from inherent heterogeneity across distributed RF devices. We propose HeteroRF, a novel FL framework addressing RF-HAR divergence at three levels: a data augmentation mechanism at the raw-data level, curriculum learning to stabilize local training, and Gumbel softmax-based client selection to address global heterogeneity. Experiments demonstrate that HeteroRF significantly improves RF-HAR accuracy.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {587–588},
numpages = {2}
}
@inbook{10.1145/3711875.3734548,
author = {Ji, Xiang and Zeng, Liekang and Yan, Zhenyu and Guo, Yunqi and Chen, Hongkai and Wang, Yao and Xing, Guoliang},
title = {Poster: Mobile Menstrual Health Advising with Multimodal Feature Engineering},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3734548},
abstract = {Menstrual pain has constituted a persistent public health challenge, impairing women's socioeconomic participation through recurrent functional limitations. Conventional diagnostic methodologies relying on symptomatic retrospection demonstrate inadequate prognostic capability for anticipatory care planning. To address these challenges, we present PainCare, an intelligent predictive system integrating multimodal self-reported data with efficient language model processing for menstrual pain forecasting. PainCare employs an advanced feature engineering process to maintain performance across varying pain severities, demonstrating significant improvements in pain prediction. Our validation study with 176 participants achieved 94.7\% prediction accuracy across four pain intensity levels, a 43.5\% improvement over conventional methods.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {589–590},
numpages = {2}
}
@inbook{10.1145/3711875.3734549,
author = {Kim, Miru and Joe, Mugon and Kwon, Minhae},
title = {Improving Network Attack Classification on Imbalanced Real-world Intrusion Incident Datasets},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3734549},
abstract = {In real-world datasets, the volume of collected data varies according to the characteristics of each attack type, resulting in class-imbalanced datasets. This class imbalance impacts attack type classifier performance by causing mispredictions of minority attack types, leading to unreliable detection of less common, yet more harmful, intrusions. To address this issue, we propose a two-step pair selection strategy to enhance contrastive learning (CL) in the realistic network traffic environment. We evaluate the proposed solution using datasets collected by the Korea Internet Security Agency (KISA) based on real-world cybersecurity incidents.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {591–592},
numpages = {2}
}
@inbook{10.1145/3711875.3734550,
author = {Hahm, Gyeongjune and Cheon, Kyung-yul and Kwon, Hyeyeon and Park, Seungkeun},
title = {Poster: A Diffusion Model for Predicting Spectrum Efficiency in 5G Networks},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3734550},
abstract = {This study proposes a regression diffusion model for predicting spectrum efficiency (SE) in 5G networks, addressing limitations of traditional analytical models in dense urban environments. Using 5G user equipment (UE) measurement data collected in Seoul, we developed a model that leverages SINR (Signal-to-Interference-plus-Noise Ratio) and TBS (Transport Block Size) as input variables with classifier-free guidance and hybrid loss functions. Experimental results demonstrate improved performance with 26.37\% MAPE compared to alternative methods. The model accurately captures SE distribution characteristics and temporal variations, contributing to enhanced simulation accuracy for 5G network management in dense urban areas.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {593–594},
numpages = {2}
}
@inbook{10.1145/3711875.3734551,
author = {Chia, Lih Wei and Motani, Mehul},
title = {Poster: OCC Is Better Than Terahertz Wave for 6G},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3734551},
abstract = {The increasing demands of next-generation wireless applications are straining the limited sub-6 GHz spectrum. While terahertz wave (T-Wave) offers expanded bandwidths for 6G, its short range, increased energy use, and high costs limit viability. Optical Camera Communications (OCC) is a practical alternative, using image sensors to simultaneously decode multiple spatially-separated optical streams. OCC offers T-Wave-like directional communication, with added benefits such as signal origin verification and precise localization, at a higher technology readiness level. As an example, OCC can support applications like vehicle-to-everything (V2X), enabling ranging, lane-level positioning, and pedestrian safety. We also validated OCC's viability in dense, high-throughput scenarios through prototypes built with COTS components.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {595–596},
numpages = {2}
}
@inbook{10.1145/3711875.3734557,
author = {Kim, Hyungkoo and Chae, ChulSeoung and Sim, ByeolHee and Yoon, DongSik and Kang, JeongHoon},
title = {Posters: Edge AI-Based Integrated Model Architecture for Optimization of Semiconductor ALD Processes: Real-Time Feedback and Model Update Framework for Thin Film Quality Control},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3734557},
abstract = {Semiconductor ALD (Atomic Layer Deposition) is a precision-critical process involving sequential stages and large-scale time-series data from recipe settings and sensors. This paper proposes an edge AI architecture combining lightweight models on edge devices with centralized model training. The system enables early predictions from recipe data, real-time adjustments via sensor inputs, and continuous refinement using post-process outcomes. Only extracted features and result data (film thickness and uniformity) are transmitted to reduce communication overhead and protect sensitive data. The architecture supports performance monitoring and seamless model redeployment, adapting to changing equipment and environments. This approach improves product quality, reduces defect rates, and enhances manufacturing adaptability.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {597–598},
numpages = {2}
}
@inbook{10.1145/3711875.3734561,
author = {Shen, Leming and Zheng, Yuanqing},
title = {Poster: Towards Federated Embodied AI with FEAI},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3734561},
abstract = {Embodied AI (EAI) transforms our daily lives by bridging intelligent agents with various sensors and actuators. Large Language Models (LLMs) further enhance EAI agents in environment comprehension, task decomposition, and action execution for robotic manipulation. However, developing a general EAI agent capable of adapting to and continuously learning from diverse operating environments is extremely challenging: 1) Robots with mobility capture environments from multiple perspectives, leading to heterogeneous semantic interpretations, particularly in large or open settings. 2) Heterogeneous environments further exacerbate the variability of decomposed tasks and corresponding actions required for robotic manipulation. To address these challenges, we propose FEAI, a novel paradigm to enhance the adaptability and self-learning capabilities of EAI agents in heterogeneous environments via federated embodied learning. Specifically, FEAI shares and constructively aggregates environment semantic maps, decomposed task templates, and action-reward rules from federated EAI agents. The aggregated information can further enhance EAI agents' local models through continuous tuning or dynamically updated knowledge databases. We believe that FEAI has significant potential to integrate more advanced technologies, further advancing performance and innovation in the field of EAI.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {599–600},
numpages = {2}
}
@inbook{10.1145/3711875.3734563,
author = {Wang, Wei-Hsiang and Hu, Yuqian and Zhu, Guozhen and Wang, Beibei and Liu, K. J. Ray},
title = {Poster: Efficient Passive Tracking using Commodity WiFi with Single-Shot Training},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3734563},
abstract = {Indoor tracking plays a critical role in a wide range of applications, yet existing solutions based on cameras, acoustics, or radar often face challenges related to privacy, deployment cost, and environmental sensitivity. WiFi-based methods offer a promising alternative by leveraging existing infrastructure, but most current approaches are active, requiring users to carry dedicated devices—limiting practicality in everyday scenarios. Passive WiFi tracking is more user-friendly, but existing solutions typically rely on complex feature engineering, require large training datasets, and struggle to generalize across different users and environments. In this work, we introduce a novel passive tracking system that requires only a single-shot training phase. By leveraging location signature based on statistical proximity metrics derived from CSI across multiple distributed WiFi devices, our method enables accurate, scalable, and training-efficient indoor tracking.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {601–602},
numpages = {2}
}
@inbook{10.1145/3711875.3734564,
author = {Hossein Motlagh, Naser and Zaidan, Martha Arbayani and Fung, Pak Lun and Varjonen, Samu and Rebeiro-Hargrave, Andrew and Nurmi, Petteri and Tarkoma, Sasu},
title = {Poster: IoT-Based Indoor Air Quality Monitoring for Health Risk Assessment and Well-Being},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3734564},
abstract = {We study the impact of air pollutants on occupants' health by deploying 26 low-cost IoT air quality sensors in the UbiKampus office space at the University of Helsinki. Our data shows significant air quality variation even within meter-scale distances, highlighting the need for detailed indoor air quality monitoring and emphasizing the potential benefits low-cost IoT sensors can bring.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {603–604},
numpages = {2}
}
@inbook{10.1145/3711875.3734565,
author = {Li, Zhiying and Aziz, Abdul and Do, Patrick Phuoc and Nguyen, Phuc and Li, Tianxing},
title = {Poster: MobiChem: A Ubiquitous Smartphone-Based Toolkit for Practical Fruit Monitoring and Analysis},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3734565},
abstract = {This paper introduces MobiChem, a low-cost, portable, practical, and ubiquitous smartphone-based toolkit for fruit monitoring. The key idea is to leverage the light emitted from a smartphone's screen and front camera, coupled with a custom-built screen cover, to perform comprehensive hyperspectral analysis on targeted objects. Specifically, we designed a zero-powered screen cover that selectively filters wavelengths essential for hyperspectral sensing. We then incorporate a CNN-based algorithm and a novel ranking-based learning technique that manipulates the latent space to classify maturity stages and characterize their chemical and physical factors. We showcased its application in tomato, banana, and avocado sensing. Our system examines the maturity, chlorophyll, lycopene content, free sugar levels, and firmness, enabling various dietary assessments and food safety applications.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {605–606},
numpages = {2}
}
@inbook{10.1145/3711875.3734566,
author = {Benito-Calvino, Guillermo and Boubrima, Ahmed and Rivano, Herve and Renzaglia, Alessandro and Shaikhanov, Zhambyl},
title = {Poster: Joint RF-Gas Sensing for Victim Localization using UAV Networks},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3734566},
abstract = {Using UAVs has recently emerged as a cost-efficient solution to assist first responders in search and rescue missions. Victims are usually equipped with wireless devices, which makes RF sensing an efficient solution for their localization in disaster situations. Nevertheless, the success of existing methods is highly diminished by the noisy nature of RF measurements. While leveraging recent advancements in lightweight gas sensing, we present in this ongoing work paper a novel localization approach that efficiently combines RF measurements with victim odor information while accounting for the dynamic nature of measurements' quality. We discuss the approach design, early simulation results, and ongoing experimental evaluation.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {607–608},
numpages = {2}
}
@inbook{10.1145/3711875.3734567,
author = {Peng, Chuyang and Ali, Mutahar and Farrukh, Habiba},
title = {Poster: PeekXR: Understanding Privacy Leakages from Eye Gaze in Extended Reality},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3734567},
abstract = {Extended Reality (XR) headsets are increasingly integrating eye tracking for enhanced user experience, adaptive interfaces, and foveated rendering. However, this rich biometric signal introduces new privacy risks. In this work, we demonstrate that eye-tracking data collected by commercial XR devices can be exploited to infer sensitive user activity. We leverage users' gaze sequences captured while interacting with a VR app to classify the type of content a user is watching. Our results reveal that eye movements alone, without any video or audio context, carry enough information to accurately predict content categories. We discuss the implications of this threat and outline how eye tracking can potentially be used to fingerprint applications and user behavior. This work is a step towards exposing and mitigating emerging privacy threats in immersive systems.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {609–610},
numpages = {2}
}
@inbook{10.1145/3711875.3734568,
author = {Peng, Chen and Du, Jiaxin and Peng, Chunyi},
title = {Poster: SiHear (Sign-to-Hear): Empowering Seemless Sign-Speech Interactions over Smart Glasses},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3734568},
abstract = {Sign language is the primary communication channel used by deaf and hard-of-hearing (DHH) people. However, they face significant barriers in interacting with hearing people who are unfamiliar with sign language. In this poster, we propose SiHear, a system over wearable smart glasses to enable two-way, seamless and real-time communication between DHH and hearing individuals for daily use. SiHear supports two communication modes: (1) "I See You Hear" for translating the signs performed by the DDH people into speech which can be heard by the hearing people, and (2) "You See I Hear" for converting the spoken language by the hearing people into the sign language which can be seen by the DDH people on their smart glasses. We implement SiHear on commodity glasses each tethered with an Android phone and a laptop.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {611–612},
numpages = {2}
}
@inbook{10.1145/3711875.3734569,
author = {Elhadi, Said and Zhao, Yang},
title = {Poster: DNN Models for Underground Root Tuber Image Reconstruction using WiFi CSI},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3734569},
abstract = {Non-invasive monitoring of underground biomass like root tubers is vital for smart agriculture. We present a wireless sensing system using Wi-Fi Channel State Information (CSI) from a low-cost ESP32 mesh network for high-resolution underground tuber imaging. Our approach uses synchronized CSI data collection and deep neural network (DNN) models including UNet, FCN and DeepLabV3+ for image reconstruction. We describe the testbed, data processing, experiments and DNN models in this paper. Comparative results show DNN models using the CSI data significantly outperforms those using traditional RSSI data. The DeepLabV3+ model using CSI data achieves the best imaging accuracy with an IoU of 0.6971, demonstrating the potential of WiFi CSI for fine-grained underground tuber sensing.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {613–614},
numpages = {2}
}
@inbook{10.1145/3711875.3734571,
author = {Ji, Yu and Hsiao, I-Han},
title = {Poster: InstaPose: Dynamic Photography Feedback via Adaptive Pose Recommendation System},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3734571},
abstract = {Many pose recommendation applications have been proposed and studied, primarily focusing on providing exercise feedback and fine-tuning postures to enhance precision for skill acquisition. These applications often utilize pose extraction and identification to help users perfect their form, whether in yoga, fitness routines, or other sports activities. However, these specialized applications are not generalizable for leisure use, such as capturing a photogenic or novel pose in front of an iconic tourist attraction. To address this issue, we present InstaPose, a mobile-friendly framework that delivers real-time pose recommendation and correction feedback. The core of our approach leverages Mobile Vision Transformers (MobileViT) to retrieve refined pose images from our dataset by matching the background context of the input photo. In addition, the proposed model outperforms VGG16 on our newly curated, large-scale high-quality dataset. This demonstrates the potential of our approach for real-time mobile pose guidance.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {615–616},
numpages = {2}
}
@inbook{10.1145/3711875.3734573,
author = {Jeong, Hyemin and Lee, Jeho and Jeon, Seunghyeok and Cha, Hojung},
title = {Poster: Mixture of Class-aware Experts for Efficient AIoT Inference},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3734573},
abstract = {Deep neural networks (DNNs) have enabled a wide range of artificial intelligence of things (AIoT) applications, but their increasing complexity poses challenges for deployment on resource-constrained devices. Model compression techniques such as pruning and quantization have been widely adopted to address these challenges; however, they inevitably incur accuracy loss due to information loss. Recently, class-aware pruning has emerged as a promising approach, but existing methods often lack flexibility, as they are typically tailored to fixed target class sets and fail to generalize well to dynamic or broad class distributions. To address this limitation, we propose Mixture of Class-aware Experts (MoCE), a novel framework that combines class-aware pruning with a Mixture of Experts (MoE) architecture. MoCE constructs multiple lightweight experts using class-aware pruning, each specialized for a subset of classes, and employs a shared encoder and a lightweight router to dynamically select the appropriate expert at runtime. Our preliminary results demonstrate the potential of combining class-aware pruning and expert selection to enable accurate and efficient inference on resource-limited AIoT devices.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {617–618},
numpages = {2}
}
@inbook{10.1145/3711875.3734578,
author = {Amadi, Lawrence and Lu, Andrew and Chou, Chih-Hsien and Lu, Ning},
title = {Poster: Real-time Keyboard Segmentation and Finger-Press Detection for Keystroke Tracking in Virtual Keyboards},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3734578},
abstract = {Real-time recognition and localization of keyboard keys combined with real-time hand tracking and detection of finger-press actions are critical functions of vision-based virtual keyboards. These are not trivial tasks because there are numerous types of keyboard layouts and typist behaviors. Furthermore, key detection is especially difficult in uncontrolled real-world scenarios, where users' hands occlude significant portions of the keyboard while typing. Also, the detection finger press-down actions is further complicated in-the-wild by frequently changing camera viewpoints without direct line of sight to the finger pressing a key. In this work, we address the challenge of complete keyboard segmentation and detection of hand-occluded keys by proposing a deep learning approach for realtime finger-press detection, visible-key detection, and occluded-key recovery. Our models were trained and evaluated on Kaggle's Keyboard Key Detection dataset [1] and further empirically tested on the MSU Typing Behavior Database [3]. Our best models achieved 88\% finger press-down detection and a key detection performance of 0.91 IOU and 97.8\% mAP@75, while running at 60 fps.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {619–620},
numpages = {2}
}
@inbook{10.1145/3711875.3734370,
author = {Wang, Rongxiang and Lin, Felix Xiaozhu},
title = {Demo: WhisperFlow: speech foundation models in real time},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3734370},
abstract = {Speech foundation models, such as OpenAI's Whisper, become the state of the art in speech understanding due to their strong accuracy and generalizability. Yet, their applications are mostly limited to processing pre-recorded speech, whereas processing of streaming speech, in particular doing it efficiently, remains rudimentary.We present a novel framework, WhisperFlow, which embodies both model and system optimizations. (1) Hush word as a short, learnable audio segment; appended to a voice input, a hush word gracefully stops the speech model from processing more input without hallucination; (2) Beam pruning, which aligns streaming audio buffers over time and reuses results from earlier decoding rounds, therefore significantly accelerating decoding; and (3) CPU/GPU pipelining, which not only maps to the encoding/decoding stages dynamically, but also tunes to an optimal resource ratio, respecting the encoding/decoding speed that varies across voice inputs, models, and hardware.We demonstrate WhisperFlow on a Macbook pro with M4 pro SoC with 14 CPU cores and 20 GPU cores on real-world conversation transcription tasks. The WhisperFlow delivers high fidelity transcripts with the Whisper medium model, also maintains the per-word latency within 1 second.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {621–622},
numpages = {2}
}
@inbook{10.1145/3711875.3734371,
author = {Chen, Jingcan and Xu, Huatao and Li, Mo},
title = {Demo: CollabTrans: Device-cloud Collaborative Inference Framework for Transformer-based Models},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3734371},
abstract = {Model splitting and offloading part of the DNN model from the mobile device to the cloud server, known as collaborative inference, improves end-to-end latency and server throughput in CNN-based models. However, current collaborative approaches do not apply to popular Transformer-based models, as these models generate large intermediate outputs with significant transmission latency and have a uniform block structure that makes it challenging to serve tail models efficiently on the server. We propose CollabTrans, a collaborative inference framework designed for Transformer-based models to enhance server scalability when handling requests from numerous end devices, taming the large output with truncated SVD and serving heterogeneous tail models by sharing a complete model. We demonstrate our framework with a real-time image classification mobile application together with background inference request traffic, showing the high inference throughput of our framework.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {623–624},
numpages = {2}
}
@inbook{10.1145/3711875.3734372,
author = {Wang, Liujianfu and Long, Xinyi and Du, Yuyang and Liu, Xiaoyan and Chen, Kexin and Liew, Soung Chang},
title = {Demo: Cellular-X: An LLM-empowered Cellular Agent for Efficient Base Station Operations},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3734372},
abstract = {This paper introduces Cellular-X, an LLM-powered agent designed to automate cellular base station (BS) maintenance. Leveraging multimodal LLM and retrieval-augmented generation (RAG) techniques, Cellular-X significantly enhances field engineer efficiency by quickly interpreting user intents, retrieving relevant technical information, and configuring a BS through iterative self-correction. Key features of the demo include automatic customized BS setup, document-based query answering, and voice-controlled configuration reporting and revision. We implemented Cellular-X on a USRP X310 testbed for demonstration. Demo videos and implementation details are available at https://github.com/SeaBreezing/Cellular-X.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {625–626},
numpages = {2}
}
@inbook{10.1145/3711875.3734373,
author = {Sen, Tanmoy and Basak, Madhusudan and Dev, Himel and Linden, Michael},
title = {Demo: FlyEnJoy: An Offline-First Mobile System for Trip Planning and In-Flight Social Interaction},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3734373},
abstract = {Air travel often lacks cohesive digital tools to support both pre-trip organization and engaging in-flight experiences. We present FlyEnJoy, a mobile application with an offline-first architecture that integrates itinerary planning management, and in-flight social features to improve travel satisfaction. The system highlights local peer-to-peer networking for realtime in-flight interaction without the Internet and context-aware smart planning tools that adapt to each trip. This demo paper outlines the design and technical components of FlyEnJoy, including its offline-first design, peer-to-peer networking and context-aware smart planning approach. We also discuss insights from a traveler survey that motivated the need for such a platform and conclude with a demonstration that showcases the capabilities of FlyEnJoy in a live scenario.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {627–628},
numpages = {2}
}
@inbook{10.1145/3711875.3734375,
author = {Yang, Qirui and Xu, Huatao and Song, Mengxuan and Li, Mo},
title = {Demo: WiMU: Real-time Indoor Localization via Wi-Fi/IMU Fusion with Minimal Site Survey},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3734375},
abstract = {Due to the ubiquitous deployment of WiFi infrastructure, numerous studies have employed WiFi RSSI fingerprinting for indoor localization. However, fingerprinting methods necessitate labour-intensive site surveys for fingerprint collection and location annotation. To address these limitations, we propose WiMU, a real-time indoor localization system that integrates WiFi and inertial measurement unit (IMU) data to enhance the real-time performance and accuracy of localization. WiMU operates on commodity WiFi infrastructure without the need for additional hardware, leveraging crowd-sourced user trajectories to learn spatial representations of access points (APs). These representations can be fine-tuned with minimal labeled data to support effective localization. Extensive evaluations demonstrate that WiMU reduces the cost of building an indoor localization system while ensuring high positioning accuracy, paving the way for the large-scale deployment of real-time indoor localization systems.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {629–630},
numpages = {2}
}
@inbook{10.1145/3711875.3734376,
author = {Liang, Bo and Peng, JingZhe and Liu, Xingyuming and Gong, Chen and Xu, Chenren},
title = {Demo: Liquid Identification via Vision-Guided mmWave Imaging and LLM Reasoning},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3734376},
abstract = {We introduce ErLang Sight, a novel multimodal system designed for liquid identification, integrating vision-based object detection, millimeter-wave (mmWave) Synthetic Aperture Radar (SAR) imaging, and large language model (LLM)-based contextual reasoning. Initially, the system leverages a visual detection pipeline to identify potential liquid containers within the environment, subsequently directing a mmWave sensor to perform targeted SAR imaging of these identified regions, and the permittivity values of the liquids are estimated using reflection coefficient analysis techniques. These physical measurements, combined with visual context and environmental indicators (such as whether the scenario is a kitchen, laboratory, or bar), are then input into a pretrained LLM. The LLM employs advanced semantic and situational reasoning to accurately determine the most likely type of liquid by integrating physics-based data with contextual knowledge. Experimental evaluations demonstrate that ErLang Sight significantly enhances the accuracy of distinguishing visually ambiguous liquids and exhibits robust generalization to previously unseen environments.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {631–632},
numpages = {2}
}
@inbook{10.1145/3711875.3734378,
author = {Hatch, Matthew and Fellinge, Cody and Diller, Jonathan and Han, Qi},
title = {Demo: ROARQuad: Robust, Open Academic Research Quadcopter},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3734378},
abstract = {Commercial off-the-shelf (COTS) unmanned aerial vehicles (UAV) are undesirable for academic research due to the difficulty of modification and the inability to readily program missions from a companion computer. Furthermore, open-source designs are scarce and unreliable. To address this gap, we present ROARQuad - a Robust and Open source Academic Research Quadcopter. Our quadcopter costs around $1,500, provides over thirty minutes of flight time, and has a maximum payload of nearly 2 kilograms. To support research into swarm coordination and planning, we have experimentally derived the power consumption profile of the vehicle. Our quadcopter not only addresses the limitations of existing COTS and academic designs but also provides a flexible and reliable solution for future UAV research and development.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {633–634},
numpages = {2}
}
@inbook{10.1145/3711875.3734379,
author = {Chugh, Snehalraj and Polyakov, Elijah and Rampure, Milind and Basnyat, Bipendra and Roy, Nirmalya},
title = {Demo: InvisibleFence: Non-Lethal Edge-Optimized AI for Human Wildlife Coexistence and Crop Protection},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3734379},
abstract = {Human-wildlife conflicts in residential/agricultural settings rely on ineffective deterrents like rodenticides or fences. We introduce InvisibleFence, a modular 3D-printed Vision Pod system with off-the-shelf deterrents. The Vision Pod fuses a 2K camera and 240° motion sensing with an edge-optimized pipeline trained on 44,000 wildlife images of eleven classes—achieving 86.7\% mAP. Benchmarking YOLO variants (416p–2K) ensures performance. Upon detection, it sends MQTT commands to drive deterrent units—ultrasonic speakers or lighting/spray modules—that emit tones without affecting humans or pets. InvisibleFence creates adaptive zones that reduce false triggers and limit habituation.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {635–636},
numpages = {2}
}
@inbook{10.1145/3711875.3734380,
author = {Chinthalapani, Rajashekar Reddy and Shah, Dhairya and Varshney, Ambuj},
title = {Demo: Enabling Ubiquitous Connectivity for Embedded Systems through Audio-Broadcasting Low-power Tags},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3734380},
abstract = {Wireless connectivity challenges hinder large-scale deployment of embedded systems. We introduce AudioCast to address two critical issues: spectrum scarcity-induced contention and high power consumption of radio transmitters. The decline of FM-broadcast stations and the ubiquity of FM-receivers motivate the design of Audio-Cast. By leveraging the negative differential resistance of tunnel diodes—which occurs at low power—AudioCast rethinks the conventional transmitter design. Combined with their self-modulation capability, tunnel diode oscillators enable frequency modulated transmissions while consuming under 200 μW. The transmitter achieves upto 130 m range in line-of-sight and tens of meters in non-line-of-sight environments.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {637–638},
numpages = {2}
}
@inbook{10.1145/3711875.3734381,
author = {Mukherjee, Chandrika and Aburas, Reham Mohamed and Arunasalam, Arjun and Farrukh, Habiba and Celik, Z. Berkay},
title = {Demo: UI Based Attacks in WebXR},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3734381},
abstract = {The WebXR API enables immersive AR/VR experiences directly through web browsers on head-mounted displays (HMDs). However, prior research shows that security-sensitive UI properties and the lack of an <iframe> like element that separates different origins can be exploited to manipulate user actions, particularly within the advertising ecosystem. In our prior work, we proposed five novel UI-based attacks in WebXR, targeting the ad ecosystem. This demo presents these attacks in a unified gaming application, embedding each into distinct interactive scenarios. Our work highlights the need to address design challenges and requirements for improving immersive web-based experiences. We provide our demo video at: https://youtu.be/lTBQbxnNq34.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {639–640},
numpages = {2}
}
@inbook{10.1145/3711875.3734382,
author = {Bogosian, Biayna and Wu, Ying Choon and Nassa, Akshit and Jalali, Daniel and Yenney, Jacob},
title = {Demo: Balboa Park Alive!: Exploring Biodiversity through Mobile AR},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3734382},
abstract = {Balboa Park Alive is a mobile augmented reality (AR) application that transforms outdoor exploration into an immersive educational experience centered on biodiversity and conservation. Designed for families and informal learners, the app uses gesture-based AR to connect users with ecological phenomena in the San Diego-Tijuana region. Integrating Niantic Lightship ARDK, Mapbox SDK, and MediaPipe Hands, the system creates inquiry-driven moments that engage users through embodied interaction, geospatial storytelling, and environmental awareness. Early user testing suggests the app fosters curiosity, intergenerational dialogue, and stewardship, offering a replicable model for mobile, location-based environmental education.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {641–642},
numpages = {2}
}
@inbook{10.1145/3711875.3734384,
author = {Cifuentes-Urtubey, Federico and Vasisht, Deepak and Kravets, Robin},
title = {Demo: Unveiling Randomized Device Identity in Dynamic Wi-Fi Networks},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3734384},
abstract = {This work presents an attack to undermine MAC address randomization using a dual-layer approach combining PHY and MAC-layer analysis. By passively collecting CSI features along with MAC-layer behavioral patterns, our system applies an XGBoost classifier to distinguish devices enabling MAC randomization in dense networks, achieving up to 100\% precision and 86\% recall.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {643–644},
numpages = {2}
}
@inproceedings{10.1145/3711875.3736658,
author = {Xiao, Rui},
title = {Exploiting and Securing WiFi for Pervasive Human Sensing},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3736658},
doi = {10.1145/3711875.3736658},
abstract = {In recent years, the proliferation of WiFi-connected devices and related research has led to novel techniques of utilizing WiFi as sensors, i.e., inferring human activities through channel state information (CSI) perturbations. While this enables passive human sensing, it also introduces privacy risks from leaked WiFi signals that attackers can intercept, leading to threats like adversarial motion sensing. My research focuses on enhancing both the utility and security of WiFi sensing by addressing four key challenges: (1) lack of a secure sensing platform that also minimizes interference with normal WiFi communication, (2) environmental ambiguity caused by multipath effects, (3) confined sensing distance due to signal attenuation, and (4) limited scalability across different activities and users. I demonstrate solutions to these challenges through OneFi, a scalable WiFi sensing framework, and LeakyBeam, an adversarial WiFi sensing attack capable of long-range motion sensing. Finally, I discuss open problems and future challenges in achieving robust, scalable, and privacy-preserving WiFi sensing.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {645–646},
numpages = {2},
keywords = {wireless security, wifi sensing, wireless sensing, beamforming feedback information, BFI sensing},
location = {Hilton Anaheim, Anaheim, CA, USA},
series = {MobiSys '25}
}
@inproceedings{10.1145/3711875.3736659,
author = {Adhikari, Abhishek},
title = {mmWave Communications, Sensing, and Interference Mitigation},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3736659},
doi = {10.1145/3711875.3736659},
abstract = {The 5G mmWave rollout has provided users with a subpar experience in obtaining coverage on their mobile devices. The reason for this is that mmWave operates at a high frequency, where the signal propagation attenuates rapidly. This is purely a physics-based constraint of high path loss, and it is the core reason for why mmWave deployments have somewhat failed commercially. My research focuses on obtaining a real-world understanding of mmWave signal propagation by collecting some of the largest and most diverse real-world urban measurement campaigns to date. These measurements are collected in the COSMOS FCC Innovation Zone within Manhattan, New York City. I work with Nokia Bell Labs to measure mmWave propagation through glass, around corners, and over tops of buildings. I also have started to investigate the future potential of using mmWave backscatter as a form of opportunistic sensing, as well as some of the unintended consequences of deploying 5G mmWave on highly sensitive scientific measurement instruments.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {647–648},
numpages = {2},
keywords = {mmWave, channel sounding, phased arrays, software-defined radio, integrated sensing and comunication, RF interference},
location = {Hilton Anaheim, Anaheim, CA, USA},
series = {MobiSys '25}
}
@inproceedings{10.1145/3711875.3736660,
author = {Guo, Dongfang},
title = {Time-Resolved Designs for Narrowband Radio Localization and Vehicular Visual Sensing Compromise},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3736660},
doi = {10.1145/3711875.3736660},
abstract = {The Internet of Things relies heavily on accurate and reliable sensing for applications. Time-resolved sensing is a paradigm that leverages fine-grained temporal analysis of sensor signals, and the intrinsic relationship between space and time, to extract rich spatial and contextual information. This concept expands the sensing capabilities in IoT applications, but also introduces new challenges in synchronization, processing efficiency, and security. In my research, I explore both the constructive and adversarial aspects of time-resolved designs, highlighting their dual role in improving the capabilities of IoT systems and exposing potential security vulnerabilities. Specifically, I study how time-resolved radio signals can be carefully processed to achieve indoor localization under narrow signal bandwidth constraints. I also reveal how time-resolved camera-based visual sensing can be exploited to affect vehicular perception.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {649–650},
numpages = {2},
keywords = {time-resolved sensing, internet of things, narrowband radio localization, vehicular visual sensing},
location = {Hilton Anaheim, Anaheim, CA, USA},
series = {MobiSys '25}
}
@inproceedings{10.1145/3711875.3736661,
author = {Srinivasavaradhan, Varshika},
title = {Measuring the Mobile Gap: From Network Performance to Infrastructure Criticality},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3736661},
doi = {10.1145/3711875.3736661},
abstract = {Mobile networks are evolving rapidly, with 5G promising transformative gains in speed, reliability, and latency. However, these advancements do not always translate to improved user experience, due to device limitations, congestion, and inconsistent infrastructure deployment. This disconnect between network capability and actual user experience raises key questions about the design, measurement, and investment in mobile broadband systems. My research addresses this gap through the use of edge user measurements of mobile broadband performance. These measurements predominantly consist of crowdsourced network performance measurements and controlled experiments. Broadly, I develop statistical methods to analyze real-world performance variability and also identify performance bottlenecks, and utilize insight from my analyses to inform infrastructure planning and policy, helping ensure that technological advances lead to more consistent and equitable user outcomes. This extended abstract presents results from recent analyses, outlines my methodological approach, and discusses future directions aimed at advancing both technical understanding and policy design in the mobile broadband landscape.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {651–652},
numpages = {2},
location = {Hilton Anaheim, Anaheim, CA, USA},
series = {MobiSys '25}
}
@inbook{10.1145/3711875.3736662,
author = {An, Zhenlin},
title = {Physics-Informed AI for Wireless Communication and Sensing},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3736662},
abstract = {Deep learning models encounter fundamental challenges when directly applied to wireless systems, including limited interpretability, low data efficiency, and poor adaptability in dynamic environments. To overcome these challenges, this extended abstract presents my research on embedding electromagnetic physics into AI frameworks to create physics-informed models for wireless communication and sensing. These hybrid models leverage the structure of physical laws alongside data-driven learning, leading to substantial improvements in accuracy, generalization, interpretability, and robustness across complex scenarios. My work demonstrates how this methodology reforms wireless systems across multiple key applications: channel prediction, indoor localization, antenna design, and hardware fingerprinting. Together, these efforts establish a new paradigm for next-generation wireless system design.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {653–654},
numpages = {2}
}
@inbook{10.1145/3711875.3736663,
author = {Yang, Qiang},
title = {Ubiquitous Acoustic Intelligence: Toward Intuitive, Resilient, and Secure Mobile Sensing Systems},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3736663},
abstract = {Ubiquitous acoustic intelligence leverages the pervasiveness of sound and the ubiquity of microphones/speakers to enable intelligent, seamless, and privacy-preserving interaction between humans and mobile devices. However, enabling such intelligence poses multiple challenges: human voice and behavior manifest in acoustics in subtle, complex ways that are difficult to capture robustly; acoustic communication suffers from significant distortion in complex environments and cross-heterogeneous devices; and processing audio data requires careful design to ensure privacy and efficiency. To address these challenges, we developed a series of systems across three pillars: intelligent acoustic sensing, robust acoustic communication, and privacy-aware acoustic computing.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {655–656},
numpages = {2}
}
@inbook{10.1145/3711875.3736664,
author = {Cai, Dongqi},
title = {Federated LLM Pre-Training on Mobile Phones},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3736664},
abstract = {Over the past decades, billions of mobile phones have become the primary interface to the Internet, accessing vast amounts of private user data. These devices are idle most of the time or become obsolete after a few years. Pre-training Large Language Models (LLMs) on ubiquitous mobile phones offers a promising way to utilize both private data and idle computing power. In this work, we propose the first federated LLM pre-training framework for mobile devices and demonstrate that it can potentially achieve wall-clock training time comparable to centralized pre-training.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {657–658},
numpages = {2}
}
@inbook{10.1145/3711875.3736665,
author = {Chinthalapani, Rajashekar Reddy},
title = {Beyond Backscatter: Rethinking Low-Power Wireless Communication to Tackle the Energy Challenge of Embedded Systems},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3736665},
abstract = {Wireless communication remains the most energy-intensive component of embedded systems, often limiting their deployment in power-constrained or hard-to-reach environments. While backscatter communication offers energy efficiency by offloading carrier generation, it continues to face practical limitations, hindering widespread adoption. In this paper, we propose going beyond backscatter architecture that re-imagines both the transmitter and receiver using ultra-low-power components, such as tunnel diodes. Our approach eliminates the dependence on external emitters for transmission and moves beyond envelope detectors on the receiver side, resulting in significantly improved sensitivity and communication range.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {659–660},
numpages = {2}
}
@inproceedings{10.1145/3711875.3736666,
author = {Guo, Yunqi},
title = {Assistive AR System for Enhancing Human-Human and Human-Environment Interactions},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3736666},
doi = {10.1145/3711875.3736666},
abstract = {Augmented Reality (AR) smart glasses promise transformative enhancements to human interactions and environmental awareness, yet face substantial challenges, including limited computational resources, constrained sensing capabilities, and inadequate intuitive interfaces. We introduce Assistive AR, a new category of AR systems designed explicitly for assistance to people's everyday lives, supporting both human-human and human-environment interactions through human-like sensing, efficient computation, and intuitive presentations. Specifically, Sign-to-911 addresses communication barriers for deaf users during emergencies by providing real-time bidirectional Sign Language translation on the AR system. SocialMind enhances social interactions using proactive large language models (LLMs) to interpret and suggest responses based on conversational context. Further, Sensor2Scene and Vivar utilize generative models to visualize complex environmental sensor data to enhance the human-environment interaction. Moving forward, we will provide a general and scalable framework called MASG-MCP that enables users to create and customize tools on the Assistive AR system through a unified LLM agent.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {661–662},
numpages = {2},
keywords = {assistive AR, accessibility, mobile computing, augmented reality, human-computer interaction, large language models},
location = {Hilton Anaheim, Anaheim, CA, USA},
series = {MobiSys '25}
}
@inproceedings{10.1145/3711875.3736667,
author = {Li, Yijie},
title = {Multi-user Intelligent Personalized Acoustic Field Manipulation},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3736667},
doi = {10.1145/3711875.3736667},
abstract = {Personalized acoustic field manipulation enables tailored audio experiences in scenarios like multi-user immersive environments and acoustic augmented reality. However, conventional systems rely on headphones or bulky speaker arrays, which pose significant limitations in terms of user comfort, mobility, and practicality. Despite advances in the field, there remains a critical gap in achieving device-free, compact, and flexible personalized sound field control system for multiple users. To address the challenges, our work explores the feasibility of achieving audible sound field manipulation and integrates directional sensing through air nonlinearity while maintaining device-free and compact size. Specifically, we designed and implemented a series of novel systems to enable functionalities including multi-user directional audio delivery, independent audible zones construction, and multi-user simultaneous sensing and communication. These solutions enhance personalized audio experiences in shared spaces and open new possibilities for future applications in both immersive and interactive environments.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {663–664},
numpages = {2},
keywords = {acoustic field manipulation, air nonlinearity, acoustic sensing and communication},
location = {Hilton Anaheim, Anaheim, CA, USA},
series = {MobiSys '25}
}
@inbook{10.1145/3711875.3736668,
author = {Wang, Rongxiang},
title = {Redesigning Mobile Systems for Foundation Models with model- and system-level orchestration},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3736668},
abstract = {Transformer-based speech and language models deliver high-quality transcription and context-aware responses but require significant resources, complicating on-device deployment. Our work aims to build efficient mobile systems for real-time, accurate on-device model processing through system- and runtime-level innovations, thus eliminating cloud dependency and enhancing privacy.We address three main challenges: inefficient model execution pipelines causing temporal load imbalance, serving systems that do not fully exploit the heterogeneous compute and memory hierarchies of modern devices, and the overlooked potential of legacy devices as collective computational assets.To overcome these issues, we introduce optimizations such as pilot inference, hush word padding, and beam pruning, along with CPU/GPU pipelining for the first two challenges. These approach improves performance, reduces latency, and enables privacy-preserving speech understanding directly on personal devices as the native interface of the mobile devices. Additionally, we plan to develop scalable systems that orchestrate legacy phones to support more demanding models and applications, ultimately forming in-house clusters to democratize access to state-of-the-art models and promote a sustainable future.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {665–666},
numpages = {2}
}
@inproceedings{10.1145/3711875.3736669,
author = {Yang, Kang},
title = {Scalable and Adaptive RF Signal Propagation Modeling for Next-Generation Wireless Systems},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3736669},
doi = {10.1145/3711875.3736669},
abstract = {Next-generation wireless technologies, such as IEEE 802.11be/bf and Sixth Generation (6G), are designed to support both communication and sensing, enabling a broad range of applications powered by deep learning (DL) models. These models require large-scale, labeled Radio Frequency (RF) datasets, making data collection a bottleneck. Recent Neural Radiance Fields (NeRF)-based RF propagation models offer a promising solution by synthesizing RF signals at arbitrary transceiver positions within complex 3D environments, achieving high-fidelity RF data generation. However, these methods face key limitations in scalability and temporal adaptation, limiting their applicability in real-world settings. This work addresses these challenges by introducing an RF propagation modeling framework that is both scalable and adaptable to dynamic temporal variations, enabling practical next-generation wireless systems.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {667–668},
numpages = {2},
keywords = {RF signal propagation modeling, neural radiance fields, 3D gaussian splatting},
location = {Hilton Anaheim, Anaheim, CA, USA},
series = {MobiSys '25}
}
@inbook{10.1145/3711875.3736670,
author = {Chen, Bo},
title = {Intelligent Network Infrastructure for Extended Reality},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3736670},
abstract = {This extended abstract outlines our research on designing intelligent network infrastructures for Extended Reality (XR). The research aims to leverage Artificial Intelligence (AI) in overcoming the limitations of traditional network infrastructures for XR in discrete content representation, handcrafted compression, and redundancy-based system resilience. To build a practical AI-based XR system, we introduce our AI-system co-design methodology, featuring system optimizations driven by AI models' in-depth measurements and AI algorithms inspired by the XR system's context analysis. We design and implement systems that showcase AI-system co-design advances photo-realism, efficiency, and resilience of network infrastructures for XR practically.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {669–670},
numpages = {2}
}
@inbook{10.1145/3711875.3736671,
author = {Ling, Neiwen},
title = {Time-sensitive AI System for Physical Agents},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3736671},
abstract = {The integration of Artificial Intelligence (AI) into physical agents, such as drones, robots, and autonomous vehicles, demands not only intelligence but also timely perception and decision-making. This work presents a time-sensitive AI system architecture that addresses the critical timing challenges inherent in such intelligent physical agents. We propose system-level designs that enable deadline-aware Deep Neural Network (DNN) execution on edge/embedded platforms, along with time-sensitive Large Language Model (LLM) serving on edge servers, thereby providing end-to-end support for time-critical physical intelligence.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {671–672},
numpages = {2}
}
@inbook{10.1145/3711875.3736672,
author = {Ghoshal, Moinak},
title = {Dissecting 5G in the Wild: Performance, Coverage, and Support for Next-Gen Applications},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3736672},
abstract = {5G, officially rolled out in 2019, has been rapidly deployed by major mobile network operators across the globe. Key advancements in 5G such as higher-order modulation, massive MIMO, wider channels, and higher operating frequencies, have set high expectations over the predecessor technology LTE, especially for enabling high-bandwidth and latency-sensitive applications such as Mixed Reality (XR), Connected Autonomous Vehicles (CAVs), and 360° video streaming — often referred to as 5G "killer" apps. While 5G's technical promises have fueled widespread excitement, understanding its true impact requires a closer look at the real-world performance delivered to end users.This abstract highlights our ongoing efforts over the past four years to systematically evaluate the performance and capabilities, and uncover different operator policies of operational 5G networks through extensive out-in-the-wild measurement campaigns. We examine network coverage, throughput, latency, and emerging features, as well as 5G's interaction with the "killer" apps. Our findings reveal both the strengths and limitations of current 5G deployments. Building on these insights, we also propose and implement solutions aimed at addressing key challenges, with an eye toward shaping the design of future 5G and Beyond-5G (B5G) systems.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {673–674},
numpages = {2}
}
@inbook{10.1145/3711875.3736673,
author = {Xu, Huatao},
title = {Building Generalizable Deep Learning Solutions for AIoT Applications},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3736673},
abstract = {In an era where embedded and mobile devices are becoming ubiquitous, the intersection of Artificial Intelligence and the Internet of Things (AIoT) is rapidly transforming various fields. This paper delves into the challenges and innovations in this domain, particularly focusing on the development of generalized sensing models for wearable sensor data. We introduce novel approaches to leverage the abundant unlabeled sensor data, physical sensing knowledge, and common knowledge embedded in Large Language Models (LLMs) to enhance the generalizability of AIoT models significantly.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {675–676},
numpages = {2}
}
@inbook{10.1145/3711875.3736674,
author = {Ding, Jian},
title = {Commodity Hardware for Scalable RAN Infrastructure and Agricultural Sensing},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3736674},
abstract = {Next-generation wireless networks promise massive connectivity with higher data rates and lower latency, offering transformative potential for agriculture by enabling smarter, more efficient, and sustainable farming practices. However, the advancement of wireless infrastructure and precision agriculture is constrained by the need for specialized, costly hardware. Our research addresses this challenge by designing systems that leverage general-purpose commodity hardware for both radio-access network (RAN) infrastructure and soil monitoring. By utilizing existing servers at edge and cloud data centers, as well as ubiquitous Wi-Fi devices, we can eliminate the need for expensive, dedicated equipment, reducing costs and accelerating the evolution of wireless and agricultural sensing technologies.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {677–678},
numpages = {2}
}
@inbook{10.1145/3711875.3736675,
author = {Ren, Yidong},
title = {Toward Reliable and Scalable LoRa Networking for Rural IoT},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3736675},
abstract = {The Internet of Things (IoT) is revolutionizing our interaction with the physical world, significantly enhancing precision agriculture, infrastructure monitoring, forest fire prevention, environmental protection, and numerous other applications in rural areas. Deploying reliable and scalable wireless connectivity in rural areas is critical, but it presents distinct technical and economic challenges. Local area wireless networks, such as Wi-Fi and Bluetooth, provide limited coverage and consume high power. They are not suitable for rural scenarios. Cellular technologies like LTE and 5G offer wide-area coverage but involve high infrastructure costs and power consumption. Recently, Low-Power Wide-Area Networks (LPWANs) have become promising alternatives. They address rural IoT needs by offering long-range communication, low power consumption, and reduced deployment costs. Among LPWAN technologies, LoRa stands out due to its robustness, scalability, and affordability.In a typical LoRa network architecture (Node-Gateway-Server), sensor nodes collect data and transmit via LoRa radio signals to nearby gateways (fronthaul). Gateways subsequently relay the aggregated data to cloud servers through backhaul connections, typically using cellular networks or wired links. However, deploying LoRa in real rural environments presents unique and significant challenges, as shown in Table 1.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {679–680},
numpages = {2}
}
@inbook{10.1145/3711875.3736676,
author = {Yang, Yongjie},
title = {Democratizing Earable Computing via Hardware-Software Co-design},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3736676},
abstract = {Many of us have low-cost earphones that are often forgotten in drawers or discarded as electronic waste. Instead of letting billions of these earphones go to waste, can we transform them into smart earable devices—creating a cost-effective alternative, promoting sustainability, and ultimately democratizing Earable Computing? To answer this question, this paper presents a five-year research roadmap based on a hardware-software co-design approach, with the goal of repurposing low-cost earphones as powerful human-centric sensing devices. We begin with circuit design, develop adaptive signal processing algorithms, and ultimately integrate these efforts into a low-cost, user-friendly earable platform that pushes the boundaries of earable computing field.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {681–682},
numpages = {2}
}
@inbook{10.1145/3711875.3736677,
author = {Lee, Jeho},
title = {Towards Accurate, Adaptive, and Real-time Machine Perception on Resource-constrained Platforms},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3736677},
abstract = {Accurate, real-time machine perception is a key enabler of emerging mobile applications such as augmented reality and autonomous driving. However, running complex vision models within the tight latency budgets of resource-limited platforms remains challenging. We address two root causes: (i) the growing computational demands of state-of-the-art vision models and (ii) the variability of compute resource availability in on-device AI deployments. In this extended abstract, we introduce two adaptive perception systems that leverage AI-system co-design. Deployed on commercial devices and evaluated on representative perception workloads, our systems demonstrate high-performance perception under practical latency and resource constraints.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {683–684},
numpages = {2}
}
@inbook{10.1145/3711875.3736678,
author = {Lee, Chae Young},
title = {Ultra-Lightweight Edge Intelligence Using Vector Symbolic Architecture},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3736678},
abstract = {Edge intelligence greatly benefits Internet-of-Things devices by providing insights at the sensor node without transmitting the data to and from the cloud. Although recent work has scaled deep neural networks down to microcontrollers, these solutions remain impractical for devices operating under strict energy budgets—such as battery-powered camera traps in remote forests or micro-robots in disaster zones. My work addresses that gap by exploring a brain-inspired Vector Symbolic Architecture (VSA) over conventional neural networks. I introduce HyperCam for low-power image classification, NavHD for micro-robot navigation, and Omen for dynamic optimization during inference. These methods each optimize VSA's encoding and training algorithms to achieve meaningful speedups and energy savings. Ongoing work seeks to demonstrate these advances as real-world systems, where VSA can extend the lifespan of intelligent systems in remote or energy-constrained environments.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {685–686},
numpages = {2}
}
@inbook{10.1145/3711875.3736680,
author = {Cho, Kun Woo},
title = {Hardware-Software Co-Design for Programmable Smart Radio Environments},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3736680},
abstract = {In this abstract, we introduce the concept of a programmable smart radio environment, which can be controlled and tuned by software to reconfigure itself in real time based on application needs. We design programmable smart surface systems and deploy them on buildings and vehicles to physically control radio environments. Our ideas are implemented as hardware-software systems, integrated into city-scale wireless testbeds alongside existing network protocols, and validated through rigorous experimental evaluation.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {687–688},
numpages = {2}
}
@inbook{10.1145/3711875.3736681,
author = {Chhaglani, Bhawana},
title = {Privacy-Aware Ambient Audio Sensing for Healthy Indoor Spaces},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3736681},
abstract = {Indoor airborne transmission poses a significant health risk, yet current monitoring solutions are invasive, costly, or fail to address it directly. My research explores the untapped potential of ambient audio sensing to estimate key transmission risk factors such as ventilation, aerosol emissions, and occupant distribution—non-invasively and in real time. I develop privacy-preserving systems that leverage existing microphones to monitor the whole spectrum of indoor air quality which can have a significant effect on an individual's health. This work lays the foundation for privacy-aware airborne risk monitoring using everyday devices.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {689–690},
numpages = {2}
}
@inbook{10.1145/3711875.3736682,
author = {Zhou, Hao},
title = {Rethinking Inexpensive Wearables in the Era of AI: From Motion Analytics to Mobile Health},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3736682},
abstract = {Since their emergence, wearables have been widely proposed and utilized for counting steps, monitoring heart rate, and enhancing user interactions, suggesting their growing importance in both research and commercial domains. However, despite their ubiquity, the application scope of current wearables remains limited, largely due to an insufficient understanding of sensor signals. Mainstream devices such as smartphones and smartwatches predominantly support basic functionalities like human activity recognition and indoor navigation. However, enabling more advanced applications necessitates a deeper understanding of sensor signals within these wearables. Without such insight, these applications often depend on specialized, high-end hardware or fail to generalize across diverse conditions. For instance, sign language recognition, which requires fine-grained finger motion tracking, typically depends on bulky sensor gloves, vision-based systems that are expensive and intrusive, or wireless signal based systems that require specialized hardware, making them unsuitable for daily use. Similarly, while current smartwatches/rings can estimate heart rate, advanced health markers such as cardiac output, which offer insights into cardiac dysfunction, acute stress responses, and cognitive decline, are limited to specialized devices that cost thousands of US dollars.My Ph.D. thesis aims to unleash the potential of low-cost wearables by efficiently understanding signals with artificial intelligence (AI). Through novel software and hardware designs, I develop systems traditionally limited to expensive or impractical hardware, expanding the boundaries of what inexpensive wearables can do and making these advanced functionalities generalizable and seamlessly accessible to the general public.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {691–692},
numpages = {2}
}
@inbook{10.1145/3711875.3737658,
author = {Du, Jiaxin and Peng, Chunyi},
title = {δ-ATP: Towards Fast and Reliable Aerial Traffic Patrol},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3737658},
abstract = {In this work, we present our efforts to develop δ-ATP, a working system for fast and reliable traffic patrol using drones in the sky. Compared to the state-of-art approaches, δ-ATP improves accuracy and responsiveness in detecting speeding incidents by focusing only on essential "delta" (δ) information - regions of interest (say, roads) and key frames needed for speed estimation. We have implemented and evaluated δ-ATP in real-world experiments, successfully detecting all the speeding vehicles and outperforming all other approaches with a higher accuracy for real-time traffic enforcement. The datasets collected in our field test are available at [7].},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {693–698},
numpages = {6}
}
@inbook{10.1145/3711875.3737659,
author = {Cauwe, Dorian and Wang, Chenyang and Han, Qi},
title = {GazeboNS3: A Digital Twin System for UAV Swarms},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3737659},
abstract = {Unmanned Aerial Vehicles (UAVs) have advanced significantly, with multi-UAV systems offering advantages such as redundancy, cost-effectiveness, and less dependency on infrastructure. These systems can benefit applications like remote surveillance, search and rescue, and environmental monitoring. Ideally, UAVs should behave distributively and make their own decisions. However, developing these distributed algorithms for multi-UAV systems presents challenges, particularly in real-world testing, where hardware constraints, environmental dependencies, and limited analytics can hinder the development progress. Simulations have emerged as a key solution, allowing for rapid iteration and detailed analytics. Yet, current simulators often lack the capability to accurately model both the physical dynamics of UAVs and the complexity of their communication networks. This paper introduces GazeboNS31, a Digital Twin system designed to bridge this gap. Specifically, we combine the Gazebo physics simulator, the NS-3 network simulator, the Ardupilot's flight controller, and our empirical measurements of energy consumption and computation time on various commonly used onboard computers. Experiment results show that GazeboNS3 enables seamless emulation of UAVs' behavior and communication in realistic environments.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {699–704},
numpages = {6}
}
@inbook{10.1145/3711875.3737660,
author = {Guo, Dongfang and Tan, Rui},
title = {Rolling in the Deep: Exploiting Rolling Shutter Effect Against Stereo Depth Estimation in Drones},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3737660},
abstract = {Stereo vision plays a critical role in enabling depth perception for drones, supporting navigation and obstacle avoidance in complex environments. However, the robustness and security of stereo vision systems remain largely underexplored. In this paper, we propose Rolling in the Deep (RiD), a novel physical attack that exploits the rolling shutter effect (RSE) to inject imperceptible, structured perturbations into stereo image pairs. We analyze RSE formation in binocular camera setups and show how RSE-based perturbations can degrade deep learning-based stereo matching by exploiting model vulnerabilities and sensor misalignments, resulting in incorrect depth estimation. Preliminary results show the feasibility of RiD under realistic stereo configurations, revealing a new class of threats to drone perception systems.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {705–710},
numpages = {6}
}
@inbook{10.1145/3711875.3737661,
author = {Kim, Heegyeong and James, Alice and Seth, Avishkar and Kuantama, Endrowednes and Williamson, Jane and Feng, Yimeng and Han, Richard},
title = {Continuous Marine Monitoring via Autonomous UAV Handoff},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3737661},
abstract = {This paper introduces an autonomous UAV vision system for continuous, real-time tracking of marine animals, specifically sharks, in dynamic marine environments. The system integrates an onboard computer with a stabilised RGB-D camera and a custom-trained OSTrack pipeline, enabling visual identification under challenging lighting, occlusion, and sea-state conditions. A key innovation is the inter-UAV handoff protocol, which enables seamless transfer of tracking responsibilities between drones, extending operational coverage beyond single-drone battery limitations. Performance is evaluated on a curated shark dataset of 5,200 frames, achieving a tracking success rate of 81.9\% during real-time flight control at 100 Hz, and robustness to occlusion, illumination variation, and background clutter. We present a seamless UAV handoff framework, where target transfer is attempted via high-confidence feature matching, achieving 82.9\% target coverage. These results confirm the viability of coordinated UAV operations for extended marine tracking and lay the groundwork for scalable, autonomous monitoring.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {711–716},
numpages = {6}
}
@inbook{10.1145/3711875.3737662,
author = {Hossein Motlagh, Naser and Zaidan, Martha Arbayani and Irjala, Matti and Rebeiro-Hargrave, Andrew and Nurmi, Petteri and Tarkoma, Sasu},
title = {Drones in the Sky: Air Quality Monitoring at Heights},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3737662},
abstract = {Air pollution represents a critical global health challenge. Traditional air quality monitoring methods, which rely on expensive stations or extensive low-cost sensor networks, often fall short in capturing the fine spatial and temporal variations of pollutants, particularly in urban environments dominated by vehicular emissions. Recent advancements in drone technology offer a novel solution to these limitations, enabling the collection of high-resolution air quality data across different altitudes and environments. We contribute by investigating the potential of drone-based air quality monitoring. Specifically, we conduct measurements in two distinct settings: an industrial site and a residential area. We present analytical findings that highlight the effectiveness of drones in capturing pollutant levels and discuss the key challenges associated with this technology. Our findings underscore the promise of drone-based monitoring in enhancing air quality assessment and inform directions for future research.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {717–722},
numpages = {6}
}
@inbook{10.1145/3711875.3737663,
author = {Hayat, Samira and Raffelsberger, Christian},
title = {Real-time Adaptive Planning for Intelligent Distributed TSP-inspired Drone Swarming (RAPID-TSP)},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3737663},
abstract = {This paper presents RAPID-TSP, a distributed coordination algorithm for multi-drone scanning missions in dynamic environments. It enables drones to balance area coverage efficiency with network connectivity by leveraging a tunable parameter λ. This parameter allows each drone to adaptively choose its next scanning target based on proximity and the positions of neighboring drones, while aiming to maintain connectivity with a base station for mission updates. RAPID-TSP incorporates conflict resolution and supports two swarming strategies to minimize redundant scanning and ensure efficient collaboration. A realistic scenario involving the scanning of container stacks in a logistics hub serves as evaluation scenario. Extensive simulations show how RAPID-TSP performs robustly under varying conditions and can reduce mission completion time by up to 40\% compared to traditional mTSP coordination methods. A key design insight is that mission time is significantly reduced by adding more drones only when communication range is sufficient; otherwise, sparse connectivity limits their effectiveness.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {723–728},
numpages = {6}
}
@inbook{10.1145/3711875.3736684,
author = {Zhang, Jida and Jacques, Timothy and Chen, Joseph and Kapetanovic, Zerina},
title = {Bringing Edge Intelligence to Wildlife Camera Traps with Hyperdimensional Computing},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3736684},
abstract = {We present a smart wildlife camera trap with an efficient image classification pipeline that can identify high-value animal images while filtering out those triggered by non-animal motion (e.g., moving vegetation and vehicles) or partial animal appearances. To enable low-power, resource-efficient processing, we adopt hyperdimensional computing (HDC) techniques for image analysis. Our evaluation shows that HDC is highly effective for such binary classification tasks. We implement a hardware prototype that is able to autonomously process an image in ~100 milliseconds with 3W max power consumption, enabling the possibility for grid independence and long-term operation. The HDC algorithm achieves more than 88\% accuracy on existing datasets and 91\% accuracy in a real-time outdoor deployment when identifying animal presence, significantly reducing the number of false-positive image capture events. This paper demonstrates the feasibility of HDC in low-power edge-computing applications like wildlife camera traps.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {729–734},
numpages = {6}
}
@inbook{10.1145/3711875.3736688,
author = {Lee, Jihyuk and Han, Dongsu and Kim, Jaehong},
title = {Presto: Hybrid CPU-GPU Preprocessing Framework for Video-based AI Inference System},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3736688},
abstract = {The growing adoption of video-based AI models has created a pressing demand for high throughput, low latency inference systems. However, existing preprocessing frameworks—whether CPU or GPU based—struggle to keep up with the computational burdens of video decoding and data augmentation, resulting in suboptimal GPU utilization and degraded inference system performance.In this paper, we present PRESTO, a high-performance hybrid CPU-GPU preprocessing framework tailored for video-based AI inference systems. Presto integrates a hybrid preprocessing scheduler to dynamically balance CPU and GPU workloads, leverages selective decoding to eliminate unnecessary frame processing, and introduces a custom GPU Memory Manager that enables pipelined preprocessing and efficient GPU memory reuse. Through evaluation on the video captioning task, we show that Presto achieves up to 4.37\texttimes{} higher throughput and 2.72\texttimes{} lower latency compared to the de facto baselines, while reducing cloud costs by up to 75\%.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {735–740},
numpages = {6}
}
@inproceedings{10.1145/3711875.3736685,
author = {\v{C}op, Andrej and Bertalani\v{c}, Bla\v{z} and Grobelnik, Marko and Fortuna, Carolina},
title = {MRM3: Machine Readable ML Model Metadata},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3736685},
doi = {10.1145/3711875.3736685},
abstract = {As the complexity and number of machine learning (ML) models grows, well-documented ML models are essential for developers and companies to use or adapt them to their specific use cases. Model metadata, already present in unstructured format as model cards in online repositories such as Hugging Face, could be more structured and machine readable while also incorporating environmental impact metrics such as energy consumption and carbon footprint. Our work extends the existing State of the Art by defining a structured schema for ML model metadata focusing on machine-readable format and support for integration into a knowledge graph (KG) for better organization and querying, enabling a wider set of use cases. Furthermore, we present an example wireless localization model metadata dataset consisting of 22 models trained on 4 datasets, integrated into a Neo4j-based KG with 113 nodes and 199 relations.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {741–746},
numpages = {6},
keywords = {model metadata, knowledge graph, neo4j, ontology, taxonomy, machine learning},
location = {Hilton Anaheim, Anaheim, CA, USA},
series = {MobiSys '25}
}
@inbook{10.1145/3711875.3736687,
author = {Guo, Yunqi and Zhu, Guanyu and Liu, Kaiwei and Xing, Guoliang},
title = {SensorMCP: A Model Context Protocol Server for Custom Sensor Tool Creation},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3736687},
abstract = {The rising demand for customized sensor systems, such as wildlife and urban monitoring, underscores the need for scalable, AI-driven solutions. The Model Context Protocol (MCP) enables large language models (LLMs) to interface with external tools, yet lacks automated sensor tool generation. We propose SensorMCP, a novel MCP server framework that enables LLMs to dynamically generate and operate sensor tools through a tool-language co-development pipeline. Our contributions include: (1) a SensorMCP architecture for automated tool and language co-evolution, (2) an automated sensor toolbox generating tailored tools, and (3) language assets producing tool descriptions and linguistic modules. A preliminary evaluation using real-world zoo datasets demonstrates the practicality and efficiency of SensorMCP, achieving up to 95\% tool success rate in scenarios like animal monitoring. This work advances sensor systems by pioneering the co-evolution of LLMs and sensor tools, offering a scalable framework for customized sensing in mobile systems. The source code and dataset are publicly available at https://sensormcp.github.io/sensor-mcp/.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {747–752},
numpages = {6}
}
@inbook{10.1145/3711875.3736683,
author = {Wang, Kehan and Zhang, Siqin and Nan, Haijing and Hou, Xueyu and Zou, Jiaqi and Miao, Zicong},
title = {Empirical Analysis of LLMDPP: Advancing Log Parsing in the LLM Era},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3736683},
abstract = {In the field of software engineering, the automated analysis of log data is crucial for operations and maintenance teams. This study introduces LLMDPP, a novel log parser that leverages Large Language Models (LLMs) and Determinantal Point Process (DPP) sampling techniques to enhance the efficiency and accuracy of online log parsing. LLMDPP transforms raw log messages into structured log templates through a few-shot learning, thus simplifying the processing and analysis of log data. The study explores the accuracy of LLMDPP in log-parsing tasks and compares the effectiveness of different encoding functions (TF-IDF and Flan-T5-small embedding layer) in DPP sampling. Experimental results show that LLMDPP outperforms traditional methods in both Global Accuracy and Parsing Accuracy, with the semantic information encoding function performing better when the number of samples is low. Furthermore, we simulate an online scenario to evaluate the parsing effectiveness of different sampling methods on unseen log datasets. The results indicate that the DPP sampling method has an advantage in maintaining sample diversity and fairness, which can improve parsing accuracy.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {753–758},
numpages = {6}
}
@inbook{10.1145/3711875.3736686,
author = {Liu, Yang and Jang, SiYoung and Montanari, Alessandro and Kawsar, Fahim},
title = {SPATIUM: A Context-Aware Machine Learning Framework for Immersive Spatiotemporal Health Understanding},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3736686},
abstract = {Wearable devices have made significant progress in continuous health monitoring by providing time-series physiological data such as heart rate, respiration, and activity levels. However, most lack spatial awareness, limiting their ability to interpret physiological changes within environmental context—especially indoors where GPS is unreliable. Recent advances in indoor localization, such as Ultra-Wideband (UWB) and visual-inertial odometry, now allow precise spatial positioning in consumer devices. In this paper, we introduce SPATIUM, a context-aware machine learning framework designed to enable immersive spatiotemporal health understanding. SPATIUM integrates multimodal health signals with spatial and environmental data, such as furniture layout, lighting, and temperature, to support more contextually grounded health analysis. We present a proof-of-concept system that combines OmniBuds and UWB localization to collect and visualize spatiotemporal health data in 2D, 3D, and immersive formats. To evaluate the potential impact of spatial features, we conduct a simulation showing that incorporating spatial context improves physiological classification accuracy by up to 26\%. Our results highlight the importance of context-aware, spatially grounded modeling in achieving immersive spatiotemporal health understanding.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {759–764},
numpages = {6}
}
@inbook{10.1145/3711875.3737654,
author = {Balewski, Jan and Winick, Adam and Xu, Yilun and Vora, Neel and Huang, Gang and Santiago, David and Emerson, Joseph and Siddiqi, Irfan},
title = {First-principle crosstalk dynamics and Hamiltonian learning via Rabi experiments},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3737654},
abstract = {Coherent errors constitute a significant barrier to successful large-scale quantum computation. One such error mechanism is crosstalk, which violates spatial locality or the independence of operations. We present a description of crosstalk and learn the underlying parameters by executing novel simultaneous Rabi experiments and fitting the Hamiltonian to the observed data. We use this model to predict three- and four-qubit experiments and observe excellent agreement between our theoretical predictions and experimental results. Our technique enables researchers to study the dynamics of multi-qubit circuits without performing experiments, potentially facilitating the minimization of coherent gate errors via digital pulse precompilation. Additionally, this method provides whole-chip crosstalk characterization, a useful tool for guiding quantum processor design.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {765–770},
numpages = {6}
}
@inbook{10.1145/3711875.3737657,
author = {Le, Vu and Vora, Neel and Brahmbhatt, Devanshu and Xu, Yilun and Huang, Gang and Nguyen, Phuc "VP"},
title = {Computing Systems for Superconducting Qubits: Challenges and Opportunities},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3737657},
abstract = {Superconducting qubits have emerged as leading candidates for realizing quantum computers, which are particularly useful for solving computational problems that are beyond the capabilities of classical supercomputers. The performance of such systems critically depends on the precision and reliability of their control infrastructure. In this paper, we present an overview of state-of-the-art quantum control systems, including both closed- and open-source solutions, and discuss their respective advantages and limitations. While we review both categories, we focus more extensively on open-source control platforms and explore how their flexibility and programmability can support the research community and drive progress in the field. Lastly, we outline several promising research directions in quantum computing infrastructure, including scalable control, high-precision readout, and leakage suppression. We believe that these areas should be prioritized by the community, as they are critical to realizing fault-tolerant quantum computation.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {771–774},
numpages = {4}
}
@inbook{10.1145/3711875.3737656,
author = {Guang, Yicheng and Zanotta, Pietro and Zhou, Kai and Chen, Yueqi and Ayanzadeh, Ramin},
title = {Quantum Methods for Boundary Checking in Classical Programs},
year = {2025},
isbn = {9798400714535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711875.3737656},
abstract = {Boundary violations—array out-of-bounds accesses, integer overflows, and stray pointer offsets—remain a leading cause of software failure. Classical analyses such as abstract interpretation and symbolic execution try to detect such errors, yet the exponential growth of program states forces them to trade precision for scalability. We introduce QCheck, the first quantum framework aimed at boundary checking of classical programs.QCheck converts a program into a quantum circuit that places all feasible executions in superposition and then applies a fixed-point quantum search to amplify exactly those states that cross a user-supplied bound. This shrinks the worst-case cost of overflow detection from O(N) path explorations to [EQUATION] oracle calls while preserving soundness. The synthesised circuits require qubit counts and depths on par with Shor's factoring circuit, suggesting near-term hardware feasibility.We evaluated QCheck on 10 benchmark programs drawn from the SV-COMP suite and canonical algorithms such as GCD and Fibonacci. Running on IBM Qiskit's simulator with 3-bit variable widths, QCheck flagged all injected boundary bugs with zero false positives, whereas Frama-C raised spurious alerts on up to 70.8 \% of the same workloads and Angr timed out on half of them. Additional experiments confirm that the fixed-point search reaches its theoretical 75 \% success probability (for δ = 0.5) once the iteration count exceeds the minimal threshold, validating the practical soundness of our decoder.These results demonstrate that quantum-assisted boundary checking can combine exhaustive exploration with quantum speed-ups, opening a new path toward reliable software analysis in the NISQ era and beyond.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Systems, Applications and Services},
pages = {775–778},
numpages = {4}
}