@inproceedings{10.1145/3498361.3538931,
author = {Kiaghadi, Ali and Huang, Jin and Homayounfar, Seyedeh Zohreh and Andrew, Trisha and Ganesan, Deepak},
title = {FabToys: plush toys with large arrays of fabric-based pressure sensors to enable fine-grained interaction detection},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538931},
doi = {10.1145/3498361.3538931},
abstract = {Recent advances in fabric-based sensors have made it possible to densely instrument textile surfaces on smart toys without changing their look and feel. While such surfaces can be instrumented with traditional sensors, rigid elements change the nature of interaction and diminish the appeal of plush toys.In this work, we propose FabToy, a plush toy instrumented with a 24-sensor array of fabric-based pressure sensors located beneath the surface of the toy to have dense spatial sensing coverage while maintaining the natural feel of fabric and softness of the toy. We optimize both the hardware and software pipeline to reduce overall power consumption while achieving high accuracy in detecting a wide range of interactions at different regions of the toy. Our contributions include a) sensor array fabrication to maximize coverage and dynamic range, b) data acquisition and triggering methods to minimize the cost of sampling a large number of channels, and c) neural network models with early exit to optimize power consumed for computation when processing locally and autoencoder-based channel aggregation to optimize power consumed for communication when processing remotely. We demonstrate that we can achieve high accuracy of more than 83\% for robustly detecting and localizing complex human interactions such as swiping, patting, holding, and tickling in different regions of the toy.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {1–13},
numpages = {13},
keywords = {ubiquitous sensing and computing, smart toys, interaction detection},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3539773,
author = {Carver, Charles J. and Shao, Qijia and Lensgraf, Samuel and Sniffen, Amy and Perroni-Scharf, Maxine and Gallant, Hunter and Li, Alberto Quattrini and Zhou, Xia},
title = {Sunflower: locating underwater robots from the air},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3539773},
doi = {10.1145/3498361.3539773},
abstract = {Locating underwater robots is fundamental for enabling important underwater applications. The current mainstream method requires a physical infrastructure with relays on the water surface, which is largely ad-hoc, introduces a significant logistical overhead, and entails limited scalability. Our work, Sunflower, presents the first demonstration of wireless, 3D localization across the air-water interface - eliminating the need for additional infrastructure on the water surface. Specifically, we propose a laser-based sensing system to enable aerial drones to directly locate underwater robots. The Sunflower system consists of a queen and a worker component on a drone and each tracked underwater robot, respectively. To achieve robust sensing, key system elements include (1) a pinhole-based sensing mechanism to address the sensing skew at air-water boundary and determine the incident angle on the worker, (2) a novel optical-fiber sensing ring to sense weak retroreflected light, (3) a laser-optimized backscatter communication design that exploits laser polarization to maximize retroreflected energy, and (4) the necessary models and algorithms for underwater sensing. Real-world experiments demonstrate that our Sunflower system achieves average localization error of 9.7 cm with ranges up to 3.8 m and is robust against ambient light interference and wave conditions.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {14–27},
numpages = {14},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538945,
author = {Guan, Yongjie and Hou, Xueyu and Wu, Nan and Han, Bo and Han, Tao},
title = {DeepMix: mobility-aware, lightweight, and hybrid 3D object detection for headsets},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538945},
doi = {10.1145/3498361.3538945},
abstract = {Mobile headsets should be capable of understanding 3D physical environments to offer a truly immersive experience for augmented/mixed reality (AR/MR). However, their small form-factor and limited computation resources make it extremely challenging to execute in real-time 3D vision algorithms, which are known to be more compute-intensive than their 2D counterparts. In this paper, we propose DeepMix, a mobility-aware, lightweight, and hybrid 3D object detection framework for improving the user experience of AR/MR on mobile headsets. Motivated by our analysis and evaluation of state-of-the-art 3D object detection models, DeepMix intelligently combines edge-assisted 2D object detection and novel, on-device 3D bounding box estimations that leverage depth data captured by headsets. This leads to low end-to-end latency and significantly boosts detection accuracy in mobile scenarios. A unique feature of DeepMix is that it fully exploits the mobility of headsets to fine-tune detection results and boost detection accuracy. To the best of our knowledge, DeepMix is the first 3D object detection that achieves 30 FPS (i.e., an end-to-end latency much lower than the 100 ms stringent requirement of interactive AR/MR). We implement a prototype of DeepMix on Microsoft HoloLens and evaluate its performance via both extensive controlled experiments and a user study with 30+ participants. DeepMix not only improves detection accuracy by 9.1--37.3\% but also reduces end-to-end latency by 2.68--9.15\texttimes{}, compared to the baseline that uses existing 3D object detection models.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {28–41},
numpages = {14},
keywords = {mobile headsets, hybrid mobile vision, augmented and mixed reality, 3D object detection},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3539776,
author = {Sun, Bangjie and Tan, Sean Rui Xiang and Ren, Zhiwei and Chan, Mun Choon and Han, Jun},
title = {Detecting counterfeit liquid food products in a sealed bottle using a smartphone camera},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3539776},
doi = {10.1145/3498361.3539776},
abstract = {We are witnessing a surge in the reported cases of counterfeit liquid products in the market including olive oil, honey, and alcohol. Counterfeiters often adulterate the liquid products by replacing a large portion of the authentic content with cheaper substitutes (e.g., mixing vodka with cheaper alcohol or potentially toxic methanol). Exacerbating the problem, the counterfeits are packaged and sealed to factory standards, rendering it extremely difficult for an average consumer to identify them. While solutions exist, they are often impractical for the general public as they require specialized and costly equipment. To overcome these limitations, we propose LiquidHash, a novel counterfeit liquid food product detection system. LiquidHash is a practical solution that only requires the use of a commodity smartphone to detect adulterated liquid products without opening the bottles. LiquidHash works by detecting and tracking the shape and movement of air bubbles that form inside the bottles. We implement LiquidHash and evaluate its feasibility with real-world experiments under varying conditions with a total of more than 500 minutes of video recording and observe an overall detection accuracy of up to 95\%.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {42–55},
numpages = {14},
keywords = {smartphone camera, liquid testing, counterfeit},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538936,
author = {Chi, Guoxuan and Yang, Zheng and Xu, Jingao and Wu, Chenshu and Zhang, Jialin and Liang, Jianzhe and Liu, Yunhao},
title = {Wi-drone: wi-fi-based 6-DoF tracking for indoor drone flight control},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538936},
doi = {10.1145/3498361.3538936},
abstract = {After years of boom, drones and their applications are now entering indoors. Six-degree-of-freedom (6-DoF) pose tracking is the core of drone flight control, but existing solutions cannot be directly applied to indoor scenarios due to insufficient accuracy, low robustness to adverse texture and light conditions, and signal obstruction in indoor scenarios. To overcome the above limitations, we propose Wi-Drone, a Wi-Fi standalone 6-DoF tracking system for indoor drone flight control. Wi-Drone takes full advantage of both exte-roceptive and proprioceptive measurements of Wi-Fi to estimate the drone's absolute pose and relative motion, and fuse them in a tight-coupling manner to achieve their complementary benefits. We implement Wi-Drone and integrate it into a flight control system. The evaluation results show that Wi-Drone achieves a real-time performance with the average location accuracy of 26.1 cm and the rotation accuracy of 3.8°, which demonstrates its competency of flight control, compared to visual-inertial-based flight control. Such results also outperform existing Wi-Fi-based tracking solutions in terms of both dimensionality and accuracy.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {56–68},
numpages = {13},
keywords = {wireless sensing, flight control, COTS wi-fi, 6-DoF pose tracking},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538938,
author = {Kim, Taegyu and Ding, Aolin and Etigowni, Sriharsha and Sun, Pengfei and Chen, Jizhou and Garcia, Luis and Zonouz, Saman and Xu, Dongyan and Tian, Dave (Jing)},
title = {Reverse engineering and retrofitting robotic aerial vehicle control firmware using dispatch},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538938},
doi = {10.1145/3498361.3538938},
abstract = {Unmanned Aerial Vehicles as a service (UAVaaS) has increased the field deployment of Robotic Aerial Vehicles (RAVs) for different services such as transportation and terrain exploration. These RAVs are controlled by firmware, which is often closed-source, developed by vendors, and flashed into the ROM. While these binary blobs enable off-the-shelf management of RAVs, end users (individuals or organizations) have no idea if the control firmware is designed and implemented correctly, and can only rely on firmware updates from vendors when any vulnerability is discovered. This paper proposes DisPatch, the first reverse engineering and patching framework for understanding and improving controller design and implementation within RAV firmware. DisPatch first decompiles binary instructions and recovers controller functions and core controller variables by combining control theory with program analysis using symbolic execution and data flow analysis. End users can then write a patch in a domain-specific language (DSL), which will be translated and injected into the binary firmware by DisPatch automatically. We have applied DisPatch to two instances of commodity firmware from3DR IRIS+ and MantisQ RAVs and demonstrated 100\% and 80.7\% accuracy respectively in the controller decompilation. We have also shown the ability to prevent severe controller performance degradation by patching two real-world bugs with in the firmware and without breaking other functionality. Finally, we show that DisPatch introduces less than 0.53\% of space overhead and 1.48\% of runtime overhead without violating the soft real-time deadlines. DisPatch provides the first step towards an RAV binary firmware reverse engineering and patching system to customize controller design and implementation.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {69–83},
numpages = {15},
keywords = {security, robotic aerial vehicle, firmware analysis, binary analysis},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538941,
author = {Wu, Chuxiong and Li, Xiaopeng and Luo, Lannan and Zeng, Qiang},
title = {G2Auth: secure mutual authentication for drone delivery without special user-side hardware},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538941},
doi = {10.1145/3498361.3538941},
abstract = {Because of its cost effectiveness and timeliness, package delivery using unmanned aerial vehicles (UAVs), called drone delivery, is drawing growing attention. Authentication is critical for ensuring that a package is not picked up by an attacker's drone or delivered to an attacker. As delivery drones are costly and may carry sensitive or expensive packages, a drone should not get very close to a person unless she is authenticated; thus, conventional authentication approaches that require human-drone physical contact do not work. Existing authentication methods for drone delivery suffer from one or multiple of the following limitations: (1) requiring special user-side hardware; (2) enforcing one-way authentication only; (3) being vulnerable to relay attacks; (4) having compatibility issues. We present the first system, named Greet-to-Auth (G2Auth, for short), that supports mutual authentication between a user and a drone, without these limitations. A user waves her hand holding a smartphone to conduct the authentication. The evaluation shows that it is secure, accurate, usable, and robust.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {84–98},
numpages = {15},
keywords = {relay attacks, drone delivery, authentication},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3539775,
author = {Bai, Yang and Garg, Nakul and Roy, Nirupam},
title = {SPiDR: ultra-low-power acoustic spatial sensing for micro-robot navigation},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3539775},
doi = {10.1145/3498361.3539775},
abstract = {This paper presents the design and implementation of SPiDR, an ultra-low-power spatial sensing system for miniature mobile robots. This acoustic sensor produces a cross-sectional map of the field-of-view using only one speaker/microphone pair. While it is challenging to have enough spatial diversity of signal with a single omnidirectional source, we leverage sound's interaction with small structures to create a 3D-printed passive filter, called a stencil, that can project spatially coded signals on a region at a fine granularity. The system receives a linear combination of the reflections from nearby objects and applies a novel power-aware depth-map reconstruction algorithm. The algorithm first estimates the approximate locations of the objects in the scene and then iteratively applies fractional multi-resolution inversion. SPiDR consumes only 10mW of power to generate a depth-map in real-world scenario with over 80\% structural similarity score with the scene.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {99–113},
numpages = {15},
keywords = {spatial sensing, robot navigation, low-power sensing, acoustic metamaterial, IoT},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538918,
author = {Li, Danyang and Xu, Jingao and Yang, Zheng and Zhang, Qian and Ma, Qiang and Zhang, Li and Chen, Pengpeng},
title = {Motion inspires notion: self-supervised visual-LiDAR fusion for environment depth estimation},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538918},
doi = {10.1145/3498361.3538918},
abstract = {Environment depth estimation by fusing camera and radar enables a broad spectrum of applications such as autonomous driving, environmental perception, context-aware localization and navigation. Various pioneering approaches have been proposed to achieve accurate and dense depth estimation by integrating vision and LiDAR through deep learning. However, due to the challenges of sparse sampling of in-vehicle LiDARs, high ground-truth annotation overhead, and severe dynamics in real environments, existing solutions have not yet achieved widespread deployment on commercial autonomous vehicles. In this paper, we propose LeoVR, a visual-LiDAR fusion based self-supervised approach that enables accurate environment depth estimation. LeoVR digs into the vehicle's motion information and designs two effective system frameworks based on it to (i) optimize the depth estimation results, and (ii) provide supervision signals to train a DNN. We fully implement LeoVR on a robotic testbed and commercial vehicle to conduct extensive experiments across 6 months. The results demonstrate that LeoVR achieves remarkable performance with an average depth estimation error of 0.17m, outperforming existing state-of-the-art solutions by > 43\%. Besides, even cold-start in real environments by self-supervised training, LeoVR still achieves an average error of 0.21m, outperforming the related works by > 45\% and comparable to those supervised training methods.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {114–127},
numpages = {14},
keywords = {visual-LiDAR fusion, self-supervised learning, depth estimation},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538925,
author = {Qiu, Hang and Huang, Po-Han and Asavisanu, Namo and Liu, Xiaochen and Psounis, Konstantinos and Govindan, Ramesh},
title = {AutoCast: scalable infrastructure-less cooperative perception for distributed collaborative driving},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538925},
doi = {10.1145/3498361.3538925},
abstract = {Autonomous vehicles use 3D sensors for perception. Cooperative perception enables vehicles to share sensor readings with each other to improve safety. Prior work in cooperative perception scales poorly even with infrastructure support. AUTOCAST1 enables scalable infrastructure-less cooperative perception using direct vehicle-to-vehicle communication. It carefully determines which objects to share based on positional relationships between traffic participants, and the time evolution of their trajectories. It coordinates vehicles and optimally schedules transmissions in a distributed fashion. Extensive evaluation results under different scenarios show that, unlike competing approaches, AUTOCAST can avoid crashes and near-misses which occur frequently without cooperative perception, its performance scales gracefully in dense traffic scenarios providing 2-4x visibility into safety critical objects compared to existing cooperative perception schemes, its transmission schedules can be completed on the real radio testbed, and its scheduling algorithm is near-optimal with negligible computation overhead.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {128–141},
numpages = {14},
keywords = {cooperative perception, autonomous cars, V2V communication},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3539772,
author = {He, Liang and Shin, Kang G.},
title = {Battery-enabled anti-theft vehicle immobilizer},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3539772},
doi = {10.1145/3498361.3539772},
abstract = {Auto thieves often exploit the cyber vulnerabilities of existing key/phone-based vehicle immobilizers. To prevent this exploitation of cyber vulnerabilities for auto thefts, we present Battery Sleuth (Bleuth), a novel "physical" vehicle immobilizer which is immune to the common cyber-attack vectors - avoiding the use of wireless communication between key/phone and vehicle as well as in-vehicle networks. Bleuth achieves this by using the common 12V vehicle batteries to authenticate the driver with encrypted power-line communication and then control the battery's output power based on the authentication results, hence (im)mobilizing the vehicle without requiring drivers to carry any additional token. Bleuth is also equipped with four alarms to detect, and respond to, illegitimate operations (i.e., theft attempts), including attempts of unauthorized cranking of the engine, removal/shorting of the authenticator from the battery, abuse of the PLC to drain the car battery, and removal of the dongle from the auxiliary power outlet. Bleuth recharges its power supply automatically to free drivers from the maintenance burden. We have prototyped Bleuth as an add-on module that can be installed on commodity vehicles and evaluated it via field tests on 8 vehicles. We have also demonstrated Bleuth's utility and effectiveness via a survey of 612 car owners.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {142–154},
numpages = {13},
keywords = {vehicle immobilizer, power-line communication, automotive batteries},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538944,
author = {Woodford, Timothy and Zhang, Xinyu and Chai, Eugene and Sundaresan, Karthikeyan},
title = {Mosaic: leveraging diverse reflector geometries for omnidirectional around-corner automotive radar},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538944},
doi = {10.1145/3498361.3538944},
abstract = {A large number of traffic collisions occur as a result of obstructed sight lines, such that even an advanced driver assistance system would be unable to prevent the crash. Recent work has proposed the use of around-the-corner radar systems to detect vehicles, pedestrians, and other road users in these occluded regions. Through comprehensive measurement, we show that these existing techniques cannot sense occluded moving objects in many important real-world scenarios. To solve this problem of limited coverage, we leverage multiple, curved reflectors to provide comprehensive coverage over the most important locations near an intersection. In scenarios where curved reflectors are insufficient, we evaluate the relative benefits of using additional flat planar surfaces. Using these techniques, we more than double the probability of detecting a vehicle near the intersection in three real urban locations, and enable NLoS radar sensing using an entirely new class of reflectors.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {155–167},
numpages = {13},
keywords = {lidar, automotive sensing, around-the-corner radar, NLoS radar, ADAS},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538929,
author = {Ramanujam, Murali and Chen, Helen and Mardani, Shaghayegh and Netravali, Ravi},
title = {Floo: automatic, lightweight memoization for faster mobile apps},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538929},
doi = {10.1145/3498361.3538929},
abstract = {Owing to growing feature sets and sluggish improvements to smartphone CPUs (relative to mobile networks), mobile app response times have increasingly become bottlenecked on client-side computations. In designing a solution to this emerging issue, our primary insight is that app computations exhibit substantial stability over time in that they are entirely performed in rarely-updated codebases within app binaries and the OS. Building on this, we present Floo, a system that aims to automatically reuse (or memoize) computation results during app operation in an effort to reduce the amount of compute needed to handle user interactions. To ensure practicality - the struggle with any memoization effort - in the face of limited mobile device resources and the short-lived nature of each app computation, Floo embeds several new techniques that collectively enable it to mask cache lookup overheads and ensure high cache hit rates, all the while guaranteeing correctness for any reused computations. Across a wide range of apps, live networks, phones, and interaction traces, Floo reduces median and 95th percentile interaction response times by 32.7\% and 72.3\%.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {168–182},
numpages = {15},
keywords = {smartphones, performance, mobile apps, memoization, caching},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538942,
author = {Alcal\'{a}-Mar\'{\i}n, Sergi and Raman, Aravindh and Wu, Weili and Lutu, Andra and Bagnulo, Marcelo and Alay, Ozgu and Bustamante, Fabi\'{a}n},
title = {Global mobile network aggregators: taxonomy, roaming performance and optimization},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538942},
doi = {10.1145/3498361.3538942},
abstract = {A new model of global virtual Mobile Network Operator (MNO) - the Mobile Network Aggregator (MNA) - has recently been gaining significant traction. MNAs provide mobile communications services to their customers by leveraging multiple MNOs, and connecting through the one that best match their customers' needs at any point in time (and space). MNAs naturally provide optimized global coverage by connecting through local MNOs across the different geographic regions they provide service. In this paper, we dissect the operations of three MNAs, namely, Google Fi, Twilio and Truphone. We perform measurements using the three selected MNAs to assess their performance for three major applications, namely, DNS, web browsing and video streaming. We benchmark their performance comparing it to the one of a traditional MNO. We find that even MNAs provide some delay penalty compared to the service accessed through the local MNOs in the geographic area where the user is roaming, they can significantly improve performance compared to traditional roaming model of the MNOs (e.g. home routed roaming). Finally, in order to fully quantify the potential benefits that can be realized using the MNA model, we perform a set of emulations by deploying both control and user plane functions of open-source 5G implementations in different locations of AWS, and measure the potential gains.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {183–195},
numpages = {13},
keywords = {roaming, network aggregators, mobile networks, application performance},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538943,
author = {Liu, Yuxin (Myles) and Nakatsuka, Yoshimichi and Sani, Ardalan Amiri and Agarwal, Sharad and Tsudik, Gene},
title = {Vronicle: verifiable provenance for videos from mobile devices},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538943},
doi = {10.1145/3498361.3538943},
abstract = {Demonstrating veracity of videos is a longstanding problem that has recently become more urgent and acute. It is extremely hard to accurately detect manipulated videos using content analysis, especially in the face of subtle, yet effective, manipulations, such as frame rate changes or skin tone adjustments.In this paper, we present Vronicle, a method for generating provenance information for videos captured by mobile devices and using that information to verify authenticity of videos. A key feature of Vronicle is the use of Trusted Execution Environments (TEEs) for video capture and post-processing. This aids in constructing fine-grained provenance information that allows the consumer to verify various aspects of the video, thereby defeating numerous fake-video creation methods. Another important feature is the use of fixed-function post-processing units that facilitate verification of provenance information. These units can be deployed in any TEE, either in the mobile device that captures the video or in powerful servers.We present a prototype of Vronicle, which uses ARM TrustZone and Intel SGX for on-device and server-side post-processing, respectively. Moreover, we introduce two methods (and prototype the latter) for secure video capture on mobile devices: one using ARM TrustZone, and another using Google SafetyNet, providing a trade-off between security and immediate deployment. Our evaluation demonstrates that: (1) Vronicle's performance is well-suited for non-real-time use-cases, and (2) offloading post-processing significantly improves Vronicle's performance, matching that of uploading videos to YouTube.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {196–208},
numpages = {13},
keywords = {video provenance, deepfakes},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538932,
author = {Jia, Fucheng and Zhang, Deyu and Cao, Ting and Jiang, Shiqi and Liu, Yunxin and Ren, Ju and Zhang, Yaoxue},
title = {CoDL: efficient CPU-GPU co-execution for deep learning inference on mobile devices},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538932},
doi = {10.1145/3498361.3538932},
abstract = {Concurrent inference execution on heterogeneous processors is critical to improve the performance of increasingly heavy deep learning (DL) models. However, available inference frameworks can only use one processor at a time, or hardly achieve speedup by concurrent execution compared to using one processor. This is due to the challenges to 1) reduce data sharing overhead, and 2) properly partition each operator between processors.By solving the challenges, we propose CoDL, a concurrent DL inference framework for the CPU and GPU on mobile devices. It can fully utilize the heterogeneous processors to accelerate each operator of a model. It integrates two novel techniques: 1) hybrid-type-friendly data sharing, which allows each processor to use its efficient data type for inference. To reduce data sharing overhead, we also propose hybrid-dimension partitioning and operator chain methods; 2) non-linearity- and concurrency-aware latency prediction, which can direct proper operator partitioning by building an extremely light-weight but accurate latency predictor for different processors.Based on the two techniques, we build the end-to-end CoDL inference framework, and evaluate it on different DL models. The results show up to 4.93\texttimes{} speedup and 62.3\% energy saving compared with the state-of-the-art concurrent execution system.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {209–221},
numpages = {13},
keywords = {mobile devices, deep learning inference, CPU-GPU co-execution},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538940,
author = {Park, Jongseok and Bin, Kyungmin and Lee, Kyunghan},
title = {mGEMM: low-latency convolution with minimal memory overhead optimized for mobile devices},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538940},
doi = {10.1145/3498361.3538940},
abstract = {The convolution layer is the key building block in many neural network designs. Most high-performance implementations of the convolution operation rely on GEMM (General Matrix Multiplication) to achieve high computational throughput with a large workload size. However, in mobile environments, the user experience priority puts focus on low-latency inferences over a single or limited batch size. This signifies two major problems of current GEMM-based solutions: 1) GEMM-based solutions require mapping the convolution operation to GEMM, causing overheads in both computation and memory, 2) GEMM-based solutions lose large opportunities of data reuse while mapping, leading to under-utilization of the given hardware. Through an in-depth analysis of current GEMM-based solutions, we identify the root cause of these problems, and we propose mGEMM, a convolution solution that overcomes the aforementioned problems, without changes in accuracy. mGEMM expands the structure of GEMM in such a way that it can accommodate the convolution operation without any overhead, while the existing algorithms suffer from inefficiencies in converting the convolution operation to a static GEMM algorithm. Our extensive evaluations done over various neural networks and test devices show that mGEMM outperforms the existing solutions in the aspects of latency, memory overhead, and energy consumption. In running a real-world application, YoloV3-Tiny object detection, mGEMM achieves up to 1.29\texttimes{} and 1.58\texttimes{} speedup in total latency and convolution latency compared to the state-of-the-art, resulting in 15.5\% reduction in energy consumption while using only near-minimum heap memory.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {222–234},
numpages = {13},
keywords = {parallel computing algorithms, convolutional neural networks},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538948,
author = {Jeong, Joo Seong and Lee, Jingyu and Kim, Donghyun and Jeon, Changmin and Jeong, Changjin and Lee, Youngki and Chun, Byung-Gon},
title = {Band: coordinated multi-DNN inference on heterogeneous mobile processors},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538948},
doi = {10.1145/3498361.3538948},
abstract = {The rapid development of deep learning algorithms, as well as innovative hardware advancements, encourages multi-DNN workloads such as augmented reality applications. However, existing mobile inference frameworks like TensorFlow Lite and MNN fail to efficiently utilize heterogeneous processors available on mobile platforms, because they focus on running a single DNN on a specific processor. As mobile processors are too resource-limited to deliver reasonable performance for such workloads by their own, it is challenging to serve multi-DNN workloads with existing frameworks.This paper introduces Band, a new mobile inference system that coordinates multi-DNN workloads on heterogeneous processors. Band examines a DNN beforehand and partitions it into a set of subgraphs, while taking operator dependency into account. At runtime, Band dynamically selects a schedule of subgraphs from multiple possible schedules, following the scheduling goal of a pluggable scheduling policy. Fallback operators, which are not supported by certain mobile processors, are also considered when generating subgraphs. Evaluation results on mobile platforms show that our system outperforms TensorFlow Lite, a state-of-the-art mobile inference framework, by up to 5.04\texttimes{} for single-app workloads involving multiple DNNs. For a multi-app scenario consisting of latency-critical DNN requests, Band reaches up to 3.76\texttimes{} higher SLO satisfaction rate.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {235–247},
numpages = {13},
keywords = {multi-DNN inference, mobile deep learning, heterogeneous processors, DNN accelerators},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538919,
author = {Dong, Wei and Lv, Jiamei and Chen, Gonglong and Wang, Yihui and Li, Huikang and Gao, Yi and Bharadia, Dinesh},
title = {TinyNet: a lightweight, modular, and unified network architecture for the internet of things},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538919},
doi = {10.1145/3498361.3538919},
abstract = {Interoperability among a vast number of heterogeneous IoT nodes is a key issue. However, the communication among IoT nodes does not fully interoperate to date. The underlying reason is the lack of a lightweight and unified network architecture for IoT nodes having different radio technologies. In this paper, we design and implement TinyNet, a lightweight, modular, and unified network architecture for representative low-power radio technologies including 802.15.4, BLE, and LoRa. The modular architecture of TinyNet allows us to simplify the creation of new protocols by selecting specific modules in TinyNet. We implement TinyNet on realistic IoT nodes including TI CC2650 and Heltec IoT LoRa nodes. We perform extensive evaluations. Results show that TinyNet (1) allows interoperability at or above the network layer; (2) allows code reuse for multi-protocol co-existence and simplifies new protocols design by module composition; (3) has a small code size and memory footprint.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {248–260},
numpages = {13},
keywords = {network architecture, interoperability, internet of things},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538922,
author = {Li, Borui and Fan, Hongchang and Gao, Yi and Dong, Wei},
title = {Bringing webassembly to resource-constrained iot devices for seamless device-cloud integration},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538922},
doi = {10.1145/3498361.3538922},
abstract = {Recent years have witnessed the progressive integration between IoT (Internet of Things) devices and the cloud server, which promotes the efficiency and interoperability of IoT applications. WebAssembly, known for its performance and portability, is considered a promising technology to bridge the heterogeneity between devices and the server. Nevertheless, resource-constrained devices, which are commonly deployed in the wild, have difficulty participating in this device-cloud integration because they can hardly run WebAssembly efficiently.Hence, we propose WAIT, a lightweight <u>W</u>eb<u>A</u>ssembly runtime on resource-constrained <u>I</u>o<u>T</u> devices for device-cloud integrated applications. WAIT is the first work to enable the Ahead-of-Time (AOT) compilation of WebAssembly on resource-constrained devices by leveraging several approaches to reduce memory usage. Moreover, WAIT introduces various safety checks at compile-time to guarantee the sandbox execution of WebAssembly and optimizes energy consumption for IoT devices. Results show that WAIT achieves 84.8\texttimes{} lower RAM usage compared with the state-of-the-art WebAssembly AOT runtime, and reduces energy consumption by 1.2\texttimes{}~4.9\texttimes{} while guaranteeing the sandboxed execution of WebAssembly modules.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {261–272},
numpages = {12},
keywords = {webassembly, internet of things, ahead-of-time compilation},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538923,
author = {Varshney, Ambuj and Yan, Wenqing and Dutta, Prabal},
title = {Judo: addressing the energy asymmetry of wireless embedded systems through tunnel diode based wireless transmitters},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538923},
doi = {10.1145/3498361.3538923},
abstract = {The radio transmitter is the most power-consuming component of a wireless embedded system. We present Judo, a radio transmitter that enables power balance between the wireless transmission, sensing, and processing tasks of a wireless embedded system. Judo transmitters leverage the fact that modern radio transceivers offer high receive sensitivity at low power. Therefore, even if the radio transmitter emits a weak signal, the link budget and transmission range will often remain high. With this key insight, we revisit the radio transmitter architecture by dramatically reducing the radiated power and hence the overall power draws. Specifically, Judo transmitter uses a tunnel diode oscillator to integrate the stages of a radio transmitter into a single energy-efficient step. In this step, baseband signals are generated and mixed using peak power below 100 μW. However, we sacrifice stability of tunnel diode oscillator for low-power consumption. We use the injection-locking phenomenon to stabilise the tunnel diode oscillator with an external carrier signal. Based on this novel architecture, we implement a transmitter that supports frequency-shift keying as a modulation scheme. Judo transmits over distances greater than 100 m at a bit rate of 100 kbps. It does so with an emitter device providing the carrier signal, and located at a distance of more than 100 m from Judo transmitter. In terms of critical link metrics, it outperforms the radio transmitters commonly used in wireless embedded systems.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {273–286},
numpages = {14},
keywords = {wireless transmitters, wireless embedded systems, tunnel diodes, internet of things, backscatter communication},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538934,
author = {de Winkel, Jasper and Tang, Haozhe and Pawe\l{}czak, Przemys\l{}aw},
title = {Intermittently-powered bluetooth that works},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538934},
doi = {10.1145/3498361.3538934},
abstract = {We present an architecture for intermittently-powered wireless communication systems that does not require any changes to the official protocol specification. Our core idea is to save the intermediate state of the wireless protocol to non-volatile memory within each connection interval. The protocol state is then deterministically restored at a predefined (harvested energy-dependent) time, which follows the connection interval. As a case study for our architecture, we introduce FreeBie: a battery-free intermittently-powered Bluetooth Low Energy (BLE) mote. To the best of our knowledge FreeBie is the first battery-free active wireless system that sustains bi-directional communication on intermittent harvested energy. The strength of our architecture is articulated by FreeBie consuming at least 9.5 times less power during device inactivity periods than a state-of-the-art BLE device.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {287–301},
numpages = {15},
keywords = {mobile networks, intermittent computing, energy harvesting, embedded systems, bluetooth, battery-free},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3539774,
author = {Zhang, Han and Agarwal, Yuvraj and Fredrikson, Matt},
title = {TEO: ephemeral ownership for IoT devices to provide granular data control},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3539774},
doi = {10.1145/3498361.3539774},
abstract = {As Internet-of-Things (IoT) devices rapidly gain popularity, they raise significant privacy concerns given the breadth of sensitive data they can capture. These concerns are amplified by the fact that in many situations, IoT devices collect data about people other than their owner or administrator, and these stakeholders have no say in how that data is managed, used, or shared. To address this, we propose a new model of ownership, IoT Ephemeral Ownership (TEO). TEO allows stakeholders to quickly register with an IoT device for a limited period, and thus claim co-ownership over the sensitive data that the device generates. Device admins retain the ability to decide who may become an ephemeral owner, but no longer have access or control to the private data generated by the device. The encrypted data in TEO is accessible only by entities after seeking explicit permission from the different co-owners of that data. We verify the key security properties of our protocol underpinning TEO in the symbolic model using ProVerif. We also implement a cross-platform prototype of TEO for mobile phones and embedded devices, and integrate it into three real-world application case studies. Our evaluation shows that the latency and battery impact of TEO is typically small, adding ≤ 187 ms onto one-time operations, and introducing limited (<25\%) overhead on recurring operations like private data storage.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {302–315},
numpages = {14},
keywords = {stakeholder privacy, protocol verification, internet of things, ephemeral ownership, access control},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538924,
author = {Bae, Kang Min and Ahn, Namjo and Chae, Yoon and Pathak, Parth and Sohn, Sung-Min and Kim, Song Min},
title = {OmniScatter: extreme sensitivity mmWave backscattering using commodity FMCW radar},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538924},
doi = {10.1145/3498361.3538924},
abstract = {Massive connectivity is a key to the success of the Internet of Things. While mmWave backscatter has great potential, substantial signal attenuation and overwhelming ambient reflections impose significant challenges. We present OmniScatter, a practical mmWave backscatter with an extreme sensitivity of -115 dBm. The performance is theoretically comparable to the popular commodity RFID EPC Gen2 (900 MHz), and is empirically validated via evaluations under various practical settings with abundant ambient reflections and blockages - e.g., In an office where a tag is locked in a wooden closet 6m away, as well in libraries and retail stores where a tag is placed across two rows of metal shelves. At the heart of OmniScatter is the new High Definition FMCW (HD-FMCW), which interplays with the tag (FSK) signal to disentangle the ambient reflections from the tag signal in the frequency domain, essentially offering immunity to ambient reflections. To further support practical deployment, OmniScatter offers coordination-free Frequency Division Multiple Access (FDMA) that effortlessly scales to thousands of concurrent tags. The readers were built on commodity radars and the tags were prototyped on PCB. The trace-driven evaluation demonstrates concurrent communication of 1100 tags with the BER < 1.5\%, paving a pathway towards practical mmWave backscatter for everyday and anywhere use.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {316–329},
numpages = {14},
keywords = {retro-reflective tag, mmWave, internet-of-things, backscatter, FMCW},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538927,
author = {Zhu, Fengyuan and Ouyang, Mingwei and Feng, Luwei and Liu, Yaoyu and Tian, Xiaohua and Jin, Meng and Chen, Dongyao and Wang, Xinbing},
title = {Enabling software-defined PHY for backscatter networks},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538927},
doi = {10.1145/3498361.3538927},
abstract = {In this paper, we for the first time show how to enable software-defined PHY (SD-PHY) to achieve agile reprogrammability in wireless backscatter networks. This can facilitate innovations in this field by relieving researchers from unnecessary engineering work. With SD-PHY, the tag's PHY-layer behavior can be neatly defined by configuring a set of parameters, which allows the common hardware to generate backscattered signals complying with various wireless protocols. The SD-PHY architecture is based on the key insight that the tag's PHY-layer behavior is essentially determined by reflection coefficient sequence.The SD-PHY is factually to instruct the hardware: How to generate various reflection coefficient sequences that meet different protocols' requirements at the right time; under what clock rate to feed the generated sequence into RF switches on the tag. We abstract such instructions into a set of parameters. To make different parameter values take effect in the runtime, innovative designs of the generic wake-up receiver, baseband modulator, and clock signal generator on the tag, which are responsible for executing those parameterized instructions. We design and implement a general hardware platform to support the SD-PHY software. Moreover, we demonstrate that under the unified SD-PHY framework how the same tag can generate different kinds of backscatter signals, which could obey standardized protocols such as Wi-Fi (11b/g), BLE, LoRa, and LTE, as well as highly customized protocols such as OFDMA backscatter and NetScatter. Experimental results show that the system presents similar performance no matter it is realized with universal SD-PHY or a dedicated design approach.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {330–342},
numpages = {13},
keywords = {software-defined, backscatter, PHY},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538930,
author = {Yang, Yifan and Yuan, Longzhi and Zhao, Jia and Gong, Wei},
title = {Content-agnostic backscatter from thin air},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538930},
doi = {10.1145/3498361.3538930},
abstract = {We present CAB, a content-agnostic backscatter system that can demodulate both tag and ambient data from ambient backscattered WiFi alone. In contrast to prior ambient backscatter systems that use ambient data (content) as tag-data carriers, we focus on zero-subcarriers, which are invariant and independent for any ambient OFDM WiFi. The idea of using zero-subcarriers to convey tag data is simple and elegant. Not only does it for the first time remove the dependency of tag-data demodulation on ambient data, but it also significantly improves the practicality of ambient backscatter.We prototype CAB using off-the-shelf FPGAs and SDRs. Extensive experiments show CAB is universal as it can work with multi-band, multi-stream, and multi-user ambient traffic, including WiFi 3/4/5/6. CAB is also high-performing since it can deliver 340.9 Mbps aggregate throughput, reaching 97\% Shannon capacity. Since CAB is general, we extend it to leverage ambient LTE traffic as excitations, and the achieved tag-data BER is below 0.002\%. As the first content-agnostic backscatter that delivers near Shannon-capacity throughput, we believe CAB takes a curial step forward on ubiquitous battery-free IoTs.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {343–356},
numpages = {14},
keywords = {internet of things, backscatter, OFDM},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538946,
author = {Chen, Ruirong and Gao, Wei},
title = {TransFi: emulating custom wireless physical layer from commodity wifi},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538946},
doi = {10.1145/3498361.3538946},
abstract = {New wireless physical-layer designs are the key to improving wireless network performance. Adopting these new designs, however, requires modifications on wireless hardware and is difficult on commodity devices. In this paper, we show that this hardware modification in many cases can be avoided by TransFi, a new software technique that enables custom wireless PHY functionality on commodity WiFi transmitters via fine-grained emulation. Our basic insight is that many custom wireless signals can be emulated by manipulating the MAC payloads of WiFi MIMO streams and mixing the transmitted signals from these streams on the air. To perform such emulation, TransFi considers the target signal as a mixture of QAM constellation points on the complex plane, and reversely computes the MAC payload of each MIMO stream from one selected QAM constellation point. We implemented TransFi on commodity WiFi devices to emulate three custom wireless PHYs with diverse characteristics. Experiment results show that TransFi's accuracy of emulation is >90\% when transmitting emulated data payloads at 11.4 Mbps (46x faster than existing methods), and the decoding error at this data rate is <1\% (10x lower than existing methods).},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {357–370},
numpages = {14},
keywords = {wireless physical layer, wifi, signal emulation, quadrature amplitude modulation},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538921,
author = {Liao, Qianru and Huang, Yongzhi and Huang, Yandao and Zhong, Yuheng and Jin, Huitong and Wu, Kaishun},
title = {MagEar: eavesdropping via audio recovery using magnetic side channel},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538921},
doi = {10.1145/3498361.3538921},
abstract = {Speakers have been widely embedded in various electronic devices as a standard configuration. The security vulnerability of microspeakers (such as earphones) is commonly overlooked because it is often assumed that soundproof boundaries, such as walls, can prevent privacy-infringing sound leakage. In this paper, we present the prototype MagEar, an eavesdropping system that leverages magnetic side-channel signals leaked by a microspeaker to recover intelligible human speech. MagEar has sufficiently high sensitivity to detect magnetic fields on the order of nanotesla, exceeding some high-precision magnetometers. It can recover high-quality audio with 90\% similarity to the original audio even at a distance of 60 cm. In addition, the MagEar prototype is portable and can be hidden in a headset shell. We have implemented MagEar as a proof-of-concept system and conducted several case studies of eavesdropping on different types of speaker-embedded devices, including earphones, and we have demonstrated the ability to successfully transcribe the recovered speech using automatic speech recognition techniques even when blocked by soundproof walls. We hope that our work can push manufacturers to rethink this security vulnerability of speakers.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {371–383},
numpages = {13},
keywords = {side channel attack, privacy disclosure, mobile security, magnetic field, eavesdropping},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538933,
author = {Chatterjee, Ishan and Kim, Maruchi and Jayaram, Vivek and Gollakota, Shyamnath and Kemelmacher, Ira and Patel, Shwetak and Seitz, Steven M.},
title = {ClearBuds: wireless binaural earbuds for learning-based speech enhancement},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538933},
doi = {10.1145/3498361.3538933},
abstract = {We present ClearBuds, the first hardware and software system that utilizes a neural network to enhance speech streamed from two wireless earbuds. Real-time speech enhancement for wireless earbuds requires high-quality sound separation and background cancellation, operating in real-time and on a mobile phone. Clear-Buds bridges state-of-the-art deep learning for blind audio source separation and in-ear mobile systems by making two key technical contributions: 1) a new wireless earbud design capable of operating as a synchronized, binaural microphone array, and 2) a lightweight dual-channel speech enhancement neural network that runs on a mobile device. Our neural network has a novel cascaded architecture that combines a time-domain conventional neural network with a spectrogram-based frequency masking neural network to reduce the artifacts in the audio output. Results show that our wireless earbuds achieve a synchronization error less than 64 μs and our network has a runtime of 21.4 ms on an accompanying mobile phone. In-the-wild evaluation with eight users in previously unseen indoor and outdoor multipath scenarios demonstrates that our neural network generalizes to learn both spatial and acoustic cues to perform noise suppression and background speech removal. In a user-study with 37 participants who spent over 15.4 hours rating 1041 audio samples collected in-the-wild, our system achieves improved mean opinion score and background noise suppression.System demo video: https://youtu.be/HYu0ybjcQPA},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {384–396},
numpages = {13},
keywords = {noise cancellation, earable computing, cascaded neural networks, audio source separation, audio and speech processing},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538935,
author = {Jin, Yincheng and Gao, Yang and Guo, Xiaotao and Wen, Jun and Li, Zhengxiong and Jin, Zhanpeng},
title = {EarHealth: an earphone-based acoustic otoscope for detection of multiple ear diseases in daily life},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538935},
doi = {10.1145/3498361.3538935},
abstract = {With the aging of the population and the long-time wearing of earphones, hearing health has gradually emerged as a worldwide health issue. Early detection of hearing health conditions would greatly reduce potential risks with timely medical intervention. This study proposes an earphone-based ear condition monitoring system, named EarHealth, which is low-cost, non-invasive, and easily usable in daily life. It can detect three major hearing health conditions: ruptured eardrum, earwax buildup and blockage, and otitis media. By analyzing the recorded echoes evoked by a chirp sound stimulus, EarHealth recognizes the distinguishable characteristics from ear canal structure and eardrum mobility. EarHealth achieves an accuracy of 82.6\% in 92 human subjects, including 27 normal subjects, 22 patients with ruptured eardrum, 25 patients with otitis media, and 18 patients with earwax blockage. EarHealth is the first earphone-based system capable of monitoring hearing health conditions by utilizing the ear canal geometry and eardrum mobility. It is anticipated that EarHealth would provide pervasive and proactive protection for hearing health.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {397–408},
numpages = {12},
keywords = {earphone, eardrum mobility, ear disease, ear canal, acoustic sensing},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538937,
author = {Choi, Myeongwon and Oh, Sangeun and Kim, Insu and Kim, Hyosu},
title = {MagSnoop: listening to sounds induced by magnetic field fluctuations to infer mobile payment tokens},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538937},
doi = {10.1145/3498361.3538937},
abstract = {Samsung Pay, one of the most representative mobile payment services, allows mobile users to make payment transactions almost anywhere using only their smartphone. This is thanks to MST (Magnetic Secure Transmission) that supports communication between smartphones and payment terminals for magnetic cards by transferring payment tokens via magnetic waves. Several attack methods have targeted this new technology by eavesdropping on magnetic fields to intercept the tokens, but with the use of dedicated hardware. This paper raises new security concerns for mobile payment users in a different, yet more effective way; by introducing MagSnoop, a novel framework that infers payment tokens from listening to MST sounds generated during the activation of MST payment transactions. More specifically, we first explore the principle, causing the generation of MST sounds, and the fundamental characteristics of these sounds. We then use these observations to infer payment tokens with a high degree of accuracy, robustness, applicability, and data efficiency. Our experiments with a prototype of MagSnoop demonstrate that it can support high accuracy in token inference (more than 77.8\%). In addition, MagSnoop can maintain a reasonable level of accuracy regardless of the payment environments (e.g., 69.2\% with a noise level of 50 dBA) and even in the real world (an inference success rate of 68.0\% with 15 real-world users).},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {409–421},
numpages = {13},
keywords = {mobile security, mobile payment token inference, magnetic secure transmission, acoustic side channel attacks},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538939,
author = {Kim, Joongyum and Kim, Jihwan and Wi, Seongil and Kim, Yongdae and Son, Sooel},
title = {HearMeOut: detecting voice phishing activities in Android},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538939},
doi = {10.1145/3498361.3538939},
abstract = {In South Korea, voice phishing has been proliferating with the advent of voice phishing apps: the number of annual victims had risen to 34,527 in 2020, representing financial losses of approximately 598 million USD. However, the voice phishing functionalities that these abusive apps implement are largely understudied. To this end, we analyze 1,017 voice phishing apps and reveal new phishing functionalities: outgoing call redirection, call screen overlay, and fake call voice. We find that call redirection that changes the intended recipients of victims' outgoing calls plays a critical role in facilitating voice phishing; our user study shows that 87\% of the participants did not notice that their intended recipients were changed when call redirection occurred. We further investigate implementations of these fatal functionalities to distinguish their malicious behaviors from their corresponding behaviors in benign apps. We then propose HearMeOut, an Android system-level service that detects phishing behaviors that phishing apps conduct in runtime and blocks the detected behaviors. HearMeOut achieves high accuracy with no false positives or negatives in classifying phishing behaviors while exhibiting an unnoticeable latency of 0.36 ms on average. Our user study demonstrates that HearMeOut is able to prevent 100\% of participants from being phished by providing active warnings. Our work facilitates a better understanding of recent voice phishing and proposes practical mitigation with recommendations for Android system changes.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {422–435},
numpages = {14},
keywords = {voice phishing, system security, phishing app detection, Android voice phishing apps},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538917,
author = {Shin, Jaemin and Li, Yuanchun and Liu, Yunxin and Lee, Sung-Ju},
title = {FedBalancer: data and pace control for efficient federated learning on heterogeneous clients},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538917},
doi = {10.1145/3498361.3538917},
abstract = {Federated Learning (FL) trains a machine learning model on distributed clients without exposing individual data. Unlike centralized training that is usually based on carefully-organized data, FL deals with on-device data that are often unfiltered and imbalanced. As a result, conventional FL training protocol that treats all data equally leads to a waste of local computational resources and slows down the global learning process. To this end, we propose FedBalancer, a systematic FL framework that actively selects clients' training samples. Our sample selection strategy prioritizes more "informative" data while respecting privacy and computational capabilities of clients. To better utilize the sample selection to speed up global training, we further introduce an adaptive deadline control scheme that predicts the optimal deadline for each round with varying client training data. Compared with existing FL algorithms with deadline configuration methods, our evaluation on five datasets from three different domains shows that FedBalancer improves the time-to-accuracy performance by 1.20~4.48\texttimes{} while improving the model accuracy by 1.1~5.0\%. We also show that FedBalancer is readily applicable to other FL approaches by demonstrating that FedBalancer improves the convergence speed and accuracy when operating jointly with three different FL algorithms.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {436–449},
numpages = {14},
keywords = {sample selection, heterogeneity, federated learning, deadline control},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538928,
author = {Wang, Qipeng and Xu, Mengwei and Jin, Chao and Dong, Xinran and Yuan, Jinliang and Jin, Xin and Huang, Gang and Liu, Yunxin and Liu, Xuanzhe},
title = {Melon: breaking the memory wall for resource-efficient on-device machine learning},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538928},
doi = {10.1145/3498361.3538928},
abstract = {On-device learning is a promising technique for emerging privacy-preserving machine learning paradigms. However, through quantitative experiments, we find that commodity mobile devices cannot well support state-of-the-art DNN training with a large enough batch size, due to the limited local memory capacity. To fill the gap, we propose Melon, a memory-friendly on-device learning framework that enables the training tasks with large batch size beyond the physical memory capacity. Melon judiciously retrofits existing memory saving techniques to fit into resource-constrained mobile devices, i.e., recomputation and micro-batch. Melon further incorporates novel techniques to deal with the high memory fragmentation and memory adaptation. We implement and evaluate Melon with various typical DNN models on commodity mobile devices. The results show that Melon can achieve up to 4.33\texttimes{} larger batch size under the same memory budget. Given the same batch size, Melon achieves 1.89\texttimes{} on average (up to 4.01\texttimes{}) higher training throughput, and saves up to 49.43\% energy compared to competitive alternatives. Furthermore, Melon reduces 78.59\% computation on average in terms of memory budget adaptation.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {450–463},
numpages = {14},
keywords = {mobile device, memory optimization, deep learning},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3539765,
author = {Gim, In and Ko, JeongGil},
title = {Memory-efficient DNN training on mobile devices},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3539765},
doi = {10.1145/3498361.3539765},
abstract = {On-device deep neural network (DNN) training holds the potential to enable a rich set of privacy-aware and infrastructure-independent personalized mobile applications. However, despite advancements in mobile hardware, locally training a complex DNN is still a nontrivial task given its resource demands. In this work, we show that the limited memory resources on mobile devices are the main constraint and propose Sage as a framework for efficiently optimizing memory resources for on-device DNN training. Specifically, Sage configures a flexible computation graph for DNN gradient evaluation and reduces the memory footprint of the graph using operator- and graph-level optimizations. In run-time, Sage employs a hybrid of gradient checkpointing and micro-batching techniques to dynamically adjust its memory use to the available system memory budget. Using implementation on off-the-shelf smartphones, we show that Sage enables local training of complex DNN models by reducing memory use by more than 20-fold compared to a baseline approach. We also show that Sage successfully adapts to run-time memory budget variations, and evaluate its energy consumption to show Sage's practical applicability.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {464–476},
numpages = {13},
keywords = {mobile resource optimization, mobile DNN training framework, memory adaptive model training},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538920,
author = {Blanco, Alejandro and Mateo, Pablo Jim\'{e}nez and Gringoli, Francesco and Widmer, Joerg},
title = {Augmenting mmWave localization accuracy through sub-6 GHz on off-the-shelf devices},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538920},
doi = {10.1145/3498361.3538920},
abstract = {Millimeter-wave (mmWave) technology is an important element to increase the throughput and reduce latency of future wireless networks. At the same time, its high bandwidth and highly directional antennas allow for unprecedented accuracy in wireless sensing and localization applications. In this paper, we thoroughly analyze mmWave localization and find that it is either extremely accurate or has a very high error, since there is significant mmWave coverage via reflections and even through walls. As a consequence, sub-6 GHz technology can not only provide (coarse) localization where mmWave is not available, but is also critical to decide among multiple candidate antennas and APs for accurate mmWave localization.Based on these insights, we design a high-accuracy joint mmWave and sub-6 GHz location system. We enable CSI-based angle estimation and FTM-based ranging on off-the-shelf mmWave devices to implement our mechanism and carry out an extensive measurement campaign. Our system is the first to achieve 18 cm median location error with off-the-shelf devices under their normal mode of operation. We further release the location system (and in particular the CSI and FTM functionality) as well as the trace data from the measurement campaign to the research community.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {477–490},
numpages = {14},
keywords = {wireless networks, mmWave, indoor localization, ToF, CSI, AoA},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538926,
author = {Kong, Hao and Xu, Xiangyu and Yu, Jiadi and Chen, Qilin and Ma, Chenguang and Chen, Yingying and Chen, Yi-Chao and Kong, Linghe},
title = {m3Track: <u>mm</u>wave-based <u>m</u>ulti-user 3D posture tracking},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538926},
doi = {10.1145/3498361.3538926},
abstract = {Nowadays, the market of 3D human posture tracking has extended to a broad range of application scenarios. As current mainstream solutions, vision-based posture tracking systems suffer from privacy leakage concerns and depend on lighting conditions. Towards more privacy-preserving and robust tracking manner, recent works have exploited commodity radio frequency signals to realize 3D human posture tracking. However, these studies cannot handle the case where multiple users are in the same space. In this paper, we present a <u>mm</u>Wave-based <u>m</u>ulti-user 3D posture tracking system, m3Track, which leverages a single commercial off-the-shelf (COTS) mmWave radar to track multiple users' postures simultaneously as they move, walk, or sit. Based on the sensing signals from a mmWave radar in multi-user scenarios, m3Track first separates all the users on mmWave signals. Then, m3Track extracts shape and motion features of each user, and reconstructs 3D human posture for each user through a designed deep learning model. Furthermore. m3Track maps the reconstructed 3D postures of all users into 3D space, and tracks users' positions through a coordinate-corrected tracking method, realizing practical multi-user 3D posture tracking with a COTS mmWave radar. Experiments conducted in real-world multi-user scenarios validate the accuracy and robustness of m3Track on multi-user 3D posture tracking.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {491–503},
numpages = {13},
keywords = {radars, multi-user scenarios, mmWave, 3D posture tracking},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538947,
author = {Xie, Dianhan and Wang, Xudong and Tang, Aimin},
title = {MetaSight: localizing blocked RFID objects by modulating NLOS signals via metasurfaces},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538947},
doi = {10.1145/3498361.3538947},
abstract = {It remains a challenging issue to localize blocked RFID objects. Existing solutions rely on non-line-of-sight (NLOS) signals reflected from environments, which are not reliable and cannot be accurately measured. To eliminate such limitations, this paper develops a new approach (called MetaSight) that localizes blocked objects by modulating NLOS signals via metasurfaces. More specifically, a programmable metasurface is designed such that each reflecting element can dynamically change frequencies and phases of reflected signals. With proper setting of frequencies and phases within a certain range of reflecting elements (i.e., the reflecting window) by a reflection beamforming algorithm, a unique NLOS signal path (called metasurface path or MS path) is established between an object and its reader. By shifting the reflecting window in time domain, multiple MS paths and the corresponding channel coefficients can be obtained sequentially. To make the sequential process fast, both the preamble and the payload of a frame are utilized for channel estimation. Based on the estimated channel coefficients of multiple MS paths, the 3D direction of the object viewed from the metasurface is derived through a 3D direction estimation algorithm. By consolidating the 3D directions from two separate metasurfaces, the object's 3D location is finally determined. MetaSight is implemented as a prototype system. Extensive experiments show that MetaSight can localize a blocked object with an average accuracy of 15 cm.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {504–516},
numpages = {13},
keywords = {non-light-of-sight, metasurface, localization, RFID},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538762,
author = {He, Liang and Shin, Kang G.},
title = {Battery-enabled vehicle immobilizer},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538762},
doi = {10.1145/3498361.3538762},
abstract = {To enhance anti-theft protection of vehicles, we present Battery Sleuth (Bleuth), a novel "physical" vehicle immobilizer that is immune to the common cyber-attack vectors of car keys/keyfobs. Bleuth uses the common 12V vehicle batteries to authenticate the driver with encrypted power-line communication and then control the vehicle's drivability by regulating the battery output power based on the authentication results, hence (im)mobilizing the vehicle without requiring drivers to carry any additional token.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {517–518},
numpages = {2},
keywords = {vehicle immobilizer, power-line communication, automotive batteries},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538763,
author = {Tian, Deyu and Shen, Haiyang and Ma, Yun},
title = {Parallelizing DNN inference in mobile web browsers on heterogeneous hardware},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538763},
doi = {10.1145/3498361.3538763},
abstract = {Mobile Web apps are emerging to leverage DNN models to provide intelligent user experience. But the limited functionalities for heterogeneous hardware provided by mobile Web browsers challenge the Web apps to perform DNN inference efficiently. In this paper, we propose a novel DNN inference engine, named PipeEngine, to parallelize the DNN inference process on CPU and GPU in mobile Web browsers. The design of PipeEngine enables pipeline parallelism between two adjacent DNN inference tasks with heterogeneous hardware. Evaluation results show that PipeEngine can increase the inference throughput by up to 2.77\texttimes{}.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {519–520},
numpages = {2},
keywords = {mobile browsers, heterogeneous hardware, DNN inference},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538764,
author = {Ren, Yili and Wang, Zi and Wang, Beiyu and Tan, Sheng and Yang, Jie},
title = {Liquid level detection using wireless signals},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538764},
doi = {10.1145/3498361.3538764},
abstract = {Sensing the liquid level in a container is critical to building many smart home and mobile healthcare applications. This paper presents a liquid level sensing system that is low-cost, high accuracy, widely applicable to different daily liquids and containers, and can be easily integrated with existing smart home networks. Our system uses an existing home WiFi network and a low-cost transducer that is attached to the container to sense the resonance of the container for liquid level detection. We evaluate our system in home environments with various containers and liquids. Preliminary results show that our system achieves an accuracy of 97\% for continuous prediction and an F-score of 0.968 for discrete prediction.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {521–522},
numpages = {2},
keywords = {smart home, liquid sensing, channel state information (CSI)},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538765,
author = {Bai, Yang and Garg, Nakul and Roy, Nirupam},
title = {Ultra-low-power acoustic imaging},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538765},
doi = {10.1145/3498361.3538765},
abstract = {This poster presents the design and implementation of SPiDR, an ultra-low-power acoustic imaging system. This imaging system produces a cross-sectional map of the field-of-view using only one speaker/microphone pair. It leverages the fact that sound's interaction with small structures can project spatially coded signals on a region at a fine granularity. We create a 3D-printed passive filter, called a stencil, that can image the scene with a single omnidirectional source and sensor. With spatially coded signal, the system receives a linear combination of the reflections from nearby objects and applies a novel power-aware depth-map reconstruction algorithm. SPiDR consumes only 10mW of power to generate a depth-map in real-world scenario with over 80\% structural similarity score with the scene.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {523–524},
numpages = {2},
keywords = {spatial sensing, low-power sensing, acoustic metamaterial, IoT},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538766,
author = {Chung, Chia-Chin and Pai, Chu-Chi and Ching, Fu-Shiang and Wang, Chao and Chen, Ling-Jyh},
title = {When post-quantum cryptography meets the internet of things: an empirical study},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538766},
doi = {10.1145/3498361.3538766},
abstract = {Adapting to Post-Quantum Cryptography (PQC) is an inevitable shift due to stable quantum computers getting closer to fruition. As our lives are getting more entangled with Internet of Things (IoT) technologies, a quantum computer attack will be even more devastating. In this work, we evaluate the performance of PQC algorithms on IoT devices. Our empirical results show that PQC algorithms, besides their tolerance against quantum computer attacks, have the potential to be on par with typical asymmetric encryption algorithms from the TLS protocol.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {525–526},
numpages = {2},
keywords = {security, post-quantum cryptography, internet of things, TLS, MQTT},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538767,
author = {Nambiappan, Harish Ram and Karim, Enamul and Saurav, Jillur Rahman and Srivastav, Anushka and Makedon, Fillia},
title = {Edge-IoT framework for speech and mobile-based human-robot interaction},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538767},
doi = {10.1145/3498361.3538767},
abstract = {In this paper, a novel Edge-IoT framework is proposed which interconnects the user's smartphones with the robotic system for human-robot interaction. The user sends speech commands using mobile application which is processed by the framework and sent to the robotic system. The robotic system executes the task and sends a completion message to the user through the framework. Preliminary experiments conducted by sending multiple requests to the framework to perform a robotic task showed that the framework is able to handle multiple requests, send commands to the robotic system to perform the robotic task and send completion messages to the user efficiently.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {527–528},
numpages = {2},
keywords = {internet-of-things, human robot interaction, edge computing},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538768,
author = {Zhang, Tianfang and Shi, Cong and Zhao, Tianming and Ye, Zhengkun and Walker, Payton and Saxena, Nitesh and Wang, Yan and Chen, Yingying},
title = {Personalized health monitoring via vital sign measurements leveraging motion sensors on AR/VR headsets},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538768},
doi = {10.1145/3498361.3538768},
abstract = {Augmented reality/virtual reality (AR/VR) headsets have attracted millions of users and gained predictable popularity. However, long-period usage of immersive technology may lead to health issues (e.g., cybersickness, anxiety). In this poster, we design a low-cost and personalized healthcare monitoring system grounded on vital sign tracking (i.e., breathing and heartbeat rate tracking), by exploiting built-in AR/VR motion sensors. The key insight is that the conductive vibrations induced by chest and heart movements can propagate through the user's cranial bones, thereby vibrating the AR/VR headset mounted on the user's head. To realize this system, we design signal processing techniques to cancel the human motions and derive the periods of breathing and heartbeat through frequency-domain analyses. We further design a user identification scheme based on respiratory and cardiac biometrics, which works with vital sign monitoring to provide personalized healthcare recommendations. Our experiment shows that the proposed scheme can achieve less than 5.7\% error rate on breathing/heartbeat rate estimation and 95\% accuracy on user identification.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {529–530},
numpages = {2},
keywords = {vital sign monitoring, user identification, AR/VR headsets},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538769,
author = {Seo, Hyuna and Yi, Juheon and Lee, Youngki},
title = {LIVE: life-immersive virtual environment with physical interaction-aware adaptive blending},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538769},
doi = {10.1145/3498361.3538769},
abstract = {We present LIVE, a system enabling a life-immersive Mixed Reality experience. Daily MR usage is challenging in that the user's interaction state with the physical objects continuously change over time, while the immersion and the utility should be supported simultaneously in the process. As many works of blending the virtual and physical world are designed for a single interaction state, they are not enough to support life-immersive MR. We propose the initial design of LIVE that (i) selects the current user's context among the three states of interaction with physical object and (ii) applies the most suitable blending method to balance immersion and the utility.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {531–532},
numpages = {2},
keywords = {mixed reality, interaction-aware system design, augmented virtuality},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538770,
author = {Feng, Chao and Zhang, Yangfan and Wang, Xiaojing and Li, Xinyi},
title = {a low-cost and reconfigurable metasurface for mmWave networks: poster abstract},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538770},
doi = {10.1145/3498361.3538770},
abstract = {Millimeter-wave (mmWave) technology is emerging as the most promising candidate to support a wide range of applications with high data rate demand. However, due to the high directivity of mmWaves, its links are highly susceptible to barriers from walls and the movement of people. To address these issues, this paper introduces a low-cost and reconfigurable metasurface placed in the environment to reshape and resteer mmWave beams. The metasurface consists of many unit-cells, each acting as a phase shifter for signals going through it. By encoding the phase shifting values, the metasurface can reshape and resteer mmWave beams, thereby enabling a fast mmWave beam relay through the wall or redirects the beam power to another direction when a human body blocks the line-of-sight path. Preliminary simulated results show our designed metasurface can perform accurate beam steering within a field-of-view of [-60°, 60°]. And even with the small-size prototype (16 \texttimes{} 16 array of unit-cells), the metasurface enables up to 10.8 dB signal strength improvement.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {533–534},
numpages = {2},
keywords = {reconfigurable intelligent surfaces, mmWave, metasurface},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538771,
author = {Yuan, Longzhi and Gong, Wei},
title = {High-throughput backscatter using commodity wifi},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538771},
doi = {10.1145/3498361.3538771},
abstract = {Existing backscatter systems using commodity radios all have a limited throughput, which prevents their usage in high-rate scenarios. We present SubScatter, which realizes a high throughput and keeps commodity-radio compatibility at the same time. Specifically, SubScatter boosts throughput for 11\texttimes{} by using 802.11b WiFi as excitation (11/8\texttimes{}) and designing sub-symbol modulation (8\texttimes{}).},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {535–536},
numpages = {2},
keywords = {wifi, internet of things, backscatter},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538772,
author = {Phyu, Hnin Pann and Naboulsi, Diala and Stanica, Razvan},
title = {Privacy-aware decentralized multi-slice traffic forecasting},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538772},
doi = {10.1145/3498361.3538772},
abstract = {In this work, taking the perspective of Mobile Virtual Network Operators (MVNOs), we tackle the multi-slice traffic forecasting problem, while respecting the data privacy of users. To this end, we propose the Federated Proximal Long Short-Term Memory (FPLSTM) framework, which allows MVNOs to train at each base station their local models with their private datasets, without compromising data privacy. Prediction results obtained by evaluating the models on a real-world dataset indicate that the forecast of FPLSTM is as accurate as state-of-the-art solutions while ensuring data privacy as well as computation and communication costs efficiency.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {537–538},
numpages = {2},
keywords = {traffic forecasting, network slicing, machine learning, data privacy},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538773,
author = {Adhikari, Aakriti and Avula, Siri and Sur, Sanjib},
title = {mmSleep: monitoring sleep posture from commodity millimeter-wave devices},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538773},
doi = {10.1145/3498361.3538773},
abstract = {We propose mmSleep, a millimeter-wave (mmWave) wireless signal based sleep posture monitoring system that can assist in tracking 3D location of body joints of a person during sleep. mmSleep overcomes the limitations of existing vision-based sleep monitoring and can work under low-light without being privacy-invasive. mmSleep uses a customized Convolutional Neural Network to learn diverse sleep postures, and our preliminary results show that mmSleep can consistently predict 3D joint locations with high accuracy.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {539–540},
numpages = {2},
keywords = {toss-turn, sleep posture monitoring, millimeter-wave, convolutional neural network},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538774,
author = {Xie, Yucheng and Jiang, Ruizhe and Guo, Xiaonan and Wang, Yan and Cheng, Jerry and Chen, Yingying},
title = {Universal targeted attacks against mmWave-based human activity recognition system},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538774},
doi = {10.1145/3498361.3538774},
abstract = {Millimeter wave (mmWave)-based human activity recognition (HAR) systems have emerged in recent years due to their better privacy preservation and higher-resolution sensing. However, these systems are vulnerable to adversarial attacks. In this work, we propose a universal targeted attack method for mmWave-based HAR system. In particular, a universal perturbation is generated in advance which can be added to new-coming mmWave data to deceive the HAR system, causing it to output our desired label. We validate our proposed attack using a public mmWave dataset. We demonstrate the effectiveness of our proposed universal attack with a high attack success rate of over 95\%.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {541–542},
numpages = {2},
keywords = {universal targeted attack, millimeter wave, adversarial learning},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538775,
author = {Schellberg, Jacqueline M and Sur, Sanjib},
title = {Accurate device self-tracking for robust millimeter-wave imaging on handheld smart devices},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538775},
doi = {10.1145/3498361.3538775},
abstract = {Millimeter-wave (mmWave) imaging has been difficult to implement on handheld devices since imaging algorithms rely on millimeter-scale device self-tracking, which existing systems cannot achieve reliably. We propose CompenSAR, a handheld system which integrates a mmWave transceiver and a tracking camera, and overcomes the self-tracking limitations to enable handheld mmWave imaging.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {543–544},
numpages = {2},
keywords = {synthetic aperture radar, millimeter-wave, experimental platform},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538776,
author = {Kodukula, Venkatesh and Manetta, Mason and LiKamWa, Robert},
title = {Adaptive voltage scaling to balance energy savings and image quality in cameras},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538776},
doi = {10.1145/3498361.3538776},
abstract = {Energy-efficient visual sensing is extremely important to enable battery powered mobile and IoT applications. While several scheduling techniques have been proposed to save the digital power of sensing, the analog power [1] of capturing an image still remains a daunting barrier for a camera's energy-efficiency. To that end, we characterize the power and performance implications of analog voltage scaling on off-the-shelf image sensors. Our characterization reveals that while reducing the analog voltage supplied to image sensor helps promote sensor power efficiency, it also impairs imaging fidelity, specifically by making images brighter and noisier. Furthermore, we find that brighter and noisier images situationally affect the task accuracy of vision applications. In this poster, we propose an investigation towards a system that adaptively scales analog voltage to optimize sensor energy, while respecting the fidelity needs of visual tasks.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {545–546},
numpages = {2},
keywords = {voltage scaling, image sensors, energy-efficiency},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538777,
author = {Wong, Aslan B. and Huang, ZiQi and Wu, Kaishun},
title = {Leveraging speech and ultrasonic signals toward articulation-based smartphone user authentication},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538777},
doi = {10.1145/3498361.3538777},
abstract = {This paper presents a breakthrough framework for smartphone user authentication by analyzing the physiology and behavior of articulation, namely the vocal tract, tongue position, and lip movement, to expose individual uniqueness during uttering. The main idea is to leverage a smartphone's speaker and microphone to transmit and receive speech and ultrasonic signals, construct identity-related features, and determine whether the samples are legitimate users or attackers. The proposed system requires a smaller number of samples by utilizing single utterances resulting that the system resisting playback and mimicry attacks with an average accuracy of 99\% in three different scenarios.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {547–548},
numpages = {2},
keywords = {mobile interface, human-computer interaction, authentication, articulation},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538778,
author = {Lu, Xiaofeng and Zhao, Jinglun and Lio, Pietro},
title = {Robust android malware detection based on subgraph network and denoising GCN network},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538778},
doi = {10.1145/3498361.3538778},
abstract = {This paper proposes an Android malware detection model based on Android Function Call Graph (FCG) and Denoising Graph Convolutional Neural Network. This study proposes a method to simplify the FCG to reduce its size, and a new method to construct vertex feature vectors. The model uses the subgraph network to detect the underlying structural features of the FCG and discover the confusion attack. A denoising graph neural network is applied to graph convolution to reduce the impact of obfuscation attacks.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {549–550},
numpages = {2},
keywords = {graph convolutional neural network, call graph, adversarial attack, Android malware detection},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538779,
author = {Sun, Bangjie and Tan, Sean Rui Xiang and Ren, Zhiwei and Chan, Mun Choon and Han, Jun},
title = {On utilizing smartphone cameras to detect counterfeit liquid food products},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538779},
doi = {10.1145/3498361.3538779},
abstract = {Counterfeit liquid food products, including olive oil, honey and alcohol, are continuing to pose severe threats to the general public as counterfeiters adulterate the authentic content with cheaper and potentially harmful substitutes, and package them in authentic bottles. Existing solutions are often impractical for the general public as they require specialized and costly equipment as well as taking liquid samples. We overcome these limitations by proposing LiquidHash, a novel detection system that only requires the use of a commodity smartphone to detect adulterated liquid products without opening the bottles. LiquidHash leverages computer vision and machine learning techniques to extract characteristics of air bubbles formed by flipping a bottle. We implement LiquidHash and evaluate its feasibility with real-world experiments and achieve an overall detection accuracy of up to 95\%.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {551–552},
numpages = {2},
keywords = {smartphone camera, liquid testing, counterfeit},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538780,
author = {Tavasoli, Reza and Sur, Sanjib and Nelakuditi, Srihari},
title = {SSCense: a millimeter-wave sensing approach for estimating soluble sugar content of fruits},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538780},
doi = {10.1145/3498361.3538780},
abstract = {Soluble Sugar Content (SSC) of a fruit is indicative of its ripeness and is used in the fruit industry for quality control in the production chain. We present the design and implementation of SSCense, a low-cost, non-destructive system to estimate a fruit's SSC using the millimeter-wave wireless technology in 5G-and-beyond devices.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {553–554},
numpages = {2},
keywords = {wireless sensing, soluble sugar content, millimeter-wave, fruit ripeness, fruit quality},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538781,
author = {Sitar, Edward M and Saadat, Moh Sabbir and Sur, Sanjib},
title = {A millimeter-wave wireless sensing approach for at-home exercise recognition},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538781},
doi = {10.1145/3498361.3538781},
abstract = {At-home exercise monitoring is vital to applications like rehabilitative care and physical therapy. In this work, we use millimeter-wave signal reflections to assess the exercise, where we classify the exercise type by designing a supervised deep learning model, and estimate the number of repetitions by leveraging phase information embedded in the reflections.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {555–556},
numpages = {2},
keywords = {wireless sensing, millimeter-wave, deep learning, activity recognition},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538782,
author = {Han, Zengyi and Dong, Xuefu and Nishiyama, Yuuki and Sezaki, Kaoru},
title = {Head dynamics enabled riding maneuver prediction},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538782},
doi = {10.1145/3498361.3538782},
abstract = {While micro-mobility brings convenience to the modern city, they also cause various social problems such as traffic accidents, casualties, and huge economic losses. Wearing protective equipment has become the primary recommendation for safe riding, but passive protection cannot prevent accidents from happening after all. Thus, timely predicting the rider's maneuver is essential for more active protection and buying more time to avoid potential accidents from happening. In this poster, we explore the feasibility of using riders' head dynamics to predict their riding maneuvers. Through ten participants' preliminary study, we observed that not only do riders' head movements appear ahead of their maneuvers but also head movement patterns are distinct with different maneuver intentions. We then construct a deep learning network using Long Short Term Memory, achieving 89\% of accuracy on maneuver prediction.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {557–558},
numpages = {2},
keywords = {wearable, mobile sensing, human activity recognition},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538783,
author = {Lee, Junhyub and Kim, Insu and Heo, Jeongwoo and Kim, Hyosu},
title = {Your tapstroke tells who you are: authenticating smartphone users with tapstroke-driven vibrations},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538783},
doi = {10.1145/3498361.3538783},
abstract = {In this paper, we present TapAuth, a novel smartphone user authentication system that leverages vibrations generated from a user's tap inputs. TapAuth is based on the observation that the vibrations have their unique characteristic depending on the user due to differences in finger structure. Specifically, TapAuth collects audio and motion data using only built-in sensors, while users are entering PIN code. Then, it authenticates the user by verifying the validity of the code and comparing the collected data with a pre-built training dataset. Our experimental results with 20 real-world users show that TapAuth can achieve high accuracy.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {559–560},
numpages = {2},
keywords = {tapstroke-based authentication, multi-factor authentication, mobile security enhancement, acoustic and imu sensing},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538784,
author = {Jangid, Nitesh Kumar and Gupta, Mukesh Kumar},
title = {Protecting software design in cloud using AWS IoT},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538784},
doi = {10.1145/3498361.3538784},
abstract = {Emerging technologies like Cloud Computing, Mobile Computing, IoT, AI, etc. are gaining momentum in the IT community. These novel technologies have changed the way to design secure and expert IT Systems for the future. Over the last decade scripting languages like PHP, Python, etc. are being used to develop applications due to ease of product development with moderate IT skills. Software design is not well protected for scripting languages due to the absence of source code translation. This risk is multifold if the IT System is deployed in a public cloud. The IoT Cloud services like AWS IoT may be used to redesign the security of IT Sytems by incorporating cost-effective and powerful IoT Computers like Raspberry Pi. We are proposing here a solution to protect software design by decoupling application code design and database design using hashing for database-name, table-name, and attribute-name of the application database. Security keys used in hashing are stored on a credit card size IoT Computer on business premises. IoT computer uses a separate AWS IoT secure channel to share the security keys on-demand in the cloud IT System when user logged in and keys are saved in session variable only. This proposal may be considered as adding client-trusted hardware into the public cloud.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {561–562},
numpages = {2},
keywords = {software design, raspberry Pi, hashing, IoT computer, IT system, AWS IoT},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538785,
author = {Yang, Qi and Dong, Xinran and Cao, Xiuqi and Ma, Yun},
title = {Adaptive compression of 3D models for mobile web apps},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538785},
doi = {10.1145/3498361.3538785},
abstract = {The rise of metaverse has driven the burst of 3D Web apps to provide immersive experience across heterogeneous devices. However, loading 3D models in Web apps is usually slow especially on mobile devices with limited computation capability and dynamic network condition, harming the experience of 3D mobile Web apps. In this paper, we propose an approach called 3Dispatcher to reducing the 3D model loading time by adaptively compressing the 3D models. Preliminary results show that 3Dispatcher can speed up the 3D model load by 15\%-41\% in different scenarios.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {563–564},
numpages = {2},
keywords = {model loading time, mobile web apps, 3D model compression},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538786,
author = {Liu, Yuxin (Myles) and Nakatsuka, Yoshimichi and Sani, Ardalan Amiri and Agarwal, Sharad and Tsudik, Gene},
title = {Vronicle: verifiable provenance for videos from mobile devices},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538786},
doi = {10.1145/3498361.3538786},
abstract = {An increasing number of mobile devices are incorporating cameras, allowing users to record videos at any time, anywhere. This opens up a wide variety of applications, most notably security-critical ones, where videos are used as evidence or include sensitive content. Examples of such applications include (but are not limited to): (i) citizen journalists recording important events (e.g., protests), (ii) courts using videos as evidence, and (iii) electronic legal contract-signing platforms using videos to identify signing users [1].},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {565–566},
numpages = {2},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538787,
author = {Dureppagari, Harish Kumar and Dinesha, Ujwal and Wu, Raini and Ganji, Santosh and Ko, Woo-Hyun and Shakkottai, Srinivas and Bharadia, Dinesh},
title = {Realtime intelligent control for NextG cellular radio access networks},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538787},
doi = {10.1145/3498361.3538787},
abstract = {RAN Intelligent Control (RIC) has developed in parallel with Open Radio Access Networks (O-RAN) as a means of utilizing newly available interfaces. Focus has been largely on non-realtime (non-RT: > 1 sec) dealing with RAN management and offline training, and near-realtime (near-RT: 10 ms to 1 sec) dealing with UE load balancing and RAN configuration. We contend that the true power of RIC can be unleashed only with realtime (RT: < 100 μs) measurement, optimization, and control of RAN resources, corresponding to the cellular transmission time interval (TTI: 125 μs to 1 ms).},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {567–568},
numpages = {2},
keywords = {reinforcement learning, RAN intelligent control},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538788,
author = {Salem, Amina Ben and Khalfoun, Besma and Mokhtar, Sonia Ben and Mashhadi, Afra},
title = {Quantifying fairness of federated learning LPPM models},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538788},
doi = {10.1145/3498361.3538788},
abstract = {Despite the great potential offered by Artificial Intelligence in the context of smart mobility, it comes with the greater challenge of preserving the privacy of users. Federated Learning (FL) has gained popularity as a privacy-friendly approach, however, an equally important aspect rarely addressed in the literature, is its fairness. In this work we audit a FL-based privacy-preserving model. We use Entropy to determine similarity within the system's input data and compare its value against that of the output to detect unfair treatment.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {569–570},
numpages = {2},
keywords = {privacy, federated learning, fairness},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538789,
author = {Kim, Sungtae and Lee, Donghun and Han, Jun},
title = {EarChew: towards identifying chewing side preference using earables},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538789},
doi = {10.1145/3498361.3538789},
abstract = {Chewing Side Preference (CSP) is a dental habit that causes one to consistently and predominantly chew on only one side of the mouth, potentially causing jawbone joint disorders. Unfortunately, the state-of-the-art solutions are limited in continuously monitoring the CSP in potential patients. Hence, we present EarChew, which utilizes an earable worn on the patient's ear to be able to identify the chewing side - i.e., left or right. EarChew utilizes the microphone embedded in the earable to capture the minute but inherent vibrations caused by each chewing action to be able to identify the chewing side. We present a preliminary evaluation with a participant chewing on almonds and demonstrate a promising preliminary result of 96\% average accuracy.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {571–572},
numpages = {2},
keywords = {earable, chewing side preference},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538790,
author = {Shi, Cong and Zhang, Tianfang and Xu, Zhaoyi and Li, Shuping and Yuan, Yichao and Petropulu, Athina and Wu, Chung Tse Michael and Chen, Yingying},
title = {Speech privacy attack via vibrations from room objects leveraging a phased-MIMO radar},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538790},
doi = {10.1145/3498361.3538790},
abstract = {Speech privacy leakage has long been a public concern. Through speech eavesdropping, an adversary may steal a user's private information or an enterprise's financial/intellectual properties, leading to catastrophic consequences. Existing non-microphone-based eavesdropping attacks rely on physical contact or line-of-sight between the sensor (e.g., a motion sensor or a radar) and the victim sound source. In this poster, we discover a new form of speech eavesdropping attack that senses minor speech-induced vibrations upon common room objects using mmWave. By integrating phasedarray and multiple-input and multiple-output (MIMO) on a single mmWave transceiver, our attack can capture and fuse micrometerlevel vibrations upon the surfaces of multiple objects to reveal speech content in a remote and non-line-of-sight fashion. We successfully demonstrate such an attack by developing a deep speech recognition scheme grounded on unsupervised domain adaptation. Without prior training on the victim's data, our attack can achieve a high success rate of over 90\% in recognizing simple speech content.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {573–574},
numpages = {2},
keywords = {speech privacy attack, phased-MIMO, mmWave sensing},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538791,
author = {Kouris, Alexandros and Venieris, Stylianos I. and Laskaridis, Stefanos and Lane, Nicholas D.},
title = {Adaptable mobile vision systems through multi-exit neural networks},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538791},
doi = {10.1145/3498361.3538791},
abstract = {Semantic segmentation constitutes the backbone of many mobile vision systems, spanning from robot navigation to augmented reality and teleconferencing. Frequently operating under stringent latency constraints within the limited resource envelope of embedded/mobile devices, optimising for efficient execution becomes important. To this end, we propose a framework for converting state-of-the-art segmentation models to MESS networks: specially trained CNNs that employ parametrised early exits along their depth. Upon deployment, the predictions of these exits can be exploited either in a dynamic (input-adaptive) way, to save computation during inference on easier samples; or in a static (device-adaptive) setting, to accommodate deployment under varying device capabilities without the need of retraining. Designing and training such networks naively can hurt performance. Thus, we propose a two-staged training process that pushes semantically important features early in the network. We co-optimise the number, placement and architecture of the attached segmentation heads, along with the exit policy, to adapt to the deployment scenario and application-specific requirements. Optimising for speed, MESS networks deliver latency gains of up to 2.65\texttimes{} over state-of-the-art methods with no accuracy degradation. Accordingly, optimising for accuracy, we achieve an improvement of up to 5.33 pp, under the same computational budget.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {575–576},
numpages = {2},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538792,
author = {Bhunia, S. and Stoleru, R. and Haroon, A. and Sagor, M. and Altaweel, A. and Chao, M. and Maurice, M. and Blalock, R.},
title = {EdgeKeeper: resilient and lightweight coordination for mobile edge computing systems},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538792},
doi = {10.1145/3498361.3538792},
abstract = {Mobile Edge Computing (MEC) is gaining significant interest from first responders and tactical teams. Typical cloud-based coordination (e.g., service discovery and coordination, device naming, authentication) does not work in MEC due to high user mobility. We design and implement EdgeKeeper to provide cloud-like service coordination to distributed edge computing applications for MEC systems. It maintains an edge cluster among devices and intelligently stores data on a group of replicas to guard against node failures/disconnections. We provide a full-system implementation of Edge-Keeper for Android and Linux platforms and evaluate it with MEC applications in a real-world wide-area search and rescue operation conducted by first responders.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {577–578},
numpages = {2},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538793,
author = {Semenova, Sofiya and Meshram, Pranay and Chase, Timothy and Ko, Steven Y. and Liu, Yu David and Ziarek, Lukasz and Dantu, Karthik},
title = {A modular, extensible framework for modern visual SLAM systems},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538793},
doi = {10.1145/3498361.3538793},
abstract = {Visual SLAM is a long-standing research area with many significant advances over the years. New systems typically build on previous contributions, but this requires significant development overhead, a highly detailed understanding of previous system implementations, and is rife with programming pitfalls. To enable fast experimentation and reduce the need for researchers to re-invent the wheel, we propose an extensible Visual SLAM framework with three features: modularity, seamless edge offloading, and safe concurrency.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {579–580},
numpages = {2},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538794,
author = {Alsaid, Mohammed and Slay, Tylor and Bulusu, Nirupama and Bass, Robert B.},
title = {K-anonymity applied to the energy grid of things distributed energy resource management system},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538794},
doi = {10.1145/3498361.3538794},
abstract = {The violation of information privacy in Smart Grids can be a significant barrier to customers' participation. Employing privacy protection models such as K-anonymity in a Smart Grid implementation adds desirable privacy guarantees. This work provides an approach to applying the Mondrian algorithm to ensure data within the system excludes Personally Identifiable Information. Results suggest that a dynamically generated generalization hierarchy minimizes information loss incurred by the anonymization process.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {581–582},
numpages = {2},
keywords = {security, privacy, k-anaonymity, distributed energy resources},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538795,
author = {Qian, Feng and Li, Bin},
title = {Boosting remote multi-user AR privacy through a magic rope},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538795},
doi = {10.1145/3498361.3538795},
abstract = {In remote Multi-user AR (MuAR), a user shares with remote users his/her physical environment, which is enhanced by computer-generated perceptual information. Despite its important role in Metaverse, a user may have serious privacy concerns for MuAR: the user may want to share only a portion, or may want to block certain areas in their environment. Today's object detection and AR tracking techniques fall short of reliably, efficiently, and accurately supporting this use case, in particular on an (already overloaded) headset without offloading to an edge/cloud server for privacy preservation purposes.In this poster, we propose a novel primitive called Magic Rope to boost remote MuAR privacy. A user employs a flexible rope to circle enclosed areas as whitelisted (expose externally) or blacklisted (exclude/blur from sharing). The rope has specially designed markers allowing an AR headset to accurately and efficiently detect its enclosed area, as well as to drastically reduce the tracking overhead. We are working on addressing several research questions such as the marker design, efficient detection of chained markers on a rope, occlusion handling, multi-rope connection, and human-computer interaction design. We also plan to develop a full-fledged prototype of Magic Rope and integrate it with real remote MuAR applications. We will conduct extensive evaluations (including an IRB-approved user study) on real AR headsets.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {583–584},
numpages = {2},
keywords = {privacy preservation, mobile augmented reality, magic rope},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538796,
author = {Chen, Thomas Y.},
title = {Multi-temporal deep learning-based social media analysis for disaster relief},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538796},
doi = {10.1145/3498361.3538796},
abstract = {In recent years, using social media images to assess natural disasters and the structural damage they cause has become an increasingly prevalent research approach. Particularly, deep learning methods have enabled rapid assessment of crisis situations on the ground. As social media platforms contain firsthand accounts of these devastating instances during and after events, they provide an unprecedented opportunity for disaster relief efforts. Comparatively, for example, while satellite imagery has been previously lauded for its effective use in training deep learning models, concerns about cost of obtainment and ease of access make social media, with its multitemporal properties, potentially more useful. These computational methods provide for rapid and efficient allocation of resources and personnel, saving lives and property and minimizing economic loss. In this position paper, we discuss how social media imagery, as extracted in real time from a variety of networks, can be the most useful source of data for disaster relief when combined with machine learning and computer vision techniques, enabling effective deployment pipelines in mobile applications for use by individuals, NGOs, and local governments in disaster-prone areas.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {585–586},
numpages = {2},
keywords = {social media, disaster relief, deep learning, datasets, data mining},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538797,
author = {Du, Pei and Bulusu, Nirupama},
title = {Indoor navigation for visually impaired people with vertex colored graphs},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538797},
doi = {10.1145/3498361.3538797},
abstract = {Visually impaired people face many daily encumbrances. Traditional visual enhancements do not suffice to navigate indoor environments. In this paper, we explore path finding algorithms such as Dijkstra and A* combined with graph coloring to find a safest and shortest path for visual impaired people to navigate indoors. Our mobile application is based on a database which stores the locations of several spots in the building and their corresponding label. Visual impaired people select the start and destination when they want to find their way, and our mobile application will show the appropriate path which guarantees their safety.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {587–588},
numpages = {2},
keywords = {shortest path finding algorithm, graph coloring},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538798,
author = {Zhao, Tianming and Ye, Zhengkun and Zhang, Tianfang and Shi, Cong and Mahdad, Ahmed Tanvir and Wang, Yan and Chen, Yingying and Saxena, Nitesh},
title = {Continuous blood pressure monitoring using low-cost motion sensors on AR/VR headsets},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538798},
doi = {10.1145/3498361.3538798},
abstract = {The Augmented reality/Virtual reality (AR/VR) industry has ushered in a period of rapid development. The next decade leaves a massive imagination for AR/VR in terms of end product form, software, content, applications, and user increment. The AR \& VR technology offers a gazillion of possibilities for smart healthcare. In this poster, we develop an innovative continuous blood pressure (CBP) estimation system leveraging the built-in motion sensors of AR/VR headsets for users. We design a deep learning-based PPG construction scheme using the motion sensor-based cardiac signal and estimate the continuous blood pressure using the regression model. Our experimental results show that our system can continuously estimate both systolic blood pressure (SBP) and diastolic blood pressure (DBP) with a mean error of less than 4 mmHg and 0.9 mmHg respectively within a day.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {589–590},
numpages = {2},
keywords = {continuous blood pressure monitoring, AR/VR headsets},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538799,
author = {Cifuentes-Urtubey, Federico and Kravets, Robin and Vasisht, Deepak},
title = {Defending wi-fi network discovery from time correlation tracking},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538799},
doi = {10.1145/3498361.3538799},
abstract = {To prevent tracking a Wi-Fi device based on its MAC address, several operating systems have adopted MAC address randomization to conceal its factory-assigned address. This feature benefits users when their devices scan for networks, but a flaw arises when timing between transmissions stays consistent despite MAC address randomization in use. We present a defense mechanism, implemented with the Netlink library, against a time correlation attack for probe request packets on Wi-Fi devices. We show how adding random jitter to probe request transmissions renders a timing correlation attack infeasible to track devices during network discovery.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {591–592},
numpages = {2},
keywords = {wi-fi, probe requests, privacy, MAC address randomization},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538800,
author = {Marefat, Alireza and Nishar, Abbaas Alif Mohamed and Karve, Nikhil and Ashok, Ashwin},
title = {OpenRadon lab: democratizing soil radon modeling and mapping},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538800},
doi = {10.1145/3498361.3538800},
abstract = {The goal of this research is to model the spatio-temporal dependencies of radon gas generation and movement underground. In this regard, we have embarked upon an interdisciplinary research effort that involves studying the dependencies of radon gas emanation with soil and environmental parameters. To this end we design, implement and deploy an innovative real-time sensor network, OpenRadon Lab, to develop a radon prediction model that maps its distribution along space and time. This network constitutes a soil-to-cloud wireless computing framework to enable machine learning assisted soil radon prediction, and optimized data offloading to conserve computing resources on the sensing devices.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {593–594},
numpages = {2},
keywords = {wireless, testbed, sensor, radon, measurement, cloud},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538651,
author = {Wozinski, Samuel and He, Liang and Shin, Kang G.},
title = {Protecting electric scooters from thefts using batteries},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538651},
doi = {10.1145/3498361.3538651},
abstract = {As the electric (e-) scooter market grows rapidly, e-scooter thefts are becoming a major issue. To mitigate this problem, we design, implement and evaluate BEAS, a <u>B</u>attery-<u>E</u>nabled <u>A</u>uthentication <u>S</u>ystem to provide e-scooters with an additional layer of anti-theft protection. Installed on commodity e-scooters as an add-on module, BEAS allows users to customize their passwords for activating e-scooters in the form of pre-defined on/off patterns of the scooter lamp, authenticates users by examining the password based on the thus-generated voltage, and immobilizes e-scooters based on the authentication results by controlling the battery output power. Compared to other existing/potential anti-theft solutions for e-scooters, BEAS has the advantages of being resistant to wireless hacking (i.e., a well-known cyber vulnerability of car keys/keyfobs) and convenient as users are not required to (un)install BEAS every time the e-scooter is parked or carry any additional tokens. We will demonstrate a proof-of-concept prototype of BEAS in this demo.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {595–596},
numpages = {2},
keywords = {immobilization, electric scooter, batteries},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538652,
author = {Bang, Jihoon and Baek, Seungwoo and Kim, Hanvit and Chung, Hyeonjin and Choi, Jaehoon and Kim, Sunwoo},
title = {Vision-aided 28 GHz mmWave transmission with joint tx-rx beam tracking for 5G communications},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538652},
doi = {10.1145/3498361.3538652},
abstract = {This paper presents the first real-world demonstration of a vision-aided 28 GHz mmWave transmission with a joint Tx-Rx beam tracking for 5G communications. This demonstration employs the architecture of the joint Tx-Rx beam tracking which is designed to update Tx and Rx beams simultaneously based on computer vision with deep learning model. For the demonstration setup, we build a complete end-to-end real-time 5G 28 GHz system testbed. Through this demonstration, we showcase how the presented vision-aided mmWave transmission can effectively sustain a high-throughput link despite Tx-Rx misalignment induced by user mobility. A demo video can be found at https://youtu.be/Mu6pxEYuvbY.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {597–598},
numpages = {2},
keywords = {millimeter-wave, computer vision, beam tracking, 5G testbed},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538653,
author = {Qian, Kun and Zhang, Xinyu},
title = {Fully passive 3D printed reflecting surface for millimeter-wave coverage expansion},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538653},
doi = {10.1145/3498361.3538653},
abstract = {This demonstration presents a working prototype of MilliMirror. This fully passive metasurface expands coverage blind spots of mmWave radios by reshaping and re-steering mmWave signals to any anomalous directions. The MilliMirror prototype consists of thousands of unit elements. A closed-form model is developed for efficient beam pattern synthesis. MilliMirror further explores 3D printing technology and metal deposition to achieve economical fabrication. MilliMirror prototype successfully establishes an indirect link between the WiGig transceivers, with the maximum gain over 10 dB.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {599–600},
numpages = {2},
keywords = {smart surface, phased array signal processing, millimeter-wave networks, metasurface, metamaterial, intelligent reflecting surface, 6G, 5G, 3D printing},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538654,
author = {Chatterjee, Ishan and Kim, Maruchi and Jayaram, Vivek and Gollakota, Shyamnath and Kemelmacher, Ira and Patel, Shwetak and Seitz, Steven M.},
title = {ClearBuds - wireless binaural earbuds for learning-based speech enhancement},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538654},
doi = {10.1145/3498361.3538654},
abstract = {We present ClearBuds, the first end-to-end hardware and software system that utilizes a neural network to enhance speech streamed from two wireless earbuds. Real-time speech enhancement for wireless earbuds requires high-quality sound separation and background cancellation, operating in real-time and on a mobile phone. Clear-Buds bridges state-of-the-art deep learning for blind audio source separation and in-ear mobile systems by making two key technical contributions: 1) a new wireless earbud design capable of operating as a synchronized, binaural microphone array, and 2) a lightweight dual-channel speech enhancement neural network that runs on a mobile device. Our demo will allow MobiSys attendees wear our earbuds, and experience noise suppression as they talk in a noisy environment. Companion video can be accessed using the link below:},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {601–602},
numpages = {2},
keywords = {real-time mobile deep learning, noise cancellation, earable computing, cascaded neural networks, binaural earbuds, audio source separation, audio and speech processing},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538655,
author = {Lee, Taeckyung and Chung, Hye-Young and Park, Sooyoung and Kim, Dongwhi and Lee, Sung-Ju},
title = {Real-time attention state visualization of online classes},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538655},
doi = {10.1145/3498361.3538655},
abstract = {We propose the design of real-time end-to-end attentional state prediction system that utilizes only a webcam from a student's mobile device to provide a graph visualization.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {603–604},
numpages = {2},
keywords = {visualization, online education, mind-wandering, attention},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3539693,
author = {Katevas, Kleomenis and Perino, Diego and Kourtellis, Nicolas},
title = {FLaaS - enabling practical federated learning on mobile environments},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3539693},
doi = {10.1145/3498361.3539693},
abstract = {Federated Learning (FL) [2] has emerged as a popular solution of Confidential Computing [3] to distributedly train a model on user devices, improving privacy and system scalability. Such privacy-preserving models can be used in wide range of applications, and especially in Telco networks [4]. However, there are no practical systems to easily enable FL training on mobile apps, and especially in an as-a-service fashion. In this demo, we implement and test FLaaS, our recently proposed end-to-end FL service [1]. FLaaS includes a client-side framework with app library and service, and a back-end server, to enable secure and easy to deploy intra- and inter-app FL model training on mobile environments.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {605–606},
numpages = {2},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538656,
author = {Carver, Charles J. and Shao, Qijia and Lensgraf, Samuel and Sniffen, Amy and Perroni-Scharf, Maxine and Gallant, Hunter and Li, Alberto Quattrini and Zhou, Xia},
title = {Sunflower: locating underwater robots from the air: video},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538656},
doi = {10.1145/3498361.3538656},
abstract = {Locating underwater robots is fundamental for enabling important underwater applications. The current mainstream method requires a physical infrastructure with relays on the water surface, which is largely ad-hoc, introduces a significant logistical overhead, and entails limited scalability. Our work, Sunflower, is the first system demonstrating wireless, 3D localization across the air-water interface and eliminates the need for any sensing additional infrastructure. This demonstration video [1] evaluates the Sunflower system on a mobile drone and mobile underwater robot, showing the underwater robot's continuous ground-truth trajectory along with the Sunflower estimates at six discrete positions.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {607–608},
numpages = {2},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538657,
author = {Chan, Justin and Gollakota, Shyamnath},
title = {Inner-ear cochlea testing with earphones},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538657},
doi = {10.1145/3498361.3538657},
abstract = {In this MobiSys demo we show a low-cost earphone based system that can screen for hearing loss with a cost of $10. Our system is designed to detect otoacoustic emissions (OAE) which are sounds generated when the outer hair cells move in a healthy cochlea and provide information about their function. OAE testing is commonly used as part of universal infant hearing screening protocols in high-income countries [1]. OAE equipment however is expensive hindering early hearing screening in developing countries that bear the disproportionate brunt of disabling hearing loss. Our design sends two pure tones through each of the headphone's ear-buds and records the distortion-product OAEs generated by the cochlea using a microphone. By running algorithms on a smartphone connected to the earphones using its headphone jack, we can detect distortion-product OAEs. Our device has been validated in a clinical study on 201 pediatric ears at oto-laryngology, hearing, and craniofacial clinics, across three different sites and achieved accuracies comparable to a commercial OAE device. In our demo, users will be encouraged to perform this quick test by listening to some tones in their ear through our device to check for the presence of OAEs in their ears.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {609–610},
numpages = {2},
keywords = {smartphones, otoacoustic emissions, mobile, hearing loss, earphones},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538658,
author = {Arun, Aditya and Chang, Tyler and Yu, Yizheng and Ayyalasomayajula, Roshan and Bharadia, Dinesh},
title = {Real-time low-latency tracking for UWB tags},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538658},
doi = {10.1145/3498361.3538658},
abstract = {Wide-scale adoption of VR/AR technologies in gaming, video conferencing, and for other remote telepresence applications demands limb tracking for a more immersive experience. In an attempt to bolster limb tracking, we present UWBTrac, a UWB + IMU based fusion tracker for VR applications. In this demo, and accompanying video1, we showcase this UWB tracker in comparison with HTC Vive VR trackers.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {611–612},
numpages = {2},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538659,
author = {Zhao, Renjie and Woodford, Timothy and Wei, Teng and Qian, Kun and Zhang, Xinyu},
title = {M-cube: an open-source millimeter-wave MIMO software radio for wireless communication and sensing},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538659},
doi = {10.1145/3498361.3538659},
abstract = {Millimeter-wave (mmWave) technologies represent a cornerstone for emerging wireless network infrastructure, and for RF sensing systems in security, health, and automotive domains. Through a MIMO array of phased arrays with hundreds of antenna elements, mmWave can boost wireless bit-rates to 100+ Gbps, and potentially achieve near-vision sensing resolution. However, the lack of an experimental platform has been impeding research in this field. We propose to fill the gap with M3 (M-Cube), the first mmWave massive MIMO software radio [1]. M3 features a fully reconfigurable array of phased arrays, with up to 8 RF chains and 256 antenna elements. Despite the orders of magnitude larger antenna arrays, its cost is orders of magnitude lower, even when compared with state-of-the-art single RF chain mmWave software radios. In this demo, we will show M3's hardware modules, and demonstrate its usage in mmWave MIMO communication and sensing.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {613–614},
numpages = {2},
keywords = {testbed, millimeter-wave, experimental platform, MIMO, 60 GHz},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538660,
author = {Wen, Jiqing and Gold, Lauren and Hu, Jinhan and Bahremand, Alireza and Shaikh, Aashiq and Farber, Charmaine and Dbeis, Yasser and Channar, Sameer and Richards, Connor and Hoang, Ryan and Spencer, Craig and Tang, Nick and LiKamWa, Robert},
title = {Adaptive 5G systems for interactive volumetric sports analysis in augmented reality},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538660},
doi = {10.1145/3498361.3538660},
abstract = {Remote coaching for sports is challenged by the lack of 3D spatial communication. While athletes send live or recorded videos to their coaches, these 2D representations fail to capture the spatial relationships of the body, limiting the ability to understand timing, weight distribution, and smoothness in an athletic movement. This demonstration presents Augmented Coach, an AR sports coaching platform for coaches to remotely view, manipulate, and annotate athletic movements in 3D augmented space. Also, this demonstration provides an adaptive platform to study real-time efficient volumetric data transmission between remotely connected devices, including over 5G cellular networks.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {615–616},
numpages = {2},
keywords = {volumetric data streaming, remote coaching, augmented reality},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538662,
author = {Subbaraman, Raghav and Bhaskar, Nishant and Crow, Sam and Khazraee, Moein and Schulman, Aaron and Bharadia, Dinesh},
title = {Observing wideband RF spectrum with low-cost, resource limited SDRs},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538662},
doi = {10.1145/3498361.3538662},
abstract = {Software Defined Radios (SDRs) combine a universal radio frontend with flexible processing. The radio frontend can be tuned to capture various wireless signals, while software processing allows quick and scalable deployment for diverse applications. SDRs seem like a good fit for the ever-evolving needs of today's spectrum usage: SDRs can be deployed today, then managed and upgraded with software to support the needs of tomorrow. However, the prevailing architecture of SDRs prevent real-time observation of wideband RF signals due to backhaul and processing resource constraints.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {617–618},
numpages = {2},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538663,
author = {Kim, Hyunjun and Ko, JeongGil},
title = {Memory-efficient DNN training on mobile devices},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538663},
doi = {10.1145/3498361.3538663},
abstract = {This demo is supplementary to the accepted submission #117 "Memory-efficient DNN Training on Mobile Devices", without an extended abstract. We present (1) federated learning and (2) model fine-tuning demo applications on Android smartphones which exploit the low-memory DNN training scheme featured in our paper.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {619},
numpages = {1},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538664,
author = {Zhang, Han and Agarwal, Yuvraj and Fredrikson, Matt},
title = {Protecting user data through ephemeral ownership of IoT devices},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538664},
doi = {10.1145/3498361.3538664},
abstract = {This demonstration presents a working prototype of TEO, a new model of device ownership that divides traditional owners into "admin" and "ephemeral owners". TEO addresses the challenge that users have no say in how their data are managed when the device is controlled by third parties, such as in rental and shared spaces. We design a complete protocol suite to address several practical issues, including preserving data ownership after users stop using the device, minimizing trusts with untrusted storage providers, enabling access control and revocation, and supporting groups of owners. Our cross-platform prototype implementation enables us to demonstrate the operational flow for managing ephemeral ownership of TEO-enabled devices in a representative setup with mobile phones, embedded devices, and Linux servers.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {620–621},
numpages = {2},
keywords = {stakeholder privacy, protocol verification, internet of things, ephemeral ownership, access control},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538665,
author = {Srivastava, Tanmay and Khanna, Prerna and Pan, Shijia and Nguyen, Phuc and Jain, Shubham},
title = {Leveraging earables for unvoiced command recognition},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538665},
doi = {10.1145/3498361.3538665},
abstract = {We demonstrate an ear-worn technology that recognizes unvoiced human commands by tracking jaw motion. The ear-worn system is designed to achieve continual unvoiced command recognition for robust human-computer interaction (HCI) applications. First, the system reliably extracts the jaw motion signals buried under the noise caused by head motion, walking, and other motion artifacts to track single secondary voice articulator (i.e., word). Then, learning from linguistics and human speech anatomy, we design a novel algorithm that localizes the phonemes in the command, and reconstructs the word. We evaluate the proposed system in real-world experiments with 15 volunteers. Our preliminary results show that the proposed system obtains a word recognition accuracy of 95.6\% in noise-free conditions and 93.2\% and 91.6\%, while head nodding and walking.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {622–623},
numpages = {2},
keywords = {wearable devices, unvoiced speech recognition, earable, IMU sensing},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538666,
author = {Chen, Haige and Yin, Zixin and Dhekne, Ashutosh},
title = {Location-specific public broadcasts},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538666},
doi = {10.1145/3498361.3538666},
abstract = {This demonstration presents the Location-Specific Public Broadcast system, in which localization and wireless broadcasts are combined to deliver a scalable, privacy preserving, and generic solution to location-based services. Other interactive location-based systems either preload information on the user devices, which are usually bulky, difficult to update and have to be custom-made for each venue, or fetch information from cloud based on location, which sacrifices user privacy. In our system, a wireless access point continuously broadcasts information tagged by locations of interest, and the mobile devices performing passive localization select and display the information pertinent to themselves. In this case, the location-specific information is stored only on the WiFi AP, and the phone app would be ultra lightweight with only the location calculation and information filtering functionalities, which can be used in any space. We envision our solution being adopted in public places, such as museums, aquariums, etc., for location-specific information delivery purposes, like enhancing interactive experience for visitors.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {624–625},
numpages = {2},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538667,
author = {Zhao, Minghui and Liu, Yanchen and Dhupar, Avik and Hou, Kaiyuan and Xia, Stephen and Jiang, Xiaofan},
title = {A modular and reconfigurable sensing and actuation platform for smarter environments and drones: demo abstract},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538667},
doi = {10.1145/3498361.3538667},
abstract = {There has been an immense growth in sensors, actuators, and smart devices in recent years, which enable us to better sense, actuate, and understand the physical world. Despite this growth, we have yet to achieve fully intelligent environments. This is, in part, due to the large number of different organizations creating smart devices with proprietary technologies and communication protocols that are not compatible with each other and require significant engineering to incorporate and adapt to specific applications. In this work, we present an easy-to-install and low-cost embedded platform that allows users to rapidly configure a mixture of sensors and actuators. The system is based on the commonly-used Raspberry Pi ecosystem, easily configurable, and does not require users to have prior knowledge of programming, which allows anyone, regardless of background, to use. We also introduce a battery-powered wireless extension module that is suitable for mobile drone applications, where a chord-powered Raspberry Pi is not suitable. We demonstrate the impact our system has on enabling drones with flexible sensing modalities and creating smarter environments by integrating our platform into a variety of intelligent home applications.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {626–627},
numpages = {2},
keywords = {sensor networks, drones, embedded systems},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538668,
author = {Liu, Yuxin (Myles) and Nakatsuka, Yoshimichi and Sani, Ardalan Amiri and Agarwal, Sharad and Tsudik, Gene},
title = {Vronicle: verifiable provenance for videos from mobile devices},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538668},
doi = {10.1145/3498361.3538668},
abstract = {An increasing number of mobile devices are incorporating cameras, allowing users to record videos at any time, anywhere. This opens up a wide variety of applications, most notably security-critical ones, where videos are used as evidence or include sensitive content. Examples of such applications include (but are not limited to): (i) citizen journalists recording important events (e.g., protests), (ii) courts using videos as evidence, and (iii) electronic legal contract-signing platforms using videos to identify signing users [1].},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {628–629},
numpages = {2},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538669,
author = {Ghasemi, Mahshid and Kleisarchaki, Sofia and Calmant, Thomas and G\"{u}rgen, Levent and Ghaderi, Javad and Kostic, Zoran and Zussman, Gil},
title = {Real-time camera analytics for enhancing traffic intersection safety},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538669},
doi = {10.1145/3498361.3538669},
abstract = {Crowded metropolises present unique challenges to the potential deployment of autonomous vehicles. Safety of pedestrians cannot be compromised and personal privacy must be preserved. Smart city intersections will be at the core of Artificial Intelligence (AI)-powered citizen-friendly traffic management systems for such metropolises. Hence, the main objective of this work is to develop an experimentation framework for designing applications in support of secure and efficient traffic intersections in urban areas. We integrated a camera and a programmable edge computing node, deployed within the COSMOS testbed in New York City, with an Eclipse sensiNact data platform provided by Kentyou. We use this pipeline to collect and analyze video streams in real-time to support smart city applications. In this demo, we present a video analytics pipeline that analyzes the video stream from a COSMOS' street-level camera to extract traffic/crowd-related information and sends it to a dedicated dashboard for real-time visualization and further assessment. This is done without sending the raw video, in order to avoid violating pedestrians' privacy.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {630–631},
numpages = {2},
keywords = {smart intersection, object detection, camera networks},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538670,
author = {Chan, Justin and Gollakota, Shyamnath},
title = {Laser speckle using smartphone LiDAR},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538670},
doi = {10.1145/3498361.3538670},
abstract = {In this MobiSys demo we present a system to determine fluid properties using the LiDAR sensors present on modern smartphones, as presented in our ACM IMWUT 2022 paper [1]. Traditional methods of measuring properties like viscosity require expensive laboratory equipment or a relatively large amount of fluid. In contrast, our smartphone-based method is accessible, contactless and works with just a single drop of liquid. Our design works by targeting a coherent LiDAR beam from the phone onto the liquid. Using the phone's camera, we capture the characteristic laser speckle pattern that is formed by the interference of light reflecting from light-scattering particles. By correlating the fluctuations in speckle intensity over time, we can characterize the Brownian motion within the liquid which is correlated with its viscosity. Our demo will allow MobiSys attendees to distinguish between liquids of different viscosities, milks of different fat contents, and adulterated milk.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {632–633},
numpages = {2},
keywords = {liquid testing, lidar, laser speckle},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538671,
author = {Liu, Yanchen and Zhao, Minghui and Xia, Stephen and Wu, Eugene and Jiang, Xiaofan},
title = {A sensorless drone-based system for mapping indoor 3D airflow gradients: demo abstract},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538671},
doi = {10.1145/3498361.3538671},
abstract = {With the global spread of the COVID-19 pandemic, ventilation indoors is becoming increasingly important in preventing the spread of airborne viruses. However, while sensors exist to measure wind speed and airflow gradients, they must be manually held by a human or an autonomous vehicle, robot, or drone that moves around the space to build an airflow map of the environment. In this demonstration, we present DAE, a novel drone-based system that can automatically navigate and estimate air flow in a space without the need of additional sensors attached onto the drone. DAE directly utilizes the flight controller data that all drones use to self-stabilize in the air to estimate airflow. DAE estimates airflow gradients in a room based on how the flight controller adjusts the motors on the drone to compensate external perturbations and air currents, without the need for attaching additional wind or airflow sensors.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {634–635},
numpages = {2},
keywords = {public health, pervasive sensing, edge computing, artificial intelligence},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538672,
author = {Choi, Ryuhaerang and Yun, Chanwoo and Cho, Hyunsung and Hong, Hwajung and Lee, Uichin and Lee, Sung-Ju},
title = {Facilitating instant interactions for stressful experiences sharing and peer support},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538672},
doi = {10.1145/3498361.3538672},
abstract = {We demonstrate StressTrendmeter, a mobile app that targets college students for anonymously sharing the source of stress via the form of hashtags, viewing stress topics based on trends, and providing social support through the empathy button and hashtag-based chat.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {636–637},
numpages = {2},
keywords = {smartphone-based interaction design},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3539694,
author = {Ghoshal, Moinak and Khan, Imran and Xu, Qiang and Kong, Z. Jonny and Hu, Y. Charlie and Koutsonikolas, Dimitrios},
title = {NextG-up: a tool for measuring uplink performance of 5G networks},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3539694},
doi = {10.1145/3498361.3539694},
abstract = {5G networks are being deployed rapidly across the world and have opened door to many uplink-oriented, bandwidth-intensive applications such as Augmented Reality and Connected Autonomous Vehicles (CAV). However, the roll-out is still in the early phase and the nature of deployment also varies across different geographic regions of the world. In this demo, we present NextG-UP, an open-source Android-based tool designed to help understand the performance and evolution of 5G networks around the world. The crowd-sourcing mobile app collects various cellular network metrics and runs a short uplink throughput/latency test.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {638–639},
numpages = {2},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538673,
author = {Adam, Shuaibu Musa and Thangarajan, Ashok Samraj and Liu, Mengyao and Hughes, Danny and Man, Ka Lok},
title = {BaMbI, a battery free and energy harvesting smartphone},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538673},
doi = {10.1145/3498361.3538673},
abstract = {The short lifespan of conventional smartphone batteries leads to toxic waste and increases replacement costs. In addition, contemporary devices require reliable electrical infrastructure which remains unavailable in major parts of the developing world. To address these problems, we propose a BAttery-free Mobile Interactive device (BaMbI), which aims to offer a scaled down set of smartphone features. BaMbI is based around an ARM Cortex M33 module with integrated LTE-M. In place of a battery, BaMbI uses a solar panel and an array of super-capacitors that offer sustainable operation and that can be fully charged in under one minute when mains power becomes available.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {640–641},
numpages = {2},
keywords = {low power, energy harvesting, battery free, NB-IoT, LTE-M},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538674,
author = {Sun, Hao and Dong, Chao and Qu, Yuben and Wu, Feiyu and Zhang, Lei and Wu, Qihui},
title = {IDEA: intelligent divine eye on air through multi-UAV collaborative inference},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538674},
doi = {10.1145/3498361.3538674},
abstract = {This demonstration shows a working prototype of IDEA, <u>I</u>ntelligent <u>D</u>ivine <u>E</u>ye on <u>A</u>ir through multi-UAV collaborative inference, to improve the throughput and accuracy of onboard object detection. Different from most existing UAV airborne object detection systems relying single UAV to run the convolutional neural networks (CNN)-based inference independently, IDEA leverages the abundant resources of multiple UAVs in a swarm, and collaboratively executes the inference task. Specifically, IDEA divides the CNN model into multiple submodels (each consisting of several successive layers), and distributes each submodel to a UAV, where the execution sequence of the submodels is coordinated to output the final prediction. The prominent advantage of IDEA lies in that, it can not only run highly accurate complex CNN models, but also perform the object detection tasks in a pipeline manner, which thus boosts high detection accuracy and frame rate. IDEA prototype with three self-constructed real-world UAVs shows ~2.6\texttimes{} frame rate improvement over that with one single UAV, while achieving higher detection accuracy.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {642–643},
numpages = {2},
keywords = {object detection, collaborative inference, UAV},
location = {Portland, Oregon},
series = {MobiSys '22}
}
@inproceedings{10.1145/3498361.3538675,
author = {Chan, Justin and Chen, Tuochao and Gollakota, Shyamnath},
title = {Underwater messaging using mobile devices},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538675},
doi = {10.1145/3498361.3538675},
abstract = {In this MobiSys demo, we present the demo of our SIGCOMM 2022 paper on underwater messaging system for existing mobile devices like smartphones and smart watches. Our software-only solution leverages audio sensors, i.e., microphones and speakers, ubiquitous in today's devices to enable acoustic underwater communication between mobile devices. To achieve this, we design a communication system that in real-time adapts to differences in frequency responses across mobile devices, changes in multipath and noise levels at different locations and dynamic channel changes due to mobility. Our demo will allow MobiSys attendees to test our system in realtime in a water tank using several demo smart devices in a waterproof pouch. We will also distribute the Android executable of the system via a QR code to attendees who wish to install and try the system on their own smart devices.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {644–645},
numpages = {2},
keywords = {underwater, smartwatches, smartphones, mobile, communication},
location = {Portland, Oregon},
series = {MobiSys '22}
}