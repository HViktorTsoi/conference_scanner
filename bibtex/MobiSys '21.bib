@inproceedings{10.1145/3458864.3467675,
author = {Xu, Jingao and Chi, Guoxuan and Yang, Zheng and Li, Danyang and Zhang, Qian and Ma, Qiang and Miao, Xin},
title = {FollowUpAR: enabling follow-up effects in mobile AR applications},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3467675},
doi = {10.1145/3458864.3467675},
abstract = {Existing smartphone-based Augmented Reality (AR) systems are able to render virtual effects on static anchors. However, today's solutions lack the ability to render follow-up effects attached to moving anchors since they fail to track the 6 degrees of freedom (6-DoF) poses of them. We find an opportunity to accomplish the task by leveraging sensors capable of generating sparse point clouds on smartphones and fusing them with vision-based technologies. However, realizing this vision is non-trivial due to challenges in modeling radar error distributions and fusing heterogeneous sensor data. This study proposes FollowUpAR, a framework that integrates vision and sparse measurements to track object 6-DoF pose on smartphones. We derive a physical-level theoretical radar error distribution model based on an in-depth understanding of its hardware-level working principles and design a novel factor graph competent in fusing heterogeneous data. By doing so, FollowUpAR enables mobile devices to track anchor's pose accurately. We implement FollowUpAR on commodity smartphones and validate its performance with 800,000 frames in a total duration of 15 hours. The results show that FollowUpAR achieves a remarkable rotation tracking accuracy of 2.3° with a translation accuracy of 2.9mm, outperforming most existing tracking systems and comparable to state-of-the-art learning-based solutions. FollowUpAR can be integrated into ARCore and enable smartphones to render follow-up AR effects to moving objects.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {1–13},
numpages = {13},
keywords = {6-DoF pose tracking, augmented reality, computer vision, mmWave radar},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3467676,
author = {Hu, Jinhan and Iosifescu, Andrei and LiKamWa, Robert},
title = {LensCap: split-process framework for fine-grained visual privacy control for augmented reality apps},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3467676},
doi = {10.1145/3458864.3467676},
abstract = {Augmented Reality (AR) enables smartphone users to interact with virtual content spatially overlaid on a continuously captured physical world. Under the current permission enforcement model in popular operating systems, AR apps are given Internet permission at installation time, and request camera permission and external storage write permission at runtime through a user's approval. With these permissions granted, any Internet-enabled AR app could silently collect camera frames and derived visual information for malicious intent without a user's awareness. This raises serious concerns about the disclosure of private user data in their living environments.To give users more control over application usage of their camera frames and the information derived from them, we introduce LensCap, a split-process app design framework, in which the app is split into a camera-handling visual process and a connectivity-handling network process. At runtime, LensCap manages secured communications between split processes, enacting fine-grained data usage monitoring. LensCap also allows both processes to present interactive user interfaces. With LensCap, users can decide what forms of visual data can be transmitted to the network, while still allowing visual data to be used for AR purposes on device. We prototype LensCap as an Android library and demonstrate its usability as a plugin in Unreal Engine. Performance evaluation results on five AR apps confirm that visual privacy can be preserved with an insignificant latency penalty (< 1.3 ms) at 60 FPS.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {14–27},
numpages = {14},
keywords = {AR application development, augmented reality security, split-process control, unreal engine, visual privacy},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3467886,
author = {Zhao, Yiqin and Guo, Tian},
title = {Xihe: a 3D vision-based lighting estimation framework for mobile augmented reality},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3467886},
doi = {10.1145/3458864.3467886},
abstract = {Omnidirectional lighting provides the foundation for achieving spatially-variant photorealistic 3D rendering, a desirable property for mobile augmented reality applications. However, in practice, estimating omnidirectional lighting can be challenging due to limitations such as partial panoramas of the rendering positions, and the inherent environment lighting and mobile user dynamics. A new opportunity arises recently with the advancements in mobile 3D vision, including built-in high-accuracy depth sensors and deep learning-powered algorithms, which provide the means to better sense and understand the physical surroundings. Centering the key idea of 3D vision, in this work, we design an edge-assisted framework called Xihe to provide mobile AR applications the ability to obtain accurate omnidirectional lighting estimation in real time.Specifically, we develop a novel sampling technique that efficiently compresses the raw point cloud input generated at the mobile device. This technique is derived based on our empirical analysis of a recent 3D indoor dataset and plays a key role in our 3D vision-based lighting estimator pipeline design. To achieve the realtime goal, we develop a tailored GPU pipeline for on-device point cloud processing and use an encoding technique that reduces network transmitted bytes. Finally, we present an adaptive triggering strategy that allows Xihe to skip unnecessary lighting estimations and a practical way to provide temporal coherent rendering integration with the mobile AR ecosystem. We evaluate both the lighting estimation accuracy and time of Xihe using a reference mobile application developed with Xihe's APIs. Our results show that Xihe takes as fast as 20.67ms per lighting estimation and achieves 9.4\% better estimation accuracy than a state-of-the-art neural network.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {28–40},
numpages = {13},
keywords = {3D vision, deep learning, edge inference, lighting estimation, mobile augmented reality},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3468161,
author = {Kim, Seyeon and Bin, Kyungmin and Ha, Sangtae and Lee, Kyunghan and Chong, Song},
title = {zTT: learning-based DVFS with zero thermal throttling for mobile devices},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3468161},
doi = {10.1145/3458864.3468161},
abstract = {DVFS (dynamic voltage and frequency scaling) is a system-level technique that adjusts voltage and frequency levels of CPU/GPU at runtime to balance energy efficiency and high performance. DVFS has been studied for many years, but it is considered still challenging to realize a DVFS that performs ideally for mobile devices for two main reasons: i) an optimal power budget distribution between CPU and GPU in a power-constrained platform can only be defined by the application performance, but conventional DVFS implementations are mostly application-agnostic; ii) mobile platforms experience dynamic thermal environments for many reasons such as mobility and holding methods, but conventional implementations are not adaptive enough to such environmental changes. In this work, we propose a deep reinforcement learning-based frequency scaling technique, zTT. zTT learns thermal environmental characteristics and jointly scales CPU and GPU frequencies to maximize the application performance in an energy-efficient manner while achieving zero thermal throttling. Our evaluations for zTT implemented on Google Pixel 3a and NVIDIA JETSON TX2 platform with various applications show that zTT can adapt quickly to changing thermal environments, consistently resulting in high application performance with energy efficiency. In a high-temperature environment where a rendering application with the default mobile DVFS fails to keep producing more than a target frame rate, zTT successfully manages to do so even with 23.9\% less average power consumption.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {41–53},
numpages = {13},
keywords = {DVFS, deep reinforcement learning, mobile devices},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3467681,
author = {Ouyang, Xiaomin and Xie, Zhiyuan and Zhou, Jiayu and Huang, Jianwei and Xing, Guoliang},
title = {ClusterFL: a similarity-aware federated learning system for human activity recognition},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3467681},
doi = {10.1145/3458864.3467681},
abstract = {Federated Learning (FL) has recently received significant interests thanks to its capability of protecting data privacy. However, existing FL paradigms yield unsatisfactory performance for a wide class of human activity recognition (HAR) applications since they are oblivious to the intrinsic relationship between data of different users. We propose ClusterFL, a similarity-aware federated learning system that can provide high model accuracy and low communication overhead for HAR applications. ClusterFL features a novel clustered multi-task federated learning framework that maximizes the training accuracy of multiple learned models while automatically capturing the intrinsic clustering relationship among the data of different nodes. Based on the learned cluster relationship, ClusterFL can efficiently drop out the nodes that converge slower or have little correlation with other nodes in each cluster, significantly speeding up the convergence while maintaining the accuracy performance. We evaluate the performance of ClusterFL on an NVIDIA edge testbed using four new HAR datasets collected from total 145 users. The results show that, ClusterFL outperforms several state-of-the-art FL paradigms in terms of overall accuracy, and save more than 50\% communication overhead at the expense of negligible accuracy degradation.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {54–66},
numpages = {13},
keywords = {clustering, communication optimization, federated learning, human activity recognition, multitask learning},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3467884,
author = {Li, Tianxing and Huang, Jin and Risinger, Erik and Ganesan, Deepak},
title = {Low-latency speculative inference on distributed multi-modal data streams},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3467884},
doi = {10.1145/3458864.3467884},
abstract = {While multi-modal deep learning is useful in distributed sensing tasks like human tracking, activity recognition, and audio and video analysis, deploying state-of-the-art multi-modal models in a wirelessly networked sensor system poses unique challenges. The data sizes for different modalities can be highly asymmetric (e.g., video vs. audio), and these differences can lead to significant delays between streams in the presence of wireless dynamics. Therefore, a slow stream can significantly slow down a multi-modal inference system in the cloud, leading to either increased latency (when blocked by the slow stream) or degradation in inference accuracy (if inference proceeds without waiting). In this paper, we introduce speculative inference on multi-modal data streams to adapt to these asymmetries across modalities. Rather than blocking inference until all sensor streams have arrived and been temporally aligned, we impute any missing, corrupt, or partially-available sensor data, then generate a speculative inference using the learned models and imputed data. A rollback module looks at the class output of speculative inference and determines whether the class is sufficiently robust to incomplete data to accept the result; if not, we roll back the inference and update the model's output. We implement the system in three multi-modal application scenarios using public datasets. The experimental results show that our system achieves 7 -- 128\texttimes{} latency speedup with the same accuracy as six state-of-the-art methods.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {67–80},
numpages = {14},
keywords = {cloud computing, computation off-loading, deep neural networks, edge computing},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3467882,
author = {Zhang, Li Lyna and Han, Shihao and Wei, Jianyu and Zheng, Ningxin and Cao, Ting and Yang, Yuqing and Liu, Yunxin},
title = {nn-Meter: towards accurate latency prediction of deep-learning model inference on diverse edge devices},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3467882},
doi = {10.1145/3458864.3467882},
abstract = {With the recent trend of on-device deep learning, inference latency has become a crucial metric in running Deep Neural Network (DNN) models on various mobile and edge devices. To this end, latency prediction of DNN model inference is highly desirable for many tasks where measuring the latency on real devices is infeasible or too costly, such as searching for efficient DNN models with latency constraints from a huge model-design space. Yet it is very challenging and existing approaches fail to achieve a high accuracy of prediction, due to the varying model-inference latency caused by the runtime optimizations on diverse edge devices.In this paper, we propose and develop nn-Meter, a novel and efficient system to accurately predict the inference latency of DNN models on diverse edge devices. The key idea of nn-Meter is dividing a whole model inference into kernels, i.e., the execution units on a device, and conducting kernel-level prediction. nn-Meter builds atop two key techniques: (i) kernel detection to automatically detect the execution unit of model inference via a set of well-designed test cases; and (ii) adaptive sampling to efficiently sample the most beneficial configurations from a large space to build accurate kernel-level latency predictors. Implemented on three popular platforms of edge hardware (mobile CPU, mobile GPU, and Intel VPU) and evaluated using a large dataset of 26,000 models, nn-Meter significantly outperforms the prior state-of-the-art.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {81–93},
numpages = {13},
keywords = {deep neural network, edge AI, inference latency prediction},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3466628,
author = {Mo, Fan and Haddadi, Hamed and Katevas, Kleomenis and Marin, Eduard and Perino, Diego and Kourtellis, Nicolas},
title = {PPFL: privacy-preserving federated learning with trusted execution environments},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3466628},
doi = {10.1145/3458864.3466628},
abstract = {We propose and implement a Privacy-preserving Federated Learning (PPFL) framework for mobile systems to limit privacy leakages in federated learning. Leveraging the widespread presence of Trusted Execution Environments (TEEs) in high-end and mobile devices, we utilize TEEs on clients for local training, and on servers for secure aggregation, so that model/gradient updates are hidden from adversaries. Challenged by the limited memory size of current TEEs, we leverage greedy layer-wise training to train each model's layer inside the trusted area until its convergence. The performance evaluation of our implementation shows that PPFL can significantly improve privacy while incurring small system overheads at the client-side. In particular, PPFL can successfully defend the trained model against data reconstruction, property inference, and membership inference attacks. Furthermore, it can achieve comparable model utility with fewer communication rounds (0.54\texttimes{}) and a similar amount of network traffic (1.002\texttimes{}) compared to the standard federated learning of a complete model. This is achieved while only introducing up to ~15\% CPU time, ~18\% memory usage, and ~21\% energy consumption overhead in PPFL's client-side.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {94–108},
numpages = {15},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3467962,
author = {Talebi, Seyed Mohammadjavad Seyed and Sani, Ardalan Amiri and Saroiu, Stefan and Wolman, Alec},
title = {MegaMind: a platform for security \& privacy extensions for voice assistants},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3467962},
doi = {10.1145/3458864.3467962},
abstract = {Voice assistants raise serious security and privacy concerns because they use always-on microphones in sensitive locations (e.g., inside a home) and send audio recordings to the cloud for processing. The cloud transcribes these recordings and interprets them as user requests, and sometimes even shares these requests with third-party services. These steps may result in unintended or malicious voice data leaks and in unauthorized actions, such as a purchase. This paper presents MegaMind, a novel extensible platform that lets a user deploy security and privacy extensions locally on their voice assistant. MegaMind's extensions interpose on requests before sending them to the cloud and on responses before delivering them to the user. MegaMind's programming model enables writing powerful extensions with ease, such as one for secure conversations. Additionally, MegaMind protects against malicious extensions by providing two important guarantees, namely permission enforcement and non-interference. We implement MegaMind and integrate it with Amazon Alexa Service SDK. Our evaluation shows that MegaMind achieves a small conversation latency on platforms with adequate compute power, such as a Raspberry Pi 4 and an x86-based laptop.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {109–121},
numpages = {13},
keywords = {extensibility, security and privacy, smart speakers, voice assistants},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3467887,
author = {Park, Chang Min and Kim, Donghwi and Sidhwani, Deepesh Veersen and Fuchs, Andrew and Paul, Arnob and Lee, Sung-Ju and Dantu, Karthik and Ko, Steven Y.},
title = {Rushmore: securely displaying static and animated images using TrustZone},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3467887},
doi = {10.1145/3458864.3467887},
abstract = {We present Rushmore, a system that securely displays static or animated images using TrustZone. The core functionality of Rushmore is to securely decrypt and display encrypted images (sent by a trusted party) on a mobile device. Although previous approaches have shown that it is possible to securely display encrypted images using TrustZone, they exhibit a critical limitation that significantly hampers the applicability of using TrustZone for display security. The limitation is that, when the trusted domain of TrustZone (the secure world) takes control of the display, the untrusted domain (the normal world) cannot display anything simultaneously. This limitation comes from the fact that previous approaches give the secure world exclusive access to the display hardware to preserve security. With Rushmore, we overcome this limitation by leveraging a well-known, yet overlooked hardware feature called an IPU (Image Processing Unit) that provides multiple display channels. By partitioning these channels across the normal world and the secure world, we enable the two worlds to simultaneously display pixels on the screen without sacrificing security. Furthermore, we show that with the right type of cryptographic method, we can decrypt and display encrypted animated images at 30 FPS or higher for medium-to-small images and at around 30 FPS for large images. One notable cryptographic method we adapt for Rushmore is visual cryptography, and we demonstrate that it is a light-weight alternative to other cryptographic methods for certain use cases. Our evaluation shows that in addition to providing usable frame rates, Rushmore incurs less than 5\% overhead to the applications running in the normal world.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {122–135},
numpages = {14},
keywords = {TrustZone, secure image display, visual cryptography},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3468220,
author = {Koh, John S. and Nieh, Jason and Bellovin, Steven M.},
title = {Encrypted cloud photo storage using Google photos},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3468220},
doi = {10.1145/3458864.3468220},
abstract = {Cloud photo services are widely used for persistent, convenient, and often free photo storage, which is especially useful for mobile devices. As users store more and more photos in the cloud, significant privacy concerns arise because even a single compromise of a user's credentials give attackers unfettered access to all of the user's photos. We have created Easy Secure Photos (ESP) to enable users to protect their photos on cloud photo services such as Google Photos. ESP introduces a new client-side encryption architecture that includes a novel format-preserving image encryption algorithm, an encrypted thumbnail display mechanism, and a usable key management system. ESP encrypts image data such that the result is still a standard format image like JPEG that is compatible with cloud photo services. ESP efficiently generates and displays encrypted thumbnails for fast and easy browsing of photo galleries from trusted user devices. ESP's key management makes it simple to authorize multiple user devices to view encrypted image content via a process similar to device pairing, but using the cloud photo service as a QR code communication channel. We have implemented ESP in a popular Android photos app for use with Google Photos and demonstrate that it is easy to use and provides encryption functionality transparently to users, maintains good interactive performance and image quality while providing strong privacy guarantees, and retains the sharing and storage benefits of Google Photos without any changes to the cloud service.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {136–149},
numpages = {14},
keywords = {Google photos, image encryption, key management, usable security},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3466627,
author = {Ibrahim, Muhammad and Imran, Abdullah and Bianchi, Antonio},
title = {SafetyNOT: on the usage of the SafetyNet attestation API in Android},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3466627},
doi = {10.1145/3458864.3466627},
abstract = {Many apps performing security-sensitive tasks (e.g., online banking) attempt to verify the integrity of the device they are running in and the integrity of their own code. To ease this goal, Android provides an API, called the SafetyNet Attestation API, that can be used to detect if the device an app is running in is in a "safe" state (e.g., non-rooted) and if the app's code has not been modified (using, for instance, app repackaging). In this paper, we perform the first large-scale systematic analysis of the usage of the SafetyNet API. Our study identifies many common mistakes that app developers make when attempting to use this API. Specifically, we provide a systematic categorization of the possible misusages of this API, and we analyze how frequent each misuse is. Our results show that, for instance, more than half of the analyzed apps check SafetyNet results locally (as opposed to using a remote trusted server), rendering their checks trivially bypassable. Even more surprisingly, we found that none of the analyzed apps invoking the SafetyNet API uses it in a fully correct way.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {150–162},
numpages = {13},
keywords = {API misusage, Android, SafetyNet, attestation, reverse engineering, tampering},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3467879,
author = {Chen, Xingyu and Liu, Jia and Xiao, Fu and Chen, Shigang and Chen, Lijun},
title = {Thermotag: item-level temperature sensing with a passive RFID tag},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3467879},
doi = {10.1145/3458864.3467879},
abstract = {Temperature sensing plays a significant role in upholding quality assurance and meeting regulatory compliance in a wide variety of applications, such as fire safety and cold chain monitoring. However, existing temperature measurement devices are bulky, cost-prohibitive, or battery-powered, making item-level sensing and intelligence costly. In this paper, we present a novel tag-based thermometer called Thermotag, which uses a common passive RFID tag to sense the temperature with competitive advantages of being low-cost, battery-free, and robust to environmental conditions. The basic idea of Thermotag is that the resistance of a semiconductor diode in a tag's chip is temperature-sensitive. By measuring the discharging period through the reverse-polarized diode, we can estimate the temperature indirectly. We propose a standards-compliant measurement scheme of the discharging period by using a tag's volatile memory and build a mapping model between the discharging period and temperature for accurate and reliable temperature sensing. We implement Thermotag using a commercial off-the-shelf RFID system, with no need for any firmware or hardware modifications. Extensive experiments show that the temperature measurement has a large span ranging from 0 °C to 85 °C and a mean error of 2.7 °C.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {163–174},
numpages = {12},
keywords = {passive RFID, persistence time, temperature sensing},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3467680,
author = {Ma, Dong and Ferlini, Andrea and Mascolo, Cecilia},
title = {OESense: employing occlusion effect for in-ear human sensing},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3467680},
doi = {10.1145/3458864.3467680},
abstract = {Smart earbuds are recognized as a new wearable platform for personal-scale human motion sensing. However, due to the interference from head movement or background noise, commonly-used modalities (e.g. accelerometer and microphone) fail to reliably detect both intense and light motions. To obviate this, we propose OESense, an acoustic-based in-ear system for general human motion sensing. The core idea behind OESense is the joint use of the occlusion effect (i.e., the enhancement of low-frequency components of bone-conducted sounds in an occluded ear canal) and inward-facing microphone, which naturally boosts the sensing signal and suppresses external interference. We prototype OESense as an earbud and evaluate its performance on three representative applications, i.e., step counting, activity recognition, and hand-to-face gesture interaction. With data collected from 31 subjects, we show that OESense achieves 99.3\% step counting recall, 98.3\% recognition recall for 5 activities, and 97.0\% recall for five tapping gestures on human face, respectively. We also demonstrate that OESense is compatible with earbuds' fundamental functionalities (e.g. music playback and phone calls). In terms of energy, OESense consumes 746 mW during data recording and recognition and it has a response latency of 40.85 ms for gesture recognition. Our analysis indicates such overhead is acceptable and OESense is potential to be integrated into future earbuds.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {175–187},
numpages = {13},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3466870,
author = {Sun, Wei and Srinivasan, Kannan},
title = {Healthy diapering with passive RFIDs for diaper wetness sensing and urine pH identification},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3466870},
doi = {10.1145/3458864.3466870},
abstract = {In this paper, we present RFDiaper, a commodity passive RFID based healthy diapering system, which can sense the diaper wetness (i.e., wet/dry) and identify pH value of urine absorbed by the diaper. To do so, we leverage the coupling effect between the urine absorbed by the diaper and RFID tag, thereby the phase and amplitude variation can indicate urine pH and diaper wetness. However, rich scattering and dynamic environment exhibit a great challenge for accurate diaper wetness sensing and urine pH identification. Therefore, we propose a twin-tag based dynamic environment mitigation approach for robust and healthy diapering. Specifically, by extracting the differential amplitude and phase from the co-located sensing tag and reference tag (i.e., twin-tag) attached on the diaper, the multipath effect and the other dynamic factors (e.g., diaper wearer's body, tag's orientation and temperature, etc.) can be mitigated. Then, we detect the diaper wetness and estimate the urine pH based on differential amplitude and phase. We have implemented RFDiaper's design and evaluated its effectiveness with the experiments using commercial off-the-shelf (COTS) RFID tags attached on the diaper worn by the doll and the human subjects. RFDiaper can achieve the median accuracy of around 96\% for diaper wetness sensing and urine pH estimation error of around 0.23 in dynamic environment.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {188–201},
numpages = {14},
keywords = {RFID sensing, commodity passive RFIDs, diaper wetness sensing, healthy diapering, urine pH identification},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3468012,
author = {Korany, Belal and Mostofi, Yasamin},
title = {Counting a stationary crowd using off-the-shelf wifi},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3468012},
doi = {10.1145/3458864.3468012},
abstract = {In this paper, we are interested in the problem of counting a crowd of stationary people (i.e., seated) using a pair of WiFi transceivers. While the people in the crowd are stationary, i.e. with no major body motion except breathing, people do not stay still for a long period of time and frequently engage in small in-place body motions called fidgets (e.g., adjusting their seating position, crossing their legs, checking their phones, etc). In this paper, we propose that the aggregate natural fidgeting and in-place motions of a stationary crowd carry crucial information on the crowd count. We then mathematically characterize the Probability Distribution Function (PDF) of the crowd fidgeting and silent periods (which we can extract from the received WiFi signal) and show their dependency on the total number of people in the area. In developing our mathematical models, we show how our problem of interest resembles a several-decade-old M/G/∞ queuing theory problem, which allows us to borrow mathematical tools from the literature on M/G/∞ queues. We extensively validate our proposed approach with a total of 47 experiments in four different environments (including through-wall settings), in which up to and including N = 10 people are seated. We further test our system in different scenarios, and with different activities, representing various engagement levels of the crowd, such as attending a lecture, watching a movie, and reading. Moreover, we test our proposed system with different number of people seated in several different configurations. Our evaluation results show that our proposed approach achieves a very high counting accuracy, with the estimated number of people being only 0 or 1 off from the true number 96.3\% of the time in non-through-wall settings, and 90\% of the time in through-wall settings. Our results show the potential of our proposed framework for crowd counting in real-world scenarios.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {202–214},
numpages = {13},
keywords = {crowd counting, occupancy estimation, wifi sensing},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3467683,
author = {He, Yan and He, Qiuye and Fang, Song and Liu, Yao},
title = {MotionCompass: pinpointing wireless camera via motion-activated traffic},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3467683},
doi = {10.1145/3458864.3467683},
abstract = {Wireless security cameras are integral components of security systems used by military installations, corporations, and, due to their increased affordability, many private homes. These cameras commonly employ motion sensors to identify that something is occurring in their fields of vision before starting to record and notifying the property owner of the activity. In this paper, we discover that the motion sensing action can disclose the location of the camera through a novel wireless camera localization technique we call MotionCompass. In short, a user who aims to avoid surveillance can find a hidden camera by creating motion stimuli and sniffing wireless traffic for a response to that stimuli. With the motion trajectories within the motion detection zone, the exact location of the camera can be then computed. We develop an Android app to implement MotionCompass. Our extensive experiments using the developed app and 18 popular wireless security cameras demonstrate that for cameras with one motion sensor, MotionCompass can attain a mean localization error of around 5 cm with less than 140 seconds. This localization technique builds upon existing work that detects the existence of hidden cameras, to pinpoint their exact location and area of surveillance.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {215–227},
numpages = {13},
keywords = {hidden camera, localization, motion sensor, wireless traffic analysis},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3467677,
author = {Gu, Zhihao and He, Taiwei and Yin, Junwei and Xu, Yuedong and Wu, Jun},
title = {TyrLoc: a low-cost multi-technology MIMO localization system with a single RF chain},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3467677},
doi = {10.1145/3458864.3467677},
abstract = {This work presents the design and implementation of TyrLoc, an accurate multi-technology switching MIMO localization system that can be deployed on low-cost SDRs. TyrLoc only uses a single RF Chain to switch on each antenna in an antenna array within the coherence time asynchronously, thus mimicking a MIMO platform to pinpoint the positions of WIFI, Bluetooth Low Energy (BLE) and LoRa devices. TyrLoc makes three key technical contributions. First, TyrLoc modifies the firmware of inexpensive PlutoSDR that controls the antenna switching pattern and tags the signal associated with each antenna. Second, it develops a two-stage fine-grained carrier frequency offset (CFO) calibration algorithm that harnesses the agile antenna switching pattern and is 10\texttimes{} more accurate than the baseline method. Third, TyrLoc employs an interpolated transform approach to facilitate angle-of-arrival (AoA) estimation in the presence of missing antennas. The AoA-based localization experiments in a multipath-rich indoor environment show that TyrLoc with eight antennas achieves the median errors of 63cm for WIFI, 39cm for BLE and 32cm for LoRa, respectively.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {228–240},
numpages = {13},
keywords = {CFO calibration, SDR, localization, switching MIMO},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3468850,
author = {Pizarro, Alejandro Blanco and Beltr\'{a}n, Joan Palacios and Cominelli, Marco and Gringoli, Francesco and Widmer, Joerg},
title = {Accurate ubiquitous localization with off-the-shelf IEEE 802.11ac devices},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3468850},
doi = {10.1145/3458864.3468850},
abstract = {WiFi location systems are remarkably accurate, with decimeter-level errors for recent CSI-based systems. However, such high accuracy is achieved under Line-of-Sight (LOS) conditions and with an access point (AP) density that is much higher than that typically found in current deployments that primarily target good coverage. In contrast, when many of the APs within range are in Non-Line-of-Sight (NLOS), the location accuracy degrades drastically.In this paper we present UbiLocate, a WiFi location system that copes well with common AP deployment densities and works ubiquitously, i.e., without excessive degradation under NLOS. UbiLocate demonstrates that meter-level median accuracy NLOS localization is possible through (i) an innovative angle estimator based on a Nelder-Mead search, (ii) a fine-grained time of flight ranging system with nanosecond resolution, and (iii) the accuracy improvements brought about by the increase in bandwidth and number of antennas of IEEE 802.11ac. In combination, they provide superior resolvability of multipath components, significantly improving location accuracy over prior work. We implement our location system on off-the-shelf 802.11ac devices and make the implementation, CSI-extraction tool and custom Fine Timing Measurement design publicly available to the research community. We carry out an extensive performance analysis of our system and show that it outperforms current state-of-the-art location systems by a factor of 2--3, both under LOS and NLOS.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {241–254},
numpages = {14},
keywords = {802.11ac, AoA, CSI, ToF, indoor localization, wireless networks},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3467880,
author = {Garg, Nakul and Bai, Yang and Roy, Nirupam},
title = {Owlet: enabling spatial information in ubiquitous acoustic devices},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3467880},
doi = {10.1145/3458864.3467880},
abstract = {This paper presents a low-power and miniaturized design for acoustic direction-of-arrival (DoA) estimation and source localization, called Owlet. The required aperture, power consumption, and hardware complexity of the traditional array-based spatial sensing techniques make them unsuitable for small and power-constrained IoT devices. Aiming to overcome these fundamental limitations, Owlet explores acoustic microstructures for extracting spatial information. It uses a carefully designed 3D-printed metamaterial structure that covers the microphone. The structure embeds a direction-specific signature in the recorded sounds. Owlet system learns the directional signatures through a one-time in-lab calibration. The system uses an additional microphone as a reference channel and develops techniques that eliminate environmental variation, making the design robust to noises and multipaths in arbitrary locations of operations. Owlet prototype shows 3.6° median error in DoA estimation and 10cm median error in source localization while using a 1.5cm \texttimes{} 1.3cm acoustic structure for sensing. The prototype consumes less than 100th of the energy required by a traditional microphone array to achieve similar DoA estimation accuracy. Owlet opens up possibilities of low-power sensing through 3D-printed passive structures.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {255–268},
numpages = {14},
keywords = {IoT, acoustic metamaterial, low-power sensing, spatial sensing},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3467679,
author = {Xue, Hongfei and Ju, Yan and Miao, Chenglin and Wang, Yijiang and Wang, Shiyang and Zhang, Aidong and Su, Lu},
title = {mmMesh: towards 3D real-time dynamic human mesh construction using millimeter-wave},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3467679},
doi = {10.1145/3458864.3467679},
abstract = {In this paper, we present mmMesh, the first real-time 3D human mesh estimation system using commercial portable millimeter-wave devices. mmMesh is built upon a novel deep learning framework that can dynamically locate the moving subject and capture his/her body shape and pose by analyzing the 3D point cloud generated from the mmWave signals that bounce off the human body. The proposed deep learning framework addresses a series of challenges. First, it encodes a 3D human body model, which enables mmMesh to estimate complex and realistic-looking 3D human meshes from sparse point clouds. Second, it can accurately align the 3D points with their corresponding body segments despite the influence of ambient points as well as the error-prone nature and the multi-path effect of the RF signals. Third, the proposed model can infer missing body parts from the information of the previous frames. Our evaluation results on a commercial mmWave sensing testbed show that our mmMesh system can accurately localize the vertices on the human mesh with an average error of 2.47 cm. The superior experimental results demonstrate the effectiveness of our proposed human mesh construction system.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {269–282},
numpages = {14},
keywords = {deep learning, human mesh estimation, human sensing, millimeter wave, point cloud},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3466862,
author = {Narayana, Sujay and Prasad, R. Venkatesha and Prabhakar, T. V.},
title = {SOS: isolated health monitoring system to <u>s</u>ave <u>o</u>ur <u>s</u>atellites},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3466862},
doi = {10.1145/3458864.3466862},
abstract = {With the advent of Space-IoTs, the rate of launch of satellites has grown significantly. Alongside, the failure rate of satellites has also surged increased tremendously. Satellites are non-repairable systems in orbit, and the financial loss incurred when the satellites fail before their expected mission time is substantial. If the source of a failure is known while the satellite is in orbit, then there is a possibility to revive it by sending appropriate commands from ground stations. In this work, we present a simple, independent satellite health monitoring system called Chirper. The Chirper is equipped with multiple modules such as IMU, isolated voltage and current measurement probes, and an onboard communication channel. We present a new approach to measure low DC voltages in an isolated way, providing a resolution and accuracy of around 1 V. We evaluated the design and performance of the Chirper through simulation, testing it in space systems test facility, and by mounting it on a helium balloon. With extensive experiments we show that 90\% of the time the dc voltage measurement error is within 0.8 V, and the maximum error is 0.9 V. We expect to launch the Chirper soon on a space system.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {283–295},
numpages = {13},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3466625,
author = {Hou, Kaiyu and Li, You and Yu, Yinbo and Chen, Yan and Zhou, Hai},
title = {Discovering emergency call pitfalls for cellular networks with formal methods},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3466625},
doi = {10.1145/3458864.3466625},
abstract = {Availability and security problems in cellular emergency call systems can cost people their lives, yet this topic has not been thoroughly researched. Based on our proposed Seed-Assisted Specification method, we start to investigate this topic by looking closely into one emergency call failure case in China. Using what we learned from the case as prior knowledge, we build a formal model of emergency call systems with proper granularity. By running model checking, four public-unaware scenarios where emergency calls cannot be correctly routed are discovered. Additionally, we extract configurations of two major U.S. carriers and incorporate them as model constraints into the model. Based on the augmented model, we find two new attacks leveraging the privileges of emergency calls. Finally, we present a solution with marginal overhead to resolve issues we can foresee.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {296–309},
numpages = {14},
keywords = {cellular network protocol, emergency call, formal methods, protocol formal verification, protocol specification},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3466869,
author = {Tambe, Arjun and Nambi, Akshay and Marathe, Sumukh},
title = {Is your smoke detector working properly? robust fault tolerance approaches for smoke detectors},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3466869},
doi = {10.1145/3458864.3466869},
abstract = {Billions of smoke detectors are in use worldwide to provide early warning of fires. Despite this, they frequently fail to operate in an ongoing fire, risking death and property damage. A significant fraction of faults result from drift, or reduced sensitivity, and other faults in smoke detectors' phototransistors (PTs). Existing approaches attempt to detect drift from the PT output in normal conditions (without smoke). However, we find that drifted PTs mimic the output of working PTs in normal conditions, but diverge in the presence of smoke, making this approach ineffective.This paper presents two novel approaches to systematically detect faults and measure and compensate for drift in smoke detectors' PTs. Our first approach, called FallTime, measures a PT "fingerprint," a unique electrical characteristic with distinct behavior for working, drifted, and faulty components. FallTime can be added to many existing smoke detector models in software alone, with no/minimal hardware modifications. Our second approach, DriftTestButton, is a mechanical test button that simulates the behavior of smoke when pressed. It offers a robust, straightforward approach to detect faults, and can measure and compensate for drift across the entire smoke detector system. We empirically evaluate both approaches and present extensive experimental results from actual smoke detectors deployed in a commercial building, along with custom-built smoke detectors. By conducting tests with live smoke, we show that both FallTime and DriftTestButton perform more effectively than existing fault tolerance techniques and stand to substantially reduce the risk that a smoke detector fails to alarm in the presence of smoke.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {310–322},
numpages = {13},
keywords = {drift detection, fault tolerance, phototransistors, smoke detectors},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3467682,
author = {Dash, Pranab and Hu, Y. Charlie},
title = {How much battery does dark mode save? an accurate OLED display power profiler for modern smartphones},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3467682},
doi = {10.1145/3458864.3467682},
abstract = {By omitting external lighting, OLED display significantly reduces the power draw compared to its predecessor LCD and has gained wide adoption in modern smartphones. The real potential of OLED in saving phone battery drain lies in exploiting app UI color design, i.e., how to design app UI to use pixel colors that result in low OLED display power draw. In this paper, we design and implement an accurate per-frame OLED display power profiler, PFOP, that helps developers to gain insight into the impact of different app UI design on its OLED power draw, and an enhanced Android Battery that helps phone users to understand and manage phone display energy drain, for example, from different app and display configurations such as dark mode and screen brightness. A major challenge in designing both tools is to develop an accurate and robust OLED display power model. We experimentally show that linear-regression-based OLED power models developed in the past decade cannot capture the unique behavior of OLED display hardware in modern smartphones which have a large color space and propose a new piecewise power model that achieves much better modeling accuracy than the prior-art by applying linear regression in each small regions of the vast color space. Using the two tools, we performed to our knowledge the first power saving measurement of the emerging dark mode for a set of popular Google Android apps.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {323–335},
numpages = {13},
keywords = {OLED display, dark mode, display power profiler, power modeling},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3467678,
author = {AlDuaij, Naser and Nieh, Jason},
title = {Tap: an app framework for dynamically composable mobile systems},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3467678},
doi = {10.1145/3458864.3467678},
abstract = {As smartphones and tablets have become ubiquitous, there is a growing demand for apps that can enable users to collaboratively use multiple mobile systems. We present Tap, a framework that makes it easy for users to dynamically compose collections of mobile systems and developers to write apps that make use of those impromptu collections. Tap users control the composition by simply tapping systems together for discovery and authentication. The physical interaction mimics and supports ephemeral user interactions without the need for tediously exchanging user contact information such as phone numbers or email addresses. Tapping triggers a simple NFC-based mechanism to exchange connectivity information and security credentials that works across heterogeneous networks and requires no user accounts or cloud infrastructure support. Tap makes it possible for apps to use existing mobile platform APIs across multiple mobile systems by virtualizing data sources so that local and remote data sources can be combined together upon tapping. Virtualized data sources can be hardware or software features, including media, clipboard, calendar events, and devices such as cameras and microphones. Leveraging existing mobile platform APIs makes it easy for developers to write apps that use hardware and software features across dynamically composed collections of mobile systems. We have implemented a Tap prototype that allows apps to make use of both unmodified Android and iOS systems. We have modified and implemented various apps using Tap to demonstrate that it is easy to use and can enable apps to provide powerful new functionality by leveraging multiple mobile systems. Our results show that Tap has good performance, even for high-bandwidth features, and is user and developer friendly.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {336–349},
numpages = {14},
keywords = {Android, distributed computing, iOS, mobile computing, mobile devices, operating systems, remote display},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3466866,
author = {Ramanujam, Murali and Madhyastha, Harsha V. and Netravali, Ravi},
title = {Marauder: synergized caching and prefetching for low-risk mobile app acceleration},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3466866},
doi = {10.1145/3458864.3466866},
abstract = {Low interaction response times are crucial to the experience that mobile apps provide for their users. Unfortunately, existing strategies to alleviate the network latencies that hinder app responsiveness fall short in practice. In particular, caching is plagued by challenges in setting expiration times that match when a resource's content changes, while prefetching hinges on accurate predictions of user behavior that have proven elusive. We present Marauder, a system that synergizes caching and prefetching to improve the speedups achieved by each technique while avoiding their inherent limitations. Key to Marauder is our observation that, like web pages, apps handle interactions by downloading and parsing structured text resources that entirely list (i.e., without needing to consult app binaries) the set of other resources to load. Building on this, Marauder introduces two low-risk optimizations directly from the app's cache. First, guided by cached text files, Marauder prefetches referenced resources during an already-triggered interaction. Second, to improve the efficacy of cached content, Marauder judiciously prefetches about-to-expire resources, extending cache lives for unchanged resources, and downloading updates for lightweight (but crucial) text files. Across a wide range of apps, live networks, interaction traces, and phones, Marauder reduces median and 90th percentile interaction response times by 27.4\% and 43.5\%, while increasing data usage by only 18\%.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {350–362},
numpages = {13},
keywords = {caching, mobile apps, performance, prefetching, smartphones},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3467881,
author = {Balasingam, Arjun and Gopalakrishnan, Karthik and Mittal, Radhika and Arun, Venkat and Saeed, Ahmed and Alizadeh, Mohammad and Balakrishnan, Hamsa and Balakrishnan, Hari},
title = {Throughput-fairness tradeoffs in mobility platforms},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3467881},
doi = {10.1145/3458864.3467881},
abstract = {This paper studies the problem of allocating tasks from different customers to vehicles in mobility platforms, which are used for applications like food and package delivery, ridesharing, and mobile sensing. A mobility platform should allocate tasks to vehicles and schedule them in order to optimize both throughput and fairness across customers. However, existing approaches to scheduling tasks in mobility platforms ignore fairness.We introduce Mobius, a system that uses guided optimization to achieve both high throughput and fairness across customers. Mobius supports spatiotemporally diverse and dynamic customer demands. It provides a principled method to navigate inherent tradeoffs between fairness and throughput caused by shared mobility. Our evaluation demonstrates these properties, along with the versatility and scalability of Mobius, using traces gathered from ridesharing and aerial sensing applications. Our ridesharing case study shows that Mobius can schedule more than 16,000 tasks across 40 customers and 200 vehicles in an online manner.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {363–375},
numpages = {13},
keywords = {aerial sensing, mobility platforms, optimization, resource allocation, ridesharing, vehicle routing},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3466867,
author = {Shen, Wen-Hsuan and Tsai, Hsin-Mu},
title = {RayTrack: enabling interference-free outdoor mobile VLC with dynamic field-of-view},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3466867},
doi = {10.1145/3458864.3466867},
abstract = {Connected autonomous vehicles have boosted a high demand on communication throughput in order to timely share the information collected by in-car sensors (e.g., LiDAR). While visible light communication (VLC) has shown its capability to offer Gigabit-level throughput for applications with high demand for data rate, most are performed indoors and the throughput of outdoor VLC drops to a few Mbps. To fill this performance gap, this paper presents RayTrack, an interference-free outdoor mobile VLC system. The key idea of RayTrack is to use a small but real-time adjustable FOV according to the transmitter location, which can effectively repel interference from the environment and from other transmitters and boost the system throughput. The idea also realizes virtual point-to-point links, and eliminates the need of link access control. To be able to minimize the transmitter detection time to only 20 ms, RayTrack leverages a high-compression-ratio compressive sensing scheme, incorporating a dual-photodiode architecture, optimized measurement matrix and Gaussian-based basis to increase sparsity. Real-world driving experiments show that RayTrack is able to achieve a data rate of 607.9 kbps with over 90\% detection accuracy and lower than 15\% bit error rate at 35 m, with 70 - 100 km/hr driving speed. To the best of our knowledge, this is the first working outdoor VLC system which can offer such range, throughput and error performance while accommodating freeway mobility.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {376–388},
numpages = {13},
keywords = {compressive sensing, vehicular communication, visible light communication},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3466864,
author = {Woodford, Timothy and Zhang, Xinyu and Chai, Eugene and Sundaresan, Karthikeyan and Khojastepour, Amir},
title = {SpaceBeam: LiDAR-driven one-shot mmWave beam management},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3466864},
doi = {10.1145/3458864.3466864},
abstract = {mmWave 5G networks promise to enable a new generation of networked applications requiring a combination of high throughput and ultra-low latency. However, in practice, mmWave performance scales poorly for large numbers of users due to the significant overhead required to manage the highly-directional beams. We find that we can substantially reduce or eliminate this overhead by using out-of-band infrared measurements of the surrounding environment generated by a LiDAR sensor. To accomplish this, we develop a ray-tracing system that is robust to noise and other artifacts from the infrared sensor, create a method to estimate the reflection strength from sensor data, and finally apply this information to the multiuser beam selection process. We demonstrate that this approach reduces beam-selection overhead by over 95\% in indoor multi-user scenarios, reducing network latency by over 80\% and increasing throughput by over 2\texttimes{} in mobile scenarios.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {389–401},
numpages = {13},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3466865,
author = {Zhang, Maolin and Chen, Si and Zhao, Jia and Gong, Wei},
title = {Commodity-level BLE backscatter},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3466865},
doi = {10.1145/3458864.3466865},
abstract = {The communication reliability of state-of-the-art Bluetooth Low Energy (BLE) backscatter systems is fundamentally limited by their modulation schemes because the Binary Frequency Shift Keying (BFSK) modulation of the tag does not exactly match commodity BLE receivers designed for Gauss Frequency Shift Keying (GFSK) modulated signals with high bandwidth efficiency. Gaussian pulse shaping is a missing piece in state-of-the-art BLE backscatter systems. Inspired by active BLE and applying calculus, we present IBLE, a BLE backscatter communication system that achieves full compatibility with commodity BLE devices. IBLE leverages the fact that phase shift is the integral of frequency over time to build a reliable physical layer for BLE backscatter. IBLE uses instantaneous phase shift (IPS) modulation, GFSK modulation, and optional FEC coding to improve the reliability of BLE backscatter communication to the commodity level. We prototype IBLE using various commodity BLE devices and a customized tag with FPGA. Empirical results demonstrate that IBLE achieves PERs of 0.04\% and 0.68\% when the uplink distances are 2 m and 14 m respectively, which are 280x and 70x lower than the PERs of the state-of-the-art system RBLE. On the premise of meeting the BER requirements of the BLE specification, the uplink range of IBLE is 20 m. Since BLE devices are everywhere, IBLE is readily deployable in our everyday IoT applications.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {402–414},
numpages = {13},
keywords = {backscatter, bluetooth, internet of things},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3466863,
author = {Bonati, Leonardo and D'Oro, Salvatore and Basagni, Stefano and Melodia, Tommaso},
title = {SCOPE: an open and softwarized prototyping platform for NextG systems},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3466863},
doi = {10.1145/3458864.3466863},
abstract = {The cellular networking ecosystem is being radically transformed by openness, softwarization, and virtualization principles, which will steer NextG networks toward solutions running on "white box" infrastructures. Telco operators will be able to truly bring intelligence to the network, dynamically deploying and adapting its elements at run time according to current conditions and traffic demands. Deploying intelligent solutions for softwarized NextG networks, however, requires extensive prototyping and testing procedures, currently largely unavailable. To this aim, this paper introduces SCOPE, an open and softwarized prototyping platform for NextG systems. SCOPE is made up of: (i) A ready-to-use, portable open-source container for instantiating softwarized and programmable cellular network elements (e.g., base stations and users); (ii) an emulation module for diverse real-world deployments, channels and traffic conditions for testing new solutions; (iii) a data collection module for artificial intelligence and machine learning-based applications, and (iv) a set of open APIs for users to control network element functionalities in real time. Researchers can use SCOPE to test and validate NextG solutions over a variety of large-scale scenarios before implementing them on commercial infrastructures. We demonstrate the capabilities of SCOPE and its platform independence by prototyping exemplary cellular solutions in the controlled environment of Colosseum, the world's largest wireless network emulator. We then port these solutions to indoor and outdoor testbeds, namely, to Arena and POWDER, a PAWR platform.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {415–426},
numpages = {12},
keywords = {5G, NextG, data collection, data-driven control, network slicing},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3466868,
author = {Lacruz, Jesus O. and Ortiz, Rafael Ruiz and Widmer, Joerg},
title = {A real-time experimentation platform for sub-6 GHz and millimeter-wave MIMO systems},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3466868},
doi = {10.1145/3458864.3466868},
abstract = {The performance of wireless communication systems is evolving rapidly, making it difficult to build experimentation platforms that meet the hardware requirements of new standards. The bandwidth of current systems ranges from 160 MHz for IEEE 802.11ac/ax to 2 GHz for Millimeter-Wave (mm-wave) IEEE 802.11ad/ay, and they support up to 8 spatial MIMO streams. Mobile 5G and beyond systems have a similarly diverse set of requirements.To address this, we propose a highly configurable wireless platform that meets such requirements and is both affordable and scalable. It is implemented on a single state-of-the-art FPGA board that can be configured from 4x4 mm-wave MIMO with 2 GHz channels to 8x8 MIMO with 160 MHz channels in sub-6 GHz bands. In addition, multi-band operation will play an important role in future wireless networks and our platform supports mixed configurations with simultaneous use of mm-wave and sub-6 GHz. Finally, the platform supports real-time operation, e.g., for closed-loop MIMO beam training with low-latency, by implementing suitable hardware/software accelerators. We demonstrate the platform's performance in a wide range of experiments. The platform is provided as open-source to build a community to use and extend it.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {427–439},
numpages = {13},
keywords = {FPGA, MIMO, millimeter wave, multi-band, phased antenna array, testbed},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3467883,
author = {Fomichev, Mikhail and Hesse, Julia and Almon, Lars and Lippert, Timm and Han, Jun and Hollick, Matthias},
title = {FastZIP: faster and more secure zero-interaction pairing},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3467883},
doi = {10.1145/3458864.3467883},
abstract = {With the advent of the Internet of Things (IoT), establishing a secure channel between smart devices becomes crucial. Recent research proposes zero-interaction pairing (ZIP), which enables pairing without user assistance by utilizing devices' physical context (e.g., ambient audio) to obtain a shared secret key. The state-of-the-art ZIP schemes suffer from three limitations: (1) prolonged pairing time (i.e., minutes or hours), (2) vulnerability to brute-force offline attacks on a shared key, and (3) susceptibility to attacks caused by predictable context (e.g., replay attack) because they rely on limited entropy of physical context to protect a shared key. We address these limitations, proposing FastZIP, a novel ZIP scheme that significantly reduces pairing time while preventing offline and predictable context attacks. In particular, we adapt a recently introduced Fuzzy Password-Authenticated Key Exchange (fPAKE) protocol and utilize sensor fusion, maximizing their advantages. We instantiate FastZIP for intra-car device pairing to demonstrate its feasibility and show how the design of FastZIP can be adapted to other ZIP use cases. We implement FastZIP and evaluate it by driving four cars for a total of 800 km. We achieve up to three times shorter pairing time compared to the state-of-the-art ZIP schemes while assuring robust security with adversarial error rates below 0.5\%.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {440–452},
numpages = {13},
keywords = {fPAKE, internet of things, pairing, sensor fusion, zero-interaction},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3467885,
author = {Cao, Yifeng and Dhekne, Ashutosh and Ammar, Mostafa},
title = {ITrackU: tracking a pen-like instrument via UWB-IMU fusion},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3467885},
doi = {10.1145/3458864.3467885},
abstract = {High-precision tracking of a pen-like instrument's movements is desirable in a wide range of fields spanning education, robotics, and art, to name a few. The key challenge in doing so stems from the impracticality of embedding electronics in the tip of such instruments (a pen, marker, scalpel, etc.) as well as the difficulties in instrumenting the surface that it works on. In this paper, we present ITrackU, a movement digitization system that does not require modifications to the surface or the tracked instrument's tip. ITrackU fuses locations obtained using ultra-wideband radios (UWB), with an inertial and magnetic unit (IMU) and a pressure sensor, yielding multidimensional improvements in accuracy, range, cost, and robustness, over existing works. ITrackU embeds a micro-transmitter at the base of a pen which creates a trackable beacon, that is localized from the corners of a writing surface. Fused with inertial motion sensor and a pressure sensor, ITrackU enables accurate tracking. Our prototype of ITrackU covers a large 2.5m \texttimes{} 2m area, while obtaining around 2.9mm median error. We demonstrate the accuracy of our system by drawing numerous shapes and characters on a whiteboard, and compare them against a touchscreen and a camera-based ground-truthing system. Finally, the produced stream of digitized data is minuscule in volume, when compared with a video of the whiteboard, which saves both network bandwidth and storage space.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {453–466},
numpages = {14},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3466626,
author = {Kim, Wonjung and Lee, Seungchul and Chang, Youngjae and Lee, Taegyeong and Hwang, Inseok and Song, Junehwa},
title = {Hivemind: social control-and-use of IoT towards democratization of public spaces},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3466626},
doi = {10.1145/3458864.3466626},
abstract = {Public spaces are equipped with 'public actuators', e.g., HVAC, lighting fixtures, speakers, or streaming TV channels to ensure their visitors' comfort. However, many public actuators rarely allow the visitors to adjust their operation, limiting their utility and fairness across the visitors. Also, the social bar is often too high to speak up one's preference and attempt to change an actuator's operation. Social control and use of IoT devices is an underexplored new direction of research even with its huge potential and implication, but comes with high complexity and scale. This paper proposes a novel architecture, namely, Social Control-and-Use Architecture for IoT Devices, which provides a systematic view and an effective tool to handle the complication and intricacy in system design. It also proposes Hivemind, a first-of-a-kind system developed, upon the architecture, for sharing IoT-enabled actuators in a public space. It transforms an exclusively-controlled actuator in a public space into a true public actuator, supporting visitors to instantly participate in the democratic collective control. Also, a myriad of off-the-shelf actuators are easily incorporated without modification to their implementation. The field deployment of Hivemind shows its comprehensive service coverage as well as the users' approval on the democratic collective control of public actuators.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {467–482},
numpages = {16},
keywords = {IoT, group decision-making, influence-aware authorization, public actuator, public space, social control-and-use, urban computing},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3466907,
author = {Ramprasad, Brian and Chen, Hongkai and Veith, Alexandre and Truong, Khai and de Lara, Eyal},
title = {Pain-o-vision, effortless pain management},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3466907},
doi = {10.1145/3458864.3466907},
abstract = {Chronic pain is often an ongoing challenge for patients to track and collect data. Pain-O-Vision is a smartwatch enabled pain management system that uses computer vision to capture the details of painful events from the user. A natural reaction to pain is to clench ones fist. The embedded camera is used to capture different types of fist clenching, to represent different levels of pain. An initial prototype was built on an Android smartwatch that uses a cloud-based classification service to detect the fist clench gestures. Our results show that it is possible to map a fist clench to different levels of pain which allows the patient to record the intensity of a painful event without carrying a specialized pain management device.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {483–484},
numpages = {2},
keywords = {mobile computing, pain management, smartwatches},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3466908,
author = {Aldeer, Murtadha and Yu, Justin and Chowdhury, Tahiya and Florentine, Joseph and Kolodziejski, Jakub and Howard, Richard E. and Martin, Richard P. and Ortiz, Jorge},
title = {A smart agent guided contactless data collection system amid a pandemic},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3466908},
doi = {10.1145/3458864.3466908},
abstract = {The COVID-19 pandemic has impacted academic life in different ways. In the mobile and pervasive computing community, there was a struggle on data collection for the evaluation of human-sensing systems. An automated and contactless solution to collect data from users at home is one way that can help in the continuation of user-centric studies. In this poster, we present a portable system for remote, in-home data collection. The system is powered by a Raspberry Pi© and input peripherals (a camera, a microphone, and a wireless receiver). Our system uses a speech interface for text-to-speech and speech-to-text conversions. The system acts as a voice-based "smart agent" that guides the user during an experiment session. We aim to use our system to collect data from a set of smart pill bottles that we previously designed for medication adherence monitoring [1] and user identification [3].},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {485–486},
numpages = {2},
keywords = {data collection, portable system, smart agent, smart pillbox},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3466910,
author = {Li, Borui and Fan, Hongchang and Gao, Yi and Dong, Wei},
title = {ThingSpire OS: a WebAssembly-based IoT operating system for cloud-edge integration},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3466910},
doi = {10.1145/3458864.3466910},
abstract = {We advocate ThingSpire OS, a new IoT operating system based on WebAssembly for cloud-edge integration. By design, WebAssembly is considered as the first-class citizen in ThingSpire OS to achieve coherent execution among IoT device, edge and cloud. Furthermore, ThingSpire OS enables efficient execution of WebAssembly on resource-constrained devices by implementing a WebAssembly runtime based on Ahead-of-Time (AoT) compilation with a small footprint, achieves seamless inter-module communication wherever the modules locate, and leverages several optimizations such as lightweight preemptible invocation for memory isolation and control-flow integrity. We implement a prototype of ThingSpire OS and conduct preliminary evaluations on its inter-module communication performance.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {487–488},
numpages = {2},
keywords = {WebAssembly, internet of things, operating system},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3466911,
author = {Mohzary, Muhammad and Almalki, Khalid J and Choi, Baek-Young and Song, Sejun},
title = {Apple in my eyes (AIME): liveness detection for mobile security using corneal specular reflections},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3466911},
doi = {10.1145/3458864.3466911},
abstract = {In this paper, we present a novel software-based face Presentation Attack Detection (PAD) method named "Apple in My Eyes (AIME)" using screen display as a challenge and corneal specular reflections as a response for authenticating the liveness against presentation. To detect face liveness, AIME creates multiple image patterns on the authentication screen as a challenge, then captures meaningful corneal specular reflection responses from user's eyes using the front camera, and analyzes the reflective pattern images using various lightweight Machine Learning (ML) techniques under a subsecond level delay (200 ms). We demonstrate that AIME can detect various attacks, including digital images displayed on the phone or tablet, printed paper images, 2D paper masks, videos, 3D silicon masks, and 3D facial models using VR. AIME liveness detection can be applied for various contactless biometric authentication accurately and efficiently without any costly extra sensors.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {489–490},
numpages = {2},
keywords = {anti-spoofing, liveness detection, presentation attack detection},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3466913,
author = {Pasandi, Hannaneh Barahouei and Nadeem, Tamer},
title = {LATTE: online MU-MIMO grouping for video streaming over commodity wifi},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3466913},
doi = {10.1145/3458864.3466913},
abstract = {In this paper, we present LATTE, a novel framework that proposes MU-MIMO group selection optimization for multi-user video streaming over IEEE 802.11ac. Taking a cross-layer approach, LATTE first optimizes the MU-MIMO user group selection for the users with the same characteristics in the PHY/MAC layer. It then optimizes the video bitrate for each group accordingly. We present our design and its evaluation on smartphones over 802.11ac WiFi.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {491–492},
numpages = {2},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3466914,
author = {Chen, Hsin-Yuan and Hsu, Ruey-Tzer and Chen, Ying-Chiao and Hsu, Wei-Chen and Huang, Polly},
title = {AR game traffic characterization: a case of Pok\'{e}mon Go in a flash crowd event},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3466914},
doi = {10.1145/3458864.3466914},
abstract = {Latency is a major issue towards practical use of augmented reality (AR) in mobile apps such as navigation and gaming. A string of work has appeared recently, proposing to offload a part of the AR-related processing pipeline to the edge [8]. One pitfall in these studies is the (simplified) assumption about the network delay. As a reality check and to gather insights to realize AR in real time, we seek in this work a better understanding of how a popular AR game, Pok\'{e}mon Go, delivers its data in situ.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {493–494},
numpages = {2},
keywords = {AR game, edge offloading, traffic characterization},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3466916,
author = {Holcomb, Amelia and Tong, Bill and Penny, Megan and Keshav, Srinivasan},
title = {Measuring forest carbon with mobile phones},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3466916},
doi = {10.1145/3458864.3466916},
abstract = {Tree trunk diameter, currently measured during manual forest inventories, is a key input to tree carbon storage calculations. We designan app running on a smartphone equipped with a time-of-flight sensor that allows efficient, low-cost, and accurate measurement of trunk diameter, even in the face of natural leaf and branch occlusion. The algorithm runs in near real-time on the phone, allowing user interaction to improve the quality of the results. We evaluate the app in realistic settings and find that in a corpus of 55 sample tree images, it estimates trunk diameter with mean error of 7.8\%.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {495–496},
numpages = {2},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3468444,
author = {Kim, Wonjung and Lee, Seungchul and Chang, Youngjae and Lee, Taegyeong and Hwang, Inseok and Song, Junehwa},
title = {Facilitating in-situ shared use of IoT actuators in public spaces},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3468444},
doi = {10.1145/3458864.3468444},
abstract = {Public spaces, where we gather, commune, and take a rest, are the essential parts of a modern urban landscape, enriching citizen's everyday life [3]. How we share these spaces are considered an indicator of the quality of life. Public spaces thus have a responsibility to provide comfort and satisfaction to any visitors. However, in most times, the operations of the spaces are managed in rather an exclusive manner.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {497–498},
numpages = {2},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3466904,
author = {Liu, Hansi and Alali, Abrar and Ibrahim, Mohamed and Li, Hongyu and Gruteser, Marco and Jain, Shubham and Dana, Kristin and Ashok, Ashwin and Cheng, Bin and Lu, Hongsheng},
title = {Lost and Found! associating target persons in camera surveillance footage with smartphone identifiers},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3466904},
doi = {10.1145/3458864.3466904},
abstract = {We demonstrate an application of finding target persons on a surveillance video. Each visually detected participant is tagged with a smartphone ID and the target person with the query ID is highlighted. This work is motivated by the fact that establishing associations between subjects observed in camera images and messages transmitted from their wireless devices can enable fast and reliable tagging. This is particularly helpful when target pedestrians need to be found on public surveillance footage, without the reliance on facial recognition. The underlying system uses a multi-modal approach that leverages WiFi Fine Timing Measurements (FTM) and inertial sensor (IMU) data to associate each visually detected individual with a corresponding smartphone identifier. These smartphone measurements are combined strategically with RGB-D information from the camera, to learn affinity matrices using a multi-modal deep learning network.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {499–500},
numpages = {2},
keywords = {machine learning, multimodal learning, person identification, wifi FTM ranging},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3466905,
author = {Liu, Ruofeng and Jiang, Wenjun and Chen, Xun},
title = {Acoustic ruler using wireless earbud},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3466905},
doi = {10.1145/3458864.3466905},
abstract = {In the paper, we demonstrate an application of the wireless earbud - an acoustic ruler. Approaches are proposed to improve the robustness of the design in the low signal-to-noise ratio environment. We also share our solution to several engineering challenges, which aims at facilitating the transformation of earbuds to into acoustic sensing research platforms without any hardware modification.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {501–502},
numpages = {2},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3466906,
author = {Garg, Nakul and Bai, Yang and Roy, Nirupam},
title = {Microstructure-guided spatial sensing for low-power IoT},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3466906},
doi = {10.1145/3458864.3466906},
abstract = {This demonstration presents a working prototype of Owlet, an alternative design for spatial sensing of acoustic signals. To overcome the fundamental limitations in form-factor, power consumption, and hardware requirements with array-based techniques, Owlet explores wave's interaction with acoustic structures for sensing. By combining passive acoustic microstructures with microphones, we envision achieving the same functionalities as microphone and speaker arrays with less power consumption and in a smaller form factor. Our design uses a 3D-printed metamaterial structure over a microphone to introduce a carefully designed spatial signature to the recorded signal. Owlet prototype shows 3.6° median error in Direction-of-Arrival (DoA) estimation and 10 cm median error in source localization while using a 1.5cm \texttimes{} 1.3cm acoustic structure for sensing.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {503–504},
numpages = {2},
keywords = {IoT, acoustic metamaterial, low-power sensing, spatial sensing},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3466909,
author = {Tharpe, Bronson and Bourgeois, Anu G. and Ashok, Ashwin},
title = {A do-it-yourself computer vision based robotic ball throw trainer},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3466909},
doi = {10.1145/3458864.3466909},
abstract = {We demonstrate a self-training system for sports involving throwing a ball. We design a do-it-yourself (DIY) machinery that can be assembled using off-the-shelf items and integrates computer vision to visually track the ball throw accuracy. In this work, we demonstrate a system that can identify if the ball went through the hoop and approximately in which of the hoop's inner region. We envision that this preliminary design sets the foundation for a complete DIY sports IoT system that involves a hoola hoop, RaspberryPi, PiCamera and a LED strip, along with advanced ball placement and dynamics tracking.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {505–506},
numpages = {2},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3466912,
author = {Johnson, David and Maas, Dustin and Van Der Merwe, Jacobus},
title = {Open source RAN slicing on POWDER: a top-to-bottom O-RAN use case},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3466912},
doi = {10.1145/3458864.3466912},
abstract = {This demonstration will showcase our efforts to develop a radio access network (RAN) slicing mechanism that is controllable via management software in an Open RAN framework. To our knowledge, our work represents the first effort that combines an open source Open RAN framework with an open source mobility stack, provides a top-to-bottom RAN application via the RAN intelligent control (RIC) provided by that framework and illustrates its functionality in a realistic wireless environment. Our software is publicly available and we provide a profile in the POWDER platform to enable others to replicate and build on our work.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {507–508},
numpages = {2},
keywords = {RAN slicing, open RAN, programmability},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}
@inproceedings{10.1145/3458864.3466915,
author = {Breen, Joe and Duerig, Jonathon and Eide, Eric and Hibler, Mike and Johnson, David and Kasera, Sneha and Maas, Dustin and Orange, Alex and Patwari, Neal and Ricci, Robert and Schurig, David and Stoller, Leigh and Van der Merwe, Jacobus and Webb, Kirk and Wong, Gary},
title = {Mobile and wireless research on the POWDER platform},
year = {2021},
isbn = {9781450384438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458864.3466915},
doi = {10.1145/3458864.3466915},
abstract = {POWDER is a highly flexible, deeply programmable, and city-scale scientific instrument that enables cutting-edge research in wireless technologies. Researchers interact with the POWDER platform via the Internet to conduct their experiments, with zero penalty for remote access. In this two-part demonstration, the POWDER implementers show how to use the platform. First, they present the workflow that researchers follow to conduct experiments. Second, they highlight some of the hardware and software building blocks available through POWDER, including components related to over-the-air wireless and mobile networking, 5G, and massive MIMO.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {509–510},
numpages = {2},
location = {Virtual Event, Wisconsin},
series = {MobiSys '21}
}