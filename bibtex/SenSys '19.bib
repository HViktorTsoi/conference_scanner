@inproceedings{10.1145/3356250.3360036,
author = {Huynh, Sinh and Balan, Rajesh Krishna and Ko, JeongGil and Lee, Youngki},
title = {VitaMon: measuring heart rate variability using smartphone front camera},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3360036},
doi = {10.1145/3356250.3360036},
abstract = {We present VitaMon, a mobile sensing system that can measure the inter-heartbeat interval (IBI) from the facial video captured by a commodity smartphone's front camera. The continuous IBI measurement is used to compute heart rate variability (HRV), one of the most important markers of the autonomic nervous system (ANS) regulation. The underlying idea of VitaMon is that video recording of human face contains multiple cardiovascular pulse signals with different phase shift. Our measurement on 10 participants shows the significant time delay (36.79 ms) between the pulse signals measured at the jaw region and forehead region. VitaMon leverages deep neural network models to extract both spatial and temporal information of the video to reconstruct a pulse waveform signal that is optimized for estimating IBI. We evaluated VitaMon with a dataset collected from 30 participants under various conditions involving different light intensity levels and motion artifacts. With the 15 fps video input (66.67 ms time resolution), VitaMon can measure IBI with an average error of 14.26 ms and 21.65 ms using personal and general model respectively. HRV features including geometry Poincare plot, time- and frequency-domain features extracted from the IBI measurement all have high correlation with the reference signal.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {1–14},
numpages = {14},
keywords = {heart rate variability, mobile sensing, photoplethysmography (PPG), remote PPG},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3360032,
author = {Jeyakumar, Jeya Vikranth and Lai, Liangzhen and Suda, Naveen and Srivastava, Mani},
title = {SenseHAR: a robust virtual activity sensor for smartphones and wearables},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3360032},
doi = {10.1145/3356250.3360032},
abstract = {Modern smartphones and smartwatches are equipped with inertial sensors (accelerometer, gyroscope, and magnetometer) that can be used for Human Activity Recognition (HAR) to infer tasks such as daily activities, transportation modes and, gestures. HAR requires collecting raw inertial sensor values and training a machine learning model on the collected data. The challenge in this approach is that the models are trained for specific devices and device configurations whereas, in reality, the set of devices carried by a person may vary over time. Ideally, one would like activity inferencing to be robust of this variation and provide accurate predictions by making opportunistic use of information from available devices. Moreover, the devices may be located at different parts of the body (e.g. pocket, left and right wrist), may have different sets of sensors (e.g. a smartwatch may not have gyroscope while a smartphone might), and may differ in sampling frequencies. In this paper, we provide a solution which makes use of the information from available devices while being robust to their variations. Instead of training an end-to-end model for every permutation of device combinations and configurations, we propose a scalable deep learning based solution in which each device learns its own sensor fusion model that maps the raw sensor values to a shared low dimensional latent space which we call the 'SenseHAR'-a virtual activity sensor. The virtual sensor has the same format and similar behavior regardless of the subset of devices, sensor's availability, sampling rate, or a device's location. This would help machine learning engineers to develop their application specific (e.g., from gesture recognition to activities of daily life) models in a hardware-agnostic manner based on this virtual activity sensor. Our evaluations show that an application model trained on SenseHAR achieves the state of the art accuracies of 95.32\%, 74.22\% and 93.13\% on PAMAP2, Opportunity(gestures) and our collected datasets respectively.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {15–28},
numpages = {14},
keywords = {deep learning, human activity recognition, sensor fusion},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3360037,
author = {Nambi, Akshay Uttama and Mehta, Ishit and Ghosh, Anurag and Lingam, Vijay and Padmanabhan, Venkata N.},
title = {ALT: towards automating driver license testing using smartphones},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3360037},
doi = {10.1145/3356250.3360037},
abstract = {Can a smartphone administer a driver license test? We ask this question because of the inadequacy of manual testing and the expense of outfitting an automated testing track with sensors such as cameras, leading to less-than-thorough testing and ultimately compromising road safety. We present ALT, a low-cost smartphone-based system for automating key aspects of the driver license test. A windshield-mounted smartphone serves as the sole sensing platform, with the front camera being used to monitor driver's gaze, and the rear camera, together with inertial sensors, being used to evaluate driving maneuvers such as parallel parking. The sensors are also used in tandem, for instance, to check that the driver scanned their mirror during a lane change.The key challenges in ALT arise from the variation in the subject (driver) and the environment (vehicle geometry, camera orientation, etc.), little or no infrastructure support to keep costs low, and also the limitations of the smartphone (low-end GPU). The main contributions of this paper are: (a) robust detection of driver's gaze by combining head pose and eye gaze information, and performing auto-calibration to accommodate environmental variation, (b) a hybrid visual SLAM technique that combines visual features and a sparse set of planar markers, placed optimally in the environment, to derive accurate trajectory information, and (c) an efficient realization on smartphones using both CPU and GPU resources. We perform extensive experiments, both in controlled settings and on an actual driving test track, to validate the efficacy of ALT.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {29–42},
numpages = {14},
keywords = {SLAM, automated license testing, gaze tracking, road safety},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3360029,
author = {Liu, Zifan and Zhu, Hongzi and Chen, Junchi and Chang, Shan and Qiu, Lili},
title = {HyperSight: boosting distant 3D vision on a single dual-camera smartphone},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3360029},
doi = {10.1145/3356250.3360029},
abstract = {Smartphones with dual cameras are increasingly popular due to the need of supporting 3D vision. The depth information is critical for 3D vision. However, the two cameras on a smartphone are too close to accurately estimate the depth information especially for objects beyond two meters. In this paper, we propose an innovative system, called HyperSight, to estimate the depth information of objects using a dual camera smartphone. HyperSight realizes a virtual longbaseline stereo vision rig by having a user to move the phone in the air. The phone movement is continuously tracked and estimated using the short-baseline dual camera seeing nearby objects. We implement HyperSight as software on a Commercial-Off-The-Shelf (COTS) smartphone and conduct real-world experiments. The results show that when measuring feature-rich objects at a distance of five meters, HyperSight achieves a mean depth error of 6cm, which is up to 10\texttimes{} and 18\texttimes{} improvement in the accuracy compared with the stereo vision system using the native dual cameras and the Measure app based on ARKit 1 on mobile devices, respectively.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {43–54},
numpages = {12},
keywords = {depth estimation, dual cameras, near-far diversity, smartphone},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3360033,
author = {Branco, Adriano and Mottola, Luca and Alizai, Muhammad Hamad and Siddiqui, Junaid Haroon},
title = {Intermittent asynchronous peripheral operations},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3360033},
doi = {10.1145/3356250.3360033},
abstract = {Energy harvesting enables battery-less sensing applications, but causes executions to become intermittent as a result of erratic energy provisioning. Intermittent executions pose challenges to peripheral consistency that threaten to leave peripheral-bound workloads in failed states or to impede forward progress of programs. Intermittent synchronous peripheral operations are supported in existing literature for specific kinds of peripherals. Asynchronous peripheral operations enable reactive concurrency in application implementations, which increases reactivity and improves energy consumption, but lack dedicated support in intermittent settings. We present Karma, the first general abstraction and system design to support both synchronous and asynchronous operations in an intermittent setting. Karma employs a novel combination of peripheral roll-forward and computation roll-back to a rendezvous point guaranteeing consistency. It remains transparent to application programmers and peripheral driver, which favours portability. Our evaluation, based on three applications running on prototype hardware and using diverse energy sources, indicates that intermittent asynchronous peripheral support provided by Karma boosts data throughput by 83\% compared to existing literature.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {55–67},
numpages = {13},
keywords = {asynchronous operations, energy harvesting, intermittent computing, peripherals},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3360034,
author = {Shukla, Rishi and Kiran, Neev and Wang, Rui and Gummeson, Jeremy and Lee, Sunghoon Ivan},
title = {SkinnyPower: enabling batteryless wearable sensors via intra-body power transfer},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3360034},
doi = {10.1145/3356250.3360034},
abstract = {In this work, we present SkinnyPower, a technology for Intra-Body Power Transfer (IBPT) that wirelessly transfers power through human skin to operate batteryless wearable sensors. We envision a scenario, in which batteryless sensors placed on small body parts (e.g., on-finger, in-ear, and in-mouth) can obtain operating power from another body-worn energy sources (e.g., already existing battery-powered wearable devices such as a smartwatch or a BandAid-like battery patch attached to the neck). The key technical challenges in realizing this vision include 1) providing a robust return path in the body channel - where the forward (power signal) and return (ground) paths are not explicitly defined - using implicit capacitances formed between the devices and earth ground, and 2) achieving reliable operation despite variations in capacitive coupling between the skin and devices, devices and earth ground, and conductance of the subdermal layer. We identify and optimize critical system design parameters to maximize the power transfer between a transmitter and a receiver with the capacitively coupled return path. To demonstrate and validate the concept of IBPT, we implemented a prototype consisting of 1) a wrist-worn, battery-equipped power transmitter that sends alternating current through the human body and 2) a finger-worn, batteryless sensor device that operates solely on body-transferred power. Evaluations on five subjects show that we can reliably support the power of approximately 1 mW, which can be used to operate an embedded system, continuously collect sensor (e.g. accelerometer) data, and wirelessly transfer the collected data in real-time using Bluetooth Low Energy. Moreover, we achieve a power transfer rate of 14.5\% between the transmitter and receiver, which is significantly higher than other wireless power transfer techniques such as RFID. We believe that the proposed system has great potential to transform current architectures and designs for body-area networks, promoting the development of innovative on-body sensors that would otherwise not be possible with on-device batteries.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {68–82},
numpages = {15},
keywords = {intra-body power transfer, wearable computers, wireless power harvesting},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3360042,
author = {Geissdoerfer, Kai and Chwalisz, Miko\l{}aj and Zimmerling, Marco},
title = {Shepherd: a portable testbed for the batteryless IoT},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3360042},
doi = {10.1145/3356250.3360042},
abstract = {Collaboration of batteryless nodes is essential to their success in replacing traditional battery-based systems. Energy-harvesting sensor nodes experience spatio-temporal fluctuations of energy availability. These fluctuations become especially critical when sensor nodes do not have sufficient energy storage to compensate for them. Understanding the challenges and opportunities of operating groups of batteryless sensor nodes requires to record and reproduce spatio-temporal characteristics of real energy environments. We thus present Shepherd, a testbed for the batteryless IoT. Shepherd allows to record synchronized energy traces with a resolution of 3 μA and 50μV at a rate of 100 kHz, and to faithfully replay these traces to any number of sensor nodes to study their behavior. We release Shepherd as an open-source tool for the community, facilitating research into time synchronization, wireless networking, and other distributed algorithms for batteryless systems.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {83–95},
numpages = {13},
keywords = {batteryless, emulation, energy harvesting, intermittent networking, intermittent power, recording, testbed, transient computing},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3360044,
author = {Apicharttrisorn, Kittipat and Ran, Xukan and Chen, Jiasi and Krishnamurthy, Srikanth V. and Roy-Chowdhury, Amit K.},
title = {Frugal following: power thrifty object detection and tracking for mobile augmented reality},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3360044},
doi = {10.1145/3356250.3360044},
abstract = {Accurate tracking of objects in the real world is highly desirable in Augmented Reality (AR) to aid proper placement of virtual objects in a user's view. Deep neural networks (DNNs) yield high precision in detecting and tracking objects, but they are energy-heavy and can thus be prohibitive for deployment on mobile devices. Towards reducing energy drain while maintaining good object tracking precision, we develop a novel software framework called MARLIN. MARLIN only uses a DNN as needed, to detect new objects or recapture objects that significantly change in appearance. It employs lightweight methods in between DNN executions to track the detected objects with high fidelity. We experiment with several baseline DNN models optimized for mobile devices, and via both offline and live object tracking experiments on two different Android phones (one utilizing a mobile GPU), we show that MARLIN compares favorably in terms of accuracy while saving energy significantly. Specifically, we show that MARLIN reduces the energy consumption by up to 73.3\% (compared to an approach that executes the best baseline DNN continuously), and improves accuracy by up to 19\texttimes{} (compared to an approach that infrequently executes the same best baseline DNN). Moreover, while in 75\% or more cases, MARLIN incurs at most a 7.36\% reduction in location accuracy (using the common IOU metric), in more than 46\% of the cases, MARLIN even improves the IOU compared to the continuous, best DNN approach.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {96–109},
numpages = {14},
keywords = {convolutional neural network, energy efficiency, mobile augmented reality, object detection and tracking},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3360020,
author = {Gong, Taesik and Kim, Yeonsu and Shin, Jinwoo and Lee, Sung-Ju},
title = {MetaSense: few-shot adaptation to untrained conditions in deep mobile sensing},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3360020},
doi = {10.1145/3356250.3360020},
abstract = {Recent improvements in deep learning and hardware support offer a new breakthrough in mobile sensing; we could enjoy context-aware services and mobile healthcare on a mobile device powered by artificial intelligence. However, most related studies perform well only with a certain level of similarity between trained and target data distribution, while in practice, a specific user's behaviors and device make sensor inputs different. Consequently, the performance of such applications might suffer in diverse user and device conditions as training deep models in such countless conditions is infeasible. To mitigate the issue, we propose MetaSense, an adaptive deep mobile sensing system utilizing only a few (e.g., one or two) data instances from the target user. MetaSense employs meta learning that learns how to adapt to the target user's condition, by rehearsing multiple similar tasks generated from our unique task generation strategies in offline training. The trained model has the ability to rapidly adapt to the target user's condition when a few data are available. Our evaluation with real-world traces of motion and audio sensors shows that MetaSense not only outperforms the state-of-the-art transfer learning by 18\% and meta learning based approaches by 15\% in terms of accuracy, but also requires significantly less adaptation time for the target user.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {110–123},
numpages = {14},
keywords = {deep learning, few-shot learning, human activity recognition, meta learning, mobile sensing},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3360025,
author = {Song, Qun and Yan, Zhenyu and Tan, Rui},
title = {Moving target defense for embedded deep visual sensing against adversarial examples},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3360025},
doi = {10.1145/3356250.3360025},
abstract = {Deep learning-based visual sensing has achieved attractive accuracy but is shown vulnerable to adversarial example attacks. Specifically, once the attackers obtain the deep model, they can construct adversarial examples to mislead the model to yield wrong classification results. Deployable adversarial examples such as small stickers pasted on the road signs and lanes have been shown effective in misleading advanced driver-assistance systems. Many existing countermeasures against adversarial examples build their security on the attackers' ignorance of the defense mechanisms. Thus, they fall short of following Kerckhoffs's principle and can be subverted once the attackers know the details of the defense. This paper applies the strategy of moving target defense (MTD) to generate multiple new deep models after system deployment, that will collaboratively detect and thwart adversarial examples. Our MTD design is based on the adversarial examples' minor transferability across different models. The post-deployment dynamically generated models significantly increase the bar of successful attacks. We also apply serial data fusion with early stopping to reduce the inference time by a factor of up to 5. Evaluation based on four datasets including a road sign dataset and two GPU-equipped Jetson embedded computing platforms shows the effectiveness of our approach.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {124–137},
numpages = {14},
keywords = {adversarial examples, moving target defense, neural networks},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3360030,
author = {Lee, Seulki and Nirjon, Shahriar},
title = {Neuro.ZERO: a zero-energy neural network accelerator for embedded sensing and inference systems},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3360030},
doi = {10.1145/3356250.3360030},
abstract = {We introduce Neuro.ZERO---a co-processor architecture consisting of a main microcontroller (MCU) that executes scaled-down versions of a deep neural network1 (DNN) inference task, and an accelerator microcontroller that is powered by harvested energy and follows the intermittent computing paradigm [76]. The goal of the accelerator is to enhance the inference performance of the DNN that is running on the main microcontroller. Neuro.ZERO opportunistically accelerates the run-time performance of a DNN via one of its four acceleration modes: extended inference, expedited inference, ensemble inference, and latent training. To enable these modes, we propose two sets of algorithms: (1) energy and intermittence-aware DNN inference and training algorithms, and (2) a fast and high-precision adaptive fixed-point arithmetic that beats existing floating-point and fixed-point arithmetic in terms of speed and precision, respectively, and achieves the best of both. To evaluate Neuro.ZERO, we implement low-power image and audio recognition applications and demonstrate that their inference speedup increases by 1.6\texttimes{} and 1.7\texttimes{}, respectively, and the inference accuracy increases by 10\% and 16\%, respectively, when compared to battery-powered single-MCU systems.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {138–152},
numpages = {15},
keywords = {accelerator, batteryless, deep neural networks, zero energy},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3360038,
author = {Shen, Zhihao and Yang, Kang and Du, Wan and Zhao, Xi and Zou, Jianhua},
title = {DeepAPP: a deep reinforcement learning framework for mobile application usage prediction},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3360038},
doi = {10.1145/3356250.3360038},
abstract = {This paper aims to predict the apps a user will open on her mobile device next. Such an information is essential for many smartphone operations, e.g., app pre-loading and content pre-caching, to save mobile energy. However, it is hard to build an explicit model that accurately depicts the affecting factors and their affecting mechanism of time-varying app usage behavior. This paper presents a deep reinforcement learning framework, named as DeepAPP, which learns a model-free predictive neural network from historical app usage data. Meanwhile, an online updating strategy is designed to adapt the predictive network to the time-varying app usage behavior. To transform DeepAPP into a practical deep reinforcement learning system, several challenges are addressed by developing a context representation method for complex contextual environment, a general agent for overcoming data sparsity and a lightweight personalized agent for minimizing the prediction time. Extensive experiments on a large-scale anonymized app usage dataset reveal that DeepAPP provides high accuracy (precision 70.6\% and recall of 62.4\%) and reduces the prediction time of the state-of-the-art by 6.58\texttimes{}. A field experiment of 29 participants also demonstrates DeepAPP can effectively reduce time of loading apps.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {153–165},
numpages = {13},
keywords = {app usage prediction, deep reinforcement learning, mobile devices, neural networks},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3360021,
author = {Bloom, Rens and Zamalloa, Marco Z\'{u}\~{n}iga and Pai, Chaitra},
title = {LuxLink: creating a wireless link from ambient light},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3360021},
doi = {10.1145/3356250.3360021},
abstract = {Transmitting information with visible light requires controlling the intensity of the light source. Many light sources in our environments, however, cannot be controlled (not only the sun but also plenty of light bulbs). These uncontrollable light sources provide an immense amount of ambient light that could be used for wireless communication, yet few studies are exploiting this opportunity. We provide a detailed analysis of a Hardware- and Physical-Layer to create safe and reliable wireless links relying solely on ambient light and simple photosensors. Motivated by recent studies, our platform builds upon liquid crystal displays (LCDs) to backscatter ambient light, but it provides a unique and novel feature: our platform utilizes frequency signals to modulate ambient light. Compared to the state-of-the-art, which rely on either pulse- or color-based modulation, our approach allows us to provide simultaneously: a simple and energy-efficient platform (no cameras), flicker-free communication (safe), and the ability to work reliably in spite of the interference and light fluctuations caused by uncontrollable light sources (reliable). We test our platform in indoor and outdoor environments, and show that an LCD surface of 6\texttimes{}8 cm can transmit 80 bps at ranges between 4 meters (indoors) and 60 meters (outdoors), consuming a fraction of the energy required by comparable systems using cameras.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {166–178},
numpages = {13},
keywords = {ambient light, backscattering, passive communication},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3360022,
author = {Liu, Ruofeng and Yin, Zhimeng and Jiang, Wenchao and He, Tian},
title = {LTE2B: time-domain cross-technology emulation under LTE constraints},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3360022},
doi = {10.1145/3356250.3360022},
abstract = {Conventional gateway solutions are limited in satisfying the demand for ubiquitous connections among heterogeneous wireless devices, e.g., wide-area and personal-area network devices, due to the deployment complexity, high cost, and the incurred extra traffic. Recent advances propose the physical layer cross-technology communication to address these issues. However, existing CTC techniques commonly emulate the target waveform in the frequency domain (FDE). Despite their success, these FDE based techniques inherently suffer from high quantization errors and are insufficient for IoT applications that require high communication reliability.To improve the emulation accuracy, we are the first to introduce the time-domain emulation (TDE) that significantly outperforms FDE techniques in reducing quantization errors and offers reliable emulation even with limited sources, e.g., low modulation schemes. To validate our idea, we propose LTE2B, the first TDE based CTC work that enables LTE devices (e.g., smartphones) to transmit data frames demodulatable by ZigBee and Bluetooth low energy (BLE) devices. We implement the LTE2B on commodity devices (Nexus 5X smartphone and CC2530/CC1350 ZigBee/BLE SoC) with only payload embedding by penetrating the extremely complicated LTE stack. Our extensive evaluation demonstrates that TDE outperforms FDE, while LTE2B can achieve a robust (> 99\% accuracy), long distance (> 400m) CTC performance under a full range of wireless configurations including indoor/outdoor, mobility, and duty-cycle settings.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {179–191},
numpages = {13},
keywords = {cross-technology communication, time-domain emulation},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3360024,
author = {Xia, Xianjin and Zheng, Yuanqing and Gu, Tao},
title = {FTrack: parallel decoding for LoRa transmissions},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3360024},
doi = {10.1145/3356250.3360024},
abstract = {LoRa has emerged as a promising Low-Power Wide Area Network (LP-WAN) technology to connect a huge number of Internet-of-Things (IoT) devices. The dense deployment and an increasing number of IoT devices lead to intense collisions due to uncoordinated transmissions. However, the current MAC/PHY design of LoRaWAN fails to recover collisions, resulting in degraded performance as the system scales. This paper presents FTrack, a novel communication paradigm that enables demodulation of collided LoRa transmissions. FTrack resolves LoRa collisions at the physical layer and thereby supports parallel decoding for LoRa transmissions. We propose a novel technique to separate collided transmissions by jointly considering both the time domain and the frequency domain features. The proposed technique is motivated from two key observations: (1) the symbol edges of the same frame exhibit periodic patterns, while the symbol edges of different frames are usually misaligned in time; (2) the frequency of LoRa signal increases continuously in between the edges of symbol, yet exhibits sudden changes at the symbol edges. We detect the continuity of signal frequency to remove interference and further exploit the time-domain information of symbol edges to recover symbols of all collided frames. We implement FTrack on a low-cost software defined radio. Our testbed evaluations show that FTrack demodulates collided LoRa frames with low symbol error rates in diverse SNR conditions. It increases the throughput of LoRaWAN in real usage scenarios by up to 3 times.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {192–204},
numpages = {13},
keywords = {LoRaWAN, collision, internet of things, parallel decoding},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3360046,
author = {Chi, Zicheng and Li, Yan and Liu, Xin and Yao, Yao and Zhang, Yanchao and Zhu, Ting},
title = {Parallel inclusive communication for connecting heterogeneous IoT devices at the edge},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3360046},
doi = {10.1145/3356250.3360046},
abstract = {WiFi and Bluetooth Low Energy (BLE) are widely used in Internet of Things (IoT) devices. Since WiFi and BLE work within the overlapped ISM 2.4 GHz band, they will interfere with each other. Existing approaches have demonstrated their effectiveness in mitigating the interference. However, further performance improvement has been hampered by the design goal of exclusive communication of WiFi or BLE, which only allows one WiFi or BLE device to transmit packets at any specific time slot on the overlapped channel within the communication range. In this paper, we explore a new communication method, called Parallel Inclusive Communication (PIC), which leverages the unique modulation schemes of WiFi and BLE for parallel inclusive bi-directional transmission of both WiFi and BLE data at the same time within the overlapped channel. In this communication system, the PIC gateway is designed upon the IEEE 802.11g and 802.15.1 frameworks while the WiFi and BLE clients are commercial off-the-shelf devices. PIC achieves similar data rates for these parallel WiFi and BLE communications as if WiFi and BLE are communicating separately. PIC's system architecture naturally fits at the edge of the Internet, which is an optimal site for concurrently collecting (or disseminating) data from (or to) an exponentially increasing number of IoT devices that are using WiFi or BLE. We conducted extensive evaluations under four real-world scenarios. Results show that compared with existing approaches, PIC can significantly i) increase the packet reception ratios by 183\%; ii) reduce the round-trip delay time by 590 times and energy consumption by 50.5 times; and iii) improve the throughput under WiFi and BLE coexistence scenarios.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {205–218},
numpages = {14},
keywords = {IoT, heterogenous networks, parallel communication},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3360023,
author = {Winkler, Daniel A. and Cerpa, Alberto E.},
title = {WISDOM: watering intelligently at scale with distributed optimization and modeling},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3360023},
doi = {10.1145/3356250.3360023},
abstract = {As lawn irrigation is estimated to consume 7 billion gallons of scarce fresh water each day in North America alone, lawn irrigation systems are a high priority for improvements in efficiency. To this end, recent work has introduced several key advancements in irrigation control. Distributed actuation systems allow the irrigation system to apply water completely independently across the field allowing flexibility of control, and the use of fluid flow modeling and optimization allows more efficient schedules to be computed automatically, significantly improving the irrigation quality of service as well. However, the proposed systems are designed with centralized architectures that introduce single points of failure, computational bottlenecks in data processing, and significant network energy for data forwarding used by the centralized data-driven modeling strategies. In response to these challenges, we propose and demonstrate WISDOM, whose novel and flexible hardware and processing pipelines enable the use of a distributed system for the management of irrigation systems of any scale, with energy independence by way of energy harvesting. Across 4 weeks of live system deployment, we find that the WISDOM system can save up to 32.9\% of water in comparison to industry-best, while maintaining a perfect quality of service to the plant. Furthermore, with substantial analysis in simulation we find that in addition to practical system improvements, the use of the proposed distributed system within typical operating conditions will provide all of the efficiency and quality-of-service benefits of the globally-modeled, centrally controlled systems, while allowing the robust control of irrigation systems of any size.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {219–231},
numpages = {13},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3360041,
author = {Liu, Xiaochen and Ghosh, Pradipta and Ulutan, Oytun and Manjunath, B. S. and Chan, Kevin and Govindan, Ramesh},
title = {Caesar: cross-camera complex activity recognition},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3360041},
doi = {10.1145/3356250.3360041},
abstract = {Detecting activities from video taken with a single camera is an active research area for ML-based machine vision. In this paper, we examine the next research frontier: near real-time detection of complex activities spanning multiple (possibly wireless) cameras, a capability applicable to surveillance tasks. We argue that a system for such complex activity detection must employ a hybrid design: one in which rule-based activity detection must complement neural network based detection. Moreover, to be practical, such a system must scale well to multiple cameras and have low end-to-end latency. Caesar, our edge computing based system for complex activity detection, provides an extensible vocabulary of activities to allow users to specify complex actions in terms of spatial and temporal relationships between actors, objects, and activities. Caesar converts these specifications to graphs, efficiently monitors camera feeds, partitions processing between cameras and the edge cluster, retrieves minimal information from cameras, carefully schedules neural network invocation, and efficiently matches specification graphs to the underlying data in order to detect complex activities. Our evaluations show that Caesar can reduce wireless bandwidth, on-board camera memory, and detection latency by an order of magnitude while achieving good precision and recall for all complex activities on a public multi-camera dataset.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {232–244},
numpages = {13},
keywords = {action detection, camera networks, computer vision, edge computing, mobile sensing},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3360028,
author = {Sharma, Amit and Misra, Archan and Subramaniam, Vengateswaran and Lee, Youngki},
title = {SmrtFridge: IoT-based, user interaction-driven food item \& quantity sensing},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3360028},
doi = {10.1145/3356250.3360028},
abstract = {We present SmrtFridge, a consumer-grade smart fridge prototype that demonstrates two key capabilities: (a) identify the individual food items that users place in or remove from a fridge, and (b) estimate the residual quantity of food items inside a refrigerated container (opaque or transparent). Notably, both of these inferences are performed unobtrusively, without requiring any explicit user action or tagging of food objects. To achieve these capabilities, SmrtFridge uses a novel interaction-driven, multi-modal sensing pipeline, where Infrared (IR) and RGB video sensing, triggered whenever a user interacts naturally with the fridge, is used to extract a foreground visual image of the food item, which is then processed by a state-of-the-art DNN classifier. Concurrently, the residual food quantity is estimated by exploiting slight thermal differences, between the empty and filled portions of the container. Experimental studies, involving 12 users interacting naturally with 19 common food items and a commodity fridge, show that SmrtFridge is able to (a) extract at least 75\% of a food item's image in over 97\% of interaction episodes, and consequently identify the individual food items with precision/recall values of ~ 85\%, and (b) perform robust coarse-grained (3 level) classification of the residual food quantity with an accuracy of ~ 75\%.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {245–257},
numpages = {13},
keywords = {IR sensor, food recognition, internet of things (IoT), smart fridge},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3360031,
author = {Chen, Lili and Xiong, Jie and Chen, Xiaojiang and Lee, Sunghoon Ivan and Chen, Kai and Han, Dianhe and Fang, Dingyi and Tang, Zhanyong and Wang, Zheng},
title = {WideSee: towards wide-area contactless wireless sensing},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3360031},
doi = {10.1145/3356250.3360031},
abstract = {Contactless wireless sensing without attaching a device to the target has achieved promising progress in recent years. However, one severe limitation is the small sensing range. This paper presents WideSee to realize wide-area sensing with only one transceiver pair. WideSee utilizes the LoRa signal to achieve a larger range of sensing and further incorporates drone's mobility to broaden the sensing area. WideSee presents solutions across software and hardware to overcome two aspects of challenges for wide-range contactless sensing: (i) the interference brought by the device mobility and LoRa's high sensitivity; and (ii) the ambiguous target information such as location when employing just a single pair of transceivers. We have developed a working prototype of WideSee for human target detection and localization that are especially useful in emergency scenarios such as rescue search, and evaluated WideSee with both controlled experiments and the field study in a high-rise building. Extensive experiments demonstrate the great potential of WideSee for wide-area contactless sensing with a single LoRa transceiver pair hosted on a drone.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {258–270},
numpages = {13},
keywords = {LoRa, mobility, wide-area, wireless sensing},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3360043,
author = {Min, Chulhong and Montanari, Alessandro and Mathur, Akhil and Kawsar, Fahim},
title = {A closer look at quality-aware runtime assessment of sensing models in multi-device environments},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3360043},
doi = {10.1145/3356250.3360043},
abstract = {The increasing availability of multiple sensory devices on or near a human body has opened brand new opportunities to leverage redundant sensory signals for powerful sensing applications. For instance, personal-scale sensory inferences with motion and audio signals can be done individually on a smartphone, a smartwatch, and even an earbud - each offering unique sensor quality, model accuracy, and runtime behaviour. At execution time, however, it is incredibly challenging to assess these characteristics to select the best device for accurate and resource-efficient inferences. To this end, we look at a quality-aware collaborative sensing system that actively interplays across multiple devices and respective sensing models. It dynamically selects the best device as a function of model accuracy at any given context. We propose two complementary techniques for the runtime quality assessment. Borrowing principles from active learning, our first technique runs on three heuristic-based quality assessment functions that employ confidence, margin sampling, and entropy of models' output. Our second technique is built with a siamese neural network and acts on the premise that runtime sensing quality can be learned from historical data. Our evaluation across multiple motion and audio datasets shows that our techniques provide 12\% increase in overall accuracy through dynamic device selection at the average expense of 13 mW power on each device as compared to traditional single-device approaches.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {271–284},
numpages = {14},
keywords = {multi-device environments, quality assessment, sensing models},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3360040,
author = {Nguyen, Viet and Rupavatharam, Siddharth and Liu, Luyang and Howard, Richard and Gruteser, Marco},
title = {HandSense: capacitive coupling-based dynamic, micro finger gesture recognition},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3360040},
doi = {10.1145/3356250.3360040},
abstract = {Head-mounted devices (HMD) for Augmented Reality (AR) are gaining traction thanks to a growing number of applications in the areas of image guided therapy, computer aided design, cargo packing, manufacturing and digital field service. However, providing an always available, intuitive and user friendly input for these devices remains a challenging problem. This paper explores recognizing dynamic, micro finger gestures using capacitive coupling for interacting with a head-mounted device. Electrodes are attached to fingertips of users gloves and capacitive coupling among all pairs of electrodes is measured quickly to infer the real-time spatial relationship between fingers. The system is able to recognize fine, low-effort finger gestures, such as swiping, sliding, tap, double-tap. We evaluated our prototype with 14 gestures executed by 10 subjects and found a 97\% accuracy of gesture recognition.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {285–297},
numpages = {13},
keywords = {capacitive sensing, gesture recognition, human computer interaction (HCI)},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3360045,
author = {Yu, Yinggang and Wang, Dong and Zhao, Run and Zhang, Qian},
title = {RFID based real-time recognition of ongoing gesture with adversarial learning},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3360045},
doi = {10.1145/3356250.3360045},
abstract = {At present, wireless sensing based gesture recognition is becoming a rising star due to its convenience and non-invasiveness without privacy issues, while the strict requirement of the deployment and surrounding environment is still an unavoidable issue which limits its development and generalization. Although there are some works involving the environmental variance, the changes of relative positions between devices and users are ignored. As one of the most popular wireless sensing methods, RFID is widely used in activity recognition with its stable low-level physical characters such as phase and RSS. Besides, the signals reflected from RFID tags intuitively delineate its movements. On the other hand, many interactive gesture-driven applications, such as gesture input for video games, have a paramount and unavoidable issue about the latency between completion of a gesture and its recognition. Inspired by deep learning, this paper presents a real-time ongoing gesture recognition system EUIGR, which efficiently integrates phase and RSS data streams, and extracts both environment and user invariant features. The proposed system seamlessly integrates CNNs (Convolutional Neural Networks) and LSTM (Long Short-Term Memory) to fuse RFID low-level physical characters and extract space-temporal information. Furthermore, with adversarial learning, EUIGR suppresses environment-related factors and the user-specific features, and obtains strong robustness to individual diversity and decreases the environmental dependence. We also implement the system with COTS RFID devices, and extensive experimental results show the effectiveness and accuracy of EUIGR.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {298–310},
numpages = {13},
keywords = {RFID, adversarial learning, ongoing gesture recognition, wireless sensing},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3360026,
author = {Guida, Raffaele and Dave, Neil and Restuccia, Francesco and Demirors, Emrecan and Melodia, Tommaso},
title = {U-Verse: a miniaturized platform for end-to-end closed-loop implantable internet of medical things systems},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3360026},
doi = {10.1145/3356250.3360026},
abstract = {The promise of real-time detection and response to life-crippling diseases brought by the Implantable Internet of Medical Things (IIoMT) has recently spurred substantial advances in implantable technologies. Yet, existing devices do not provide at once the miniaturized end-to-end sensing-computation-communication-recharging capabilities to implement IIoMT applications. This paper fills the existing research gap by presenting U-Verse, the first FDA-compliant rechargeable IIoMT platform packing sensing, computation, communication, and recharging circuits into a penny-scale platform. U-Verse uses a single miniaturized transducer for data exchange and for wireless charging. To predict U-Verse's performance, we (i) derive and experimentally validate a mathematical model of U-Verse's charging efficiency; and (ii) experimentally calculate the resistance-reactance parameters of our ultrasonic transducer and rectifying circuit. We design a matching circuit to maximize the amount of power transferred from the outside. We also go through the challenge of fabricating a full-fledged cm-scale printed circuit board (PCB) for U-Verse. Extensive experimental evaluation indicates that U-Verse (i) is able to recharge a 330mF and 15F energy storage unit - several orders of magnitude higher than existing work - respectively under 20 and 60 minutes at a depth of 5cm; (ii) achieves stored charge duration of up to 610 and 40 hours in case of battery and supercapacitor energy storage, respectively. Finally, U-Verse is demonstrated through (i) a closed-loop application where a periodic sensing/actuation task sends data via ultrasounds through real porcine meat; and (ii) a real-time reconfigurable pacemaker.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {311–323},
numpages = {13},
keywords = {IoT, charging, medical, platform, ultrasound, wireless},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3360019,
author = {Li, Zhengxiong and Chen, Baicheng and Yang, Zhuolin and Li, Huining and Xu, Chenhan and Chen, Xingyu and Wang, Kun and Xu, Wenyao},
title = {FerroTag: a paper-based mmWave-scannable tagging infrastructure},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3360019},
doi = {10.1145/3356250.3360019},
abstract = {Inventory management is pivotal in the supply chain to supervise the non-capitalized products and stock items. Item counting, indexing and identification are the major jobs of inventory management. Currently, the most adopted inventory technologies in product counting/identification are using either the laser-scannable barcode or the radio-frequency identification (RFID). However, the laser-scannable barcode is entangled by an alignment issue (i.e., the laser reader must align with one barcode in line-of-sight), and the RFID is economically and environmentally unfriendly (i.e., high-cost and not naturally disposable). To this end, we propose FerroTag which is a paper-based mmWave-scannable tagging infrastructure for the next generation inventory management system, featuring ultra-low cost, environment-friendly, battery-free and in-situ (i.e., multiple tags can be simultaneously processed outside the line-of-sight). FerroTag is developed on top of the FerroRF effects. Specifically, the magnetic nanoparticles within the ferrofluidic ink reply to probing mmWave with classifiable features (i.e., the FerroRF response). By designating the ink pattern and hence the location of particles, the related FerroRF response can be modified. Thus, a specifically designated ferrofluidic ink printed pattern, which is associated with a unique FerroRF response, is a remotely retrievable (a.k.a., mmWave-scannable) identity. Furthermore, we augment FerroTag by designing a high capacity pattern system and a fine-grained identification protocol such that the capacity and robustness of FerroTag can be systematically improved in mass product management in inventory. Last but not least, we evaluate the performance of FerroTag with 201 different tag design patterns. Results show that FerroTag can identify tags with an accuracy of more than 99\% in a controlled lab setup. Moreover, we examine the reliability, robustness and performance of FerroTag under various real-world circumstances, where FerroTag maintains the accuracy over 97\%. Therefore, FerroTag is a promising tagging infrastructure for the applications in inventory management systems.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {324–337},
numpages = {14},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3360027,
author = {Xie, Binbin and Xiong, Jie and Chen, Xiaojiang and Chai, Eugene and Li, Liyao and Tang, Zhanyong and Fang, Dingyi},
title = {Tagtag: material sensing with commodity RFID},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3360027},
doi = {10.1145/3356250.3360027},
abstract = {Material sensing is an essential ingredient for many IoT applications. While hyperspectral camera, infrared, X-Ray, and Radar provide potential solutions for material identification, high cost is the major concern limiting their applications. In this paper, we explore the capability of employing RF signals for fine-grained material sensing with commodity RFID device. The key reason for our system to work is that the tag antenna's impedance is changed when it is close or attached to a target. The amount of impedance change is dependent on the target's material type, thus enabling us to utilize the impedance-related phase change available at commodity RFID devices for material sensing. Several key challenges are addressed before we turn the idea into a functional system: (i) the random tag-reader distance causes an additional unknown phase change on top of the phase change caused by the target material; (ii) the tag rotations cause phase shifts and (iii) for conductive liquid, there exists liquid reflection which interferes with the impedance-caused phase change. We address these challenges with novel solutions. Comprehensive experiments show high identification accuracies even for very similar materials such as Pepsi and Coke.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {338–350},
numpages = {13},
keywords = {RFID, antenna impedance, conductive liquid, material sensing, tag},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3360035,
author = {Xu, Huatao and Wang, Dong and Zhao, Run and Zhang, Qian},
title = {FaHo: deep learning enhanced holographic localization for RFID tags},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3360035},
doi = {10.1145/3356250.3360035},
abstract = {In recent years, radio frequency identification (RFID)-based approaches have been demonstrated to be a promising indoor localization techniques for many valuable applications, such as tracking tagged objects on the manufacturing lines, locating items in smart warehouses, and so on. In the near future, many applications will gain great benefits from knowing the positions of RFID-tagged objects. However, existing localization approaches often suffer from severe accuracy degradation in real-world environments due to the prevalent environmental interferences, such as the multipath effects. To this end, we designed an RFID-based localization system FaHo, which leverages a deep learning enhanced holographic technique for locating RFID tags accurately even in complex indoor environments. By carefully analyzing the features of the traditional holographic method, we created a new hologram-based algorithm called joint hologram, which yields a robust likelihood for each assumed position to be the true tag position. FaHo then adopts a deep convolutional neural network for analyzing the whole hologram, and subsequently estimate the true location of the RFID tag rather than simply seek for the largest-likelihood location. Furthermore, we implemented FaHo and evaluated its performance in several multipath-rich scenarios. The experimental results show that FaHo can achieve centimeter-level accuracy in both the lateral and radial directions using only one moving antenna. More importantly, our work also demonstrates that hologram-based localization is a highly effective technique for RFID indoor localization tasks.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {351–363},
numpages = {13},
keywords = {RFID, joint hologram, localization, synthetic aperture},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3360039,
author = {Alloulah, Mohammed and Radivojevic, Zoran and Mayrhofer, Ren\'{e} and Huang, Howard},
title = {KinPhy: a kinetic in-band channel for millimetre-wave networks},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3360039},
doi = {10.1145/3356250.3360039},
abstract = {We propose a system for enabling auxiliary communication channels in which a node transmits a millimeter (mm) wave signal which is reflected off a deliberately vibrating surface of a second node and then received by the first node. Data sequences can be encoded in the modulation of the surface, and radar sensing techniques can be used to demodulate the reflected signal. Hence our system enables not only conventional sensing in terms of range, velocity, and orientation estimation but also allows for information to be conveyed by the sensed device. We introduce the design of a metasurface driven by an energy-efficient programmable piezo-electric actuator, detail suitable radar processing, and characterize the link performance of the kinetically induced channel at distances up to five meters. As this metasurface could be used for both mobile devices and infrastructure devices, we describe opportunities for enabling novel capabilities including secure device authentication and extended-range wireless sensing across multiple devices.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {364–377},
numpages = {14},
keywords = {5G, millimetre-wave},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3361963,
author = {Ahn, Jae-wook and Grueneberg, Keith and Ko, Bong Jun and Lee, Wei-Han and Morales, Eduardo and Wang, Shiqiang and Wang, Xiping and Wood, David},
title = {Acoustic anomaly detection system: demo abstract},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3361963},
doi = {10.1145/3356250.3361963},
abstract = {Acoustic signals contain rich information of the environment. They can be used for detecting anomalous events such as in automated machine monitoring. In this demonstration, we present our acoustic anomaly detection system that captures acoustic signals and classifies them using machine learning techniques. Our system includes a server for sound management and model training, a mobile client for sound capturing and real-time classification, and a workbench that acts as a user interface. We will show the full operational pipeline of our system in this demonstration.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {378–379},
numpages = {2},
keywords = {acoustic signal analytics, anomaly detection, machine learning},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3361935,
author = {DeChicchis, Joseph and Ahn, Surin and Gorlatova, Maria},
title = {Adaptive AR visual output security using reinforcement learning trained policies: demo abstract},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3361935},
doi = {10.1145/3356250.3361935},
abstract = {Augmented reality (AR) technologies have seen significant improvement in recent years with several consumer and commercial solutions being developed. New security challenges arise as AR becomes increasingly ubiquitous. Previous work has proposed techniques for securing the output of AR devices and used reinforcement learning (RL) to train security policies which can be difficult to define manually. However, whether such systems and policies can be deployed on a physical AR device without degrading performance was left an open question. We develop a visual output security application using a RL trained policy and deploy it on a Magic Leap One head-mounted AR device. The demonstration illustrates that RL based visual output security systems are feasible.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {380–381},
numpages = {2},
keywords = {augmented reality, magic leap AR headset, policy optimization, reinforcement learning, visual output security},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3361934,
author = {Nasir, Nabeel and Campbell, Bradford},
title = {An architecture for edge computing over underutilized gateways: demo abstract},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3361934},
doi = {10.1145/3356250.3361934},
abstract = {Internet of Things applications typically run on the cloud and away from the end devices, leading to potential privacy and security risks, lower latency, and reduced reliability. Such deployments commonly use gateways to aggregate and send device data to the cloud. We hypothesize that these gateways are underutilized and can perform more than just packet forwarding. This work is an attempt to build a distributed platform over such gateways, with an objective to push applications to the edge of the network to overcome the shortcomings of the cloud. Our solution enables heterogeneous gateways to discover each other, and provides programming interfaces for developers to run applications on the platform without having to deal with the underlying network and device complexities. We showcase two applications that use our APIs, and also demonstrate how end users can interact with the platform via their personal computers (smartphones, laptops, etc.).},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {382–383},
numpages = {2},
keywords = {distributed applications, edge computing, internet of things},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3361951,
author = {Guo, Gabriel and Segal, Joshua and Zhang, Hanbin and Xu, Wenyao},
title = {ARMove: A smartphone augmented reality exergaming system for upper and lower extremities stroke rehabilitation: demo abstract},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3361951},
doi = {10.1145/3356250.3361951},
abstract = {Effective at-home rehabilitation of both upper and lower extremities is important for regaining proficiency in activities of daily living (ADLs) post-stroke. We introduce ARMove, a smartphone augmented reality (AR) exergaming system for upper and lower extremities stroke rehabilitation. The AR technology facilitates exergaming that utilizes full range of motion in real-world spatial environments, while creating interesting graphics to engage users in gamified environments. ARMove's novelty comes from its multifaceted rehabilitation of both upper and lower extremities. Furthermore, ARMove provides simultaneous training of fine and gross movements; it also considers bilateral training, preparing users for ADLs such as using computers or playing sports. Additionally, our utilization of smartphone embedded vision sensors and mobile computing give our system scalability, with potential for ubiquitous deployment.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {384–385},
numpages = {2},
keywords = {augmented reality, embedded sensor system, exergame, mobile computing, rehabilitation, smart health, stroke},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3361946,
author = {Varvello, Matteo and Katevas, Kleomenis and Hang, Wei and Plesa, Mihai and Haddadi, Hamed and Bustamante, Fabi\'{a}n E. and Livshits, Benjamin},
title = {BatteryLab, a distributed power monitoring platform for mobile devices: demo abstract},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3361946},
doi = {10.1145/3356250.3361946},
abstract = {There has been a growing interest in measuring and optimizing the power efficiency of mobile apps. Traditional power evaluations rely either on inaccurate software-based solutions or on ad-hoc testbeds composed of a power meter and a mobile device. This demonstration presents BatteryLab, our solution to share existing battery testing setups to build a distributed platform for battery measurements. Our vision is to transform independent battery testing setups into vantage points of a planetary-scale measurement platform offering heterogeneous devices and testing conditions. We demonstrate BatteryLab functionalities by investigating the energy efficiency of popular websites when loaded via both Android and iOS browsers. Our demonstration is also live at https://batterylab.dev/.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {386–387},
numpages = {2},
keywords = {distributed system, energy consumption, smartphones},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3361949,
author = {Comstock, Emery and Guo, Gabriel and Xu, Wenyao},
title = {BIGHand - A bilateral, integrated, and gamified handgrip stroke rehabilitation system for independent at-home exercise: demo abstract},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3361949},
doi = {10.1145/3356250.3361949},
abstract = {Effective home rehabilitation is important for recovery of hand grip ability in post-stroke individuals. This paper presents BIGHand, a bilateral, integrated, and gamified handgrip stroke rehabilitation system for independent at-home exercise. BIGHand consists of affordable sensor-integrated hardware (Vernier hand dynamometers, Arduino Uno, interface shield) used to obtain real-time grip force data, and a set of exergames designed as parts of an interactive structural rehabilitation program. This program pairs targeted difficulty progression with user-ability scaled controls to create an adaptive, challenging, and enticing rehabilitation environment. This training prepares users for the many activities of daily living (ADLs) by targeting strength, bilateral coordination, hand-eye coordination, speed, endurance, precision, and dynamic grip force adjustment. Multiple measures are taken to engage, motivate, and guide users through the at-home rehabilitation process, including "smart" post-game feedback and in-game goals. A demo video is available at https://youtu.be/zrLVkZZ4Ukc.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {388–389},
numpages = {2},
keywords = {exergame, rehabilitation, sensor system, smart health, stroke},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3361945,
author = {Geissdoerfer, Kai and Chwalisz, Miko\l{}aj and Zimmerling, Marco},
title = {Detailed recording and emulation of spatio-temporal energy environments with shepherd: demo abstract},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3361945},
doi = {10.1145/3356250.3361945},
abstract = {Collaboration of batteryless nodes is essential to their success in replacing traditional battery-based systems. This abstract describes a demonstration of the recently proposed Shepherd testbed that allows to record and reproduce spatio-temporal characteristics of real energy environments. It consists of a number of spatially distributed Shepherd nodes that are tightly time-synchronized with each other and record synchronized energy traces with a resolution of 3 μA and 50 μV at a rate of 100 kHz. Additionally, Shepherd can faithfully replay these traces to any number of nodes to study their behavior, both individually and as an ensemble. Shepherd works with various sources of energy harvesting, such as kinetic or solar, is based on a modular design and provides a generic interface for sensor nodes allowing users to experiment with new platforms.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {390–391},
numpages = {2},
keywords = {batteryless sensing, energy harvesting, intermittent networking, intermittent power, transient computing},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3361932,
author = {Chen, Baicheng and Li, Zhengxiong and Yang, Zhuolin and Li, Changzhi and Lin, Feng and Xu, Wenyao},
title = {E-Eye: mmWave nonlinear response for hidden electronic device recognition: demo abstract},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3361932},
doi = {10.1145/3356250.3361932},
abstract = {Hidden electronics possess the risk of both security threat and privacy intrusion. We present a wireless hidden electronic recognition system, through electronic components unique mmWave nonlinear responses to identify the threats. We then evaluate E-Eye's performance and robustness with a controlled experiment and a field study using iconic devices and score the system with metrics. Results prove that E-Eye is an accurate and robust hidden electronic recognition system.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {392–393},
numpages = {2},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3361944,
author = {Stojkovic, Jovan and Liu, Zida and Lan, Guohao and Joe-Wong, Carlee and Gorlatova, Maria},
title = {Edge-assisted collaborative image recognition for augmented reality: demo abstract},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3361944},
doi = {10.1145/3356250.3361944},
abstract = {Mobile Augmented Reality (AR), which overlays digital information with real-world scenes surrounding a user, provides an enhanced mode of interaction with the ambient world. Contextual AR applications rely on image recognition to identify objects in the view of the mobile device. In practice, due to image distortions and device resource constraints, achieving high performance image recognition for AR is challenging. Recent advances in edge computing offer opportunities for designing collaborative image recognition frameworks for AR. In this demonstration, we present CollabAR, an edge-assisted collaborative image recognition framework. CollabAR allows AR devices that are facing the same scene to collaborate on the recognition task. Demo participants develop an intuition for different image distortions and their impact on image recognition accuracy. We showcase how heterogeneous images taken by different users can be aggregated to improve recognition accuracy and provide a better user experience in AR.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {394–395},
numpages = {2},
keywords = {collaborative augmented reality, edge computing, image recognition},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3361959,
author = {Shukla, Rishi and Kiran, Neev and Wang, Rui and Gummeson, Jeremy and Lee, Sunghoon Ivan},
title = {Enabling battery-less wearable sensors via intra-body power transfer: demo abstract},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3361959},
doi = {10.1145/3356250.3361959},
abstract = {In this work, we present SkinnyPower, a concept of Intra-Body Power Transfer (IBPT) that wirelessly transfers power through human skin to operate batteryless wearable sensors. We envision a scenario, in which batteryless sensors placed on small body parts (e.g., on-finger, in-ear, and in-mouth) can obtain operating power from another body-worn energy sources (e.g., already existing battery-powered wearable devices such as a smartphone, smartwatch, or BandAid-like battery patch attached to the neck). To demonstrate and validate the concept of IBPT, we implemented a prototype consisting of 1) a wrist-worn, battery-powered power transmitter that sends alternating current through the human body and 2) a finger-worn, batteryless sensor device that operates solely on body-transferred power. Evaluations on five subjects show that we can reliably support the power of approximately 1 mW, which can be used to operate an embedded system, continuously collect sensor (accelerometer) data, and wirelessly transfer the collected data in real-time using Bluetooth Low Energy. We believe that the proposed system has great potential to transform current architectures and designs for body-area networks, promoting the development of innovative on-body sensors that would otherwise not be possible with on-device batteries.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {396–397},
numpages = {2},
keywords = {intra-body power transfer, power harvesting, wearable computers},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3361941,
author = {Puri, Chetanya and Dolui, Koustabh and Kooijman, Gerben and Masculo, Felipe and Van Sambeek, Shannon and Boer, Sebastiaan Den and Michiels, Sam and Hallez, Hans and Luca, Stijn and Vanrumste, Bart},
title = {Privacy preserving pregnancy weight gain management: demo abstract},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3361941},
doi = {10.1145/3356250.3361941},
abstract = {Early gestational weight gain prediction can help expecting women overcome several associated risks. However, training the model requires access to centrally stored privacy sensitive weight and other meta-data. In this demo, we present a privacy preserving federated learning approach where we train a global weight gain prediction model by aggregating client models trained locally on their personal data. We showcase a software data-exploration tool that exhibits local model generation, sharing and updating across users and server for proposed collaborative learning. Our proposed model predicts the final weight category with 61.3\% accuracy on day 140, with a 8.8\% compromise on the centralized training accuracy.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {398–399},
numpages = {2},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3361936,
author = {Ghosh, Anurag and Lingam, Vijay and Mehta, Ishit and Nambi, Akshay Uttama and Padmanabhan, Venkata N. and Sangameswaran, Satish},
title = {Smartphone-based driver license testing: demo abstract},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3361936},
doi = {10.1145/3356250.3361936},
abstract = {Road safety is compromised today by the inadequacies in driver license testing. Testing is typically still performed manually, and efforts aimed at automating testing are stymied by the cost of outfitting a testing track with sensors. We demonstrate a low-cost, smartphone-based system for automating key aspects of the driver license test. We have a pilot deployment of our system at an official testing track in India. We will present an analysis of license test results obtained from this pilot, comparing the smartphone-based testing results with manual evaluation.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {400–401},
numpages = {2},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3361962,
author = {Yao, Shuochao and Wang, Tianshi and Li, Jinyang and Abdelzaher, Tarek},
title = {Stardust: A deep learning serving system in IoT: demo abstract},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3361962},
doi = {10.1145/3356250.3361962},
abstract = {The deep neural network becomes an increasingly crucial component in recent intelligent applications. The excessive resource consumptions of state-of-the-art neural networks, however, remains a huge impediment towards their widespread deployment in the Internet of Things (IoT). In this paper, we propose an IoT-oriented deep learning serving system, Stardust, that accelerates the neural network inference to improve the quality of IoT services. Stardust integrates several joint contributions from both the system and AI perspectives, including system performance predictor, model compression, and compressive offloading. On one hand, the performance predictor profiles and predicts the runtime characteristics of neural network operations on a particular device with the targeted runtime environment, which enables a hardware and software oriented performance optimization during model compression and offloading. On the other hand, the model compression minimizes the computation time of neural networks on different devices, and the compressive offloading diminishes the network data transferring time during the mobile-edge offloading. Moreover, all these optimizations can be done with almost no compromise on inference accuracy. The integration of these modules, therefore, collaboratively reduce the end-to-end latency of serving deep learning services that reside across embedded/mobile devices and edge servers. We deploy illustrative applications on Stardust, performing human perception tasks with on-device camera microphone and motion sensors to demonstrate the capability of Stardust serving system.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {402–403},
numpages = {2},
keywords = {IoT, deep learning, model compression, offloading},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3361937,
author = {Park, Huiung and Kim, Haeyong and Kim, Seon-Tae and Mah, Pyeongsoo and Lim, Chaedeok},
title = {Two-phase dissemination scheme for CoAP-based firmware-over-the-air update of wireless sensor networks: demo abstract},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3361937},
doi = {10.1145/3356250.3361937},
abstract = {Firmware Over-the-Air (FOTA) update technology is required to keep the software of the massively deployed Wireless Sensor Network (WSN) devices up to date. However, when there are many nodes in a WSN and the cellular network connects the server and the WSN, the traditional method of transferring data directly from the server to the end node can be costly. We propose a Two-Phase FOTA Dissemination Scheme to address the communication cost problem by reducing traffic between WSN and external networks. This demonstration shows that the two-phase update reduces traffic between the external network and the WSN.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {404–405},
numpages = {2},
keywords = {constraint application protocol, firmware-over-the-air, internet of things, wireless sensor networks},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3361953,
author = {Popescu, Diana Andreea and Safronov, Vadim and Yadav, Poonam and Kolcun, Roman and Mandalari, Anna-Maria and Haddadi, Hamed and McAuley, Derek and Mortier, Richard},
title = {"Sensing" the IoT network: Ethical capture of domestic IoT network traffic: poster abstract},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3361953},
doi = {10.1145/3356250.3361953},
abstract = {As more and more devices are connected to the Internet-of-Things, often made by non-specialist companies or short-lived startups, the likelihood that these devices will be hacked and used for nefarious activity online increases. We seek to support non-expert users in managing the network behaviour of their IoT devices, and assisting them in handling the cases where those devices are hacked. To do so, we wish to enable anomaly detection at the network level, determining when a device starts behaving unusually. This requires capturing data about how devices behave in a diverse range of real deployments, not just lab environments.To that end, we present IoTCrowdsourcery, a toolset for capturing traffic data from real-world IoT deployments. Participants collect packet traces from their IoT devices through our software, and provide them via a crowdsourcing infrastructure. The key challenges to overcome are to make the process straightforward enough for non-expert participants to carry out, and to ensure that legal (notably GDPR) and ethical issues are carefully handled by ensuring that participants understand what they are doing, and are provided with various means to exercise agency in participating, and ultimately to withdraw their participation if they wish. We envisage the captured traces being analysed to develop behavioural models of IoT devices which will be used for anomaly detection, improving the security of our smart homes and more generally of the Internet.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {406–407},
numpages = {2},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3361961,
author = {Qin, Zhou and Xian, Yikun and Zhang, Desheng},
title = {A neural networks based caching scheme for mobile edge networks: poster abstract},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3361961},
doi = {10.1145/3356250.3361961},
abstract = {Mobile edge networks are pervasive now due to the ubiquitous 4G networks and coming 5G networks, broad edge computing applications are enabled in the meantime, such as mobile bus WiFi. In this paper, we focus on the caching problem in the mobile edge networks and use bus WiFi as an example to further investigate. Mobile bus WiFi is a newly emerged service in modern cities, which provides convenience for citizens and gains certain benefits for operators via commercial advertisements and other services. While the vital challenge for the bus WiFi industry is the high cost of cellular traffic considering the massive number of users and longtime running hours of the bus system. To tackle this, we investigate the caching problem in a nation-scale bus WiFi network deployed in 22 cities of China with 34,377 WiFi devices. We then delve a fundamental question, i.e., how can historical visiting records help us predict future visiting events and further save the cellular traffic by caching. In detail, we propose a Deep Neural Networks (DNN) based method by considering bus WiFi users' historical visits to cached contents to save cellular traffic data for WiFi providers. We implement our method via the city-scale bus WiFi data and compare with a series of state-of-the-art models, the results show that our method achieves the best performance.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {408–409},
numpages = {2},
keywords = {bus wifi, cache, deep neural networks, user behavior},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3361948,
author = {Amarasinghe, Akarshani and Wijesuriya, Viraj B. and Ganepola, Dilshan and Jayaratne, Lakshman},
title = {A swarm of crop spraying drones solution for optimising safe pesticide usage in arable lands: poster abstract},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3361948},
doi = {10.1145/3356250.3361948},
abstract = {Pesticides are detrimental to the well-being of all living beings. Inappropriate pesticide usage has been identified as a major health hazard. Therefore, it is important to devise solutions that enable agricultural pest control without excessive use of pesticides. At present, drone technology is being increasingly applied in agriculture through soil and field analysis, planting, crop spraying, crop monitoring, irrigation and health assessment. The primary focus of this ongoing work is to leverage a swarm of drones solution that includes precision agriculture techniques, to efficiently spray pesticides in arable lands with optimal pesticides usage and minimum human intervention.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {410–411},
numpages = {2},
keywords = {drone systems, swarm of drones},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3361942,
author = {Li, Mingyang and Wang, Hanling and Zhang, Yue and Huang, Shao-Lun and Zhang, Lin},
title = {Anomaly detection in surface mount technology process using multi-modal data: poster abstract},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3361942},
doi = {10.1145/3356250.3361942},
abstract = {Anomaly detection is an important area for both research and real-world applications. In the surface mounting technology (SMT) process, the defectives of solder paste printing need to be detected immediately or it may cause great effort for recycling and slow down the whole process. In this paper, we propose a novel model, MM-DNN, for anomaly detection with multi-modal data. We collect a multi-modal dataset from different sensors in the factory. Our method efficiently extracts both predictive features for classification and correlative features between multi-modal data to achieve a higher detection rate. As shown in the experiment, our method can further reduce 77\% false alarm rate of the detection result in the factory while keeping 95\% of real defectives be correctly detected.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {412–413},
numpages = {2},
keywords = {anomaly detection, machine learning, multi-modal fusion},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3361965,
author = {Kim, Taeyoung and Lee, Seung-seob and Kim, Chang Kyung and Lee, SuKyoung},
title = {Caching scheme for internet of vehicles using parked vehicles: poster abstract},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3361965},
doi = {10.1145/3356250.3361965},
abstract = {Since Internet of Vehicles (IoV) generate and consume huge amount of data, caching becomes indispensable technique to provide better Internet services in IoV. However, deployment and operation of infrastructure to cache the massive vehicular data is very costly. To tackle this problem, we propose a vehicular caching scheme that reduces data delivery delay and cost using parked vehicles.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {414–415},
numpages = {2},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3361966,
author = {Zhong, Shuxin and Zhang, Desheng},
title = {Conflict detection for smart cities services: poster abstract},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3361966},
doi = {10.1145/3356250.3361966},
abstract = {The effort of trying to integrate individual services leads to the increasing potential conflicts across smart city services. This kind of conflicts might cause unsafe environment or degrade the overall performance. In this paper, we mainly focus on conflict detection by exploring the fundamental correlations among services and intend to monitor the city's safety and performance collectively. The key insight is that there is a multi-dimensional dependency in predicted conflicts. Therefore, we propose a novel multitask learning framework to detect various conflicts simultaneously. We evaluate this service conflict detection model with a large-scale real dataset across a few transportation and environment services, including the location and transaction information from the taxi, bus and subway systems. The results show the advantage of our system.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {416–417},
numpages = {2},
keywords = {conflict detection, multi-task learning, smart city services},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3361938,
author = {Liu, Ning and Wang, Yue and Huang, Jiayi and Ma, Rui and Zhang, Lin},
title = {Enhanced air quality inference with mobile sensing attention mechanism: poster abstract},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3361938},
doi = {10.1145/3356250.3361938},
abstract = {Mobile sensor networks are widely deployed for air quality monitoring. However, fine-grained pollution inference based on these systems is challenging. Specifically, diverse geospatial attributes in urban areas bring great spatial variations of the pollution field. Besides, the preprocessing on raw samples, such as discretization and averaging, leads to the lost of fine-grained information of mobile sensing. In this paper, we propose an inference algorithm with the attention mechanism to better capture high-frequency information in the pollution field. Furthermore, we introduce the sensing gradients in the attention network to utilize the high-granularity information from the mobile sensors. Evaluations on real-world dataset show that our model outperforms the state-of-the-art method by 13.15\% ~ 27.04\%.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {418–419},
numpages = {2},
keywords = {air quality inference, attention mechanism, mobile sensor network},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3361955,
author = {Fang, Zhihan and Zhang, Fan and Zhang, Desheng},
title = {Fine-grained travel time sensing in heterogeneous mobile networks: poster abstract},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3361955},
doi = {10.1145/3356250.3361955},
abstract = {Sensing and estimating real-time passenger travel time in urban mobile networks provides essential information for passengers to select different transportation modalities, improving their travel experience. Most existing work on travel time sensing has been focused on individual transportation modalities and its riding time based on the static time tables. However, passengers often consider different modes of transportation, e.g. taxis, subways, buses or personal vehicles, and a significant portion of the travel time is spent in the uncertain waiting or walking, which cannot be accurately obtained by static time tables. In this paper, we design a real-time data-driven framework FineTravel for fine-grained travel time sensing based on multi-source real-time data from multi-modal mobile networks including taxi, bus, subway, and private vehicle. The key challenge we address in FineTravel is to estimate implicit components (including walking, waiting and riding time) of the total travel time without direct measurement. In contrast to the existing work based on single-modal transportation networks mostly with offline data, the novelty of the FineTravel is based on its real-time multi-source data-driven (including both vehicle GPS and smartcard data) modeling for a comprehensive travel time sensing.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {420–421},
numpages = {2},
keywords = {fine grained, heterogeneous mobile networks, travel time sensing},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3361968,
author = {Jia, Hong and Wu, Yuezhong and Liu, Jun and Yao, Lina and Hu, Wen},
title = {Mobile golf swing tracking using deep learning with data fusion: poster abstract},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3361968},
doi = {10.1145/3356250.3361968},
abstract = {Swing tracking is one of the key information for many sports such as golf. One approach to track swing is to use IMU to measure linear acceleration then get position by two-time integration. However, the complex noise model of the IMU limit the accuracy of the tracking. Another approach is to use depth sensor to measure 3D location of a point of interest directly. Unfortunately, the depth sensor-based approach cannot accurately measure the trajectory of a swing when the sensor is occluded, which happens regularly. To overcome these limitations, we develop a novel solution to make use of these two sensor modalities (i.e., IMU and depth sensor) by a novel deep neural network to produce high precision swing trajectory tracking. The learned network automatically makes use of the IMU when the depth sensor is occluded, and relies on depth sensor when IMU signal is noisy. Our experiment shows that the proposed method outperforms state-of-the-art swing tracking method by 62\% of error reduction.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {422–423},
numpages = {2},
keywords = {mobile computing, neural networks, sports analytics, swing tracking},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3361933,
author = {Aldeer, Murtadha and Florentine, Joseph and Kolodziejski, Jakub and Ortiz, Jorge and Howard, Richard E. and Martin, Richard P.},
title = {Patient identification using a smart pill-bottle: poster abstract},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3361933},
doi = {10.1145/3356250.3361933},
abstract = {In this work, we investigate the identification of persons taking medication using a sensor-equipped pill-bottle. The bottle embeds inertial sensors in both the cap and body, making the added hardware un-obtrusive, low-cost, and wireless. Our system uses inertial data to build a patient discrimination model using classification techniques. We evaluated the system using 16 subjects. Our results show that using binary Support Vector Machine (SVM), the system can discriminate one patient among 16 subjects with 94 \% accuracy. Identifying the exact person in a set of 3 subjects has an accuracy higher than 91 \%..},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {424–425},
numpages = {2},
keywords = {DTW, SVM, clustering, smart pill bottle, user discrimination},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3361947,
author = {Aloufi, Ranya and Haddadi, Hamed and Boyle, David},
title = {Privacy preserving speech analysis using emotion filtering at the edge: poster abstract},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3361947},
doi = {10.1145/3356250.3361947},
abstract = {Voice controlled devices and services are commonplace in consumer IoT. Cloud-based analysis services extract information from voice input using speech recognition techniques. Services providers can build detailed profiles of users' demographics, preferences and emotional states, etc., and may therefore significantly compromise privacy. To address this problem, a privacy-preserving intermediate layer between users and cloud services is proposed to sanitize voice input directly at edge devices by generating neutralized signals for forwarding. We show that a trained model, based on CycleGAN and deployed on a Raspberry Pi, enables identification and removal of sensitive emotional state information by ~91\%, with minimal losses to speech recognition accuracy.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {426–427},
numpages = {2},
keywords = {internet of things (IoT), machine learning, speech analysis, voice privacy, voice synthesis},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3361957,
author = {Zhang, Fan and Zhang, Desheng},
title = {Privacy-aware synthesis of sensing data based on learning model at metropolitan scale: poster abstract},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3361957},
doi = {10.1145/3356250.3361957},
abstract = {With development of ubiquitous urban sensory infrastructures, large scale human mobility data have various applications and benefits in mobile networking and urban planning. Demands for sharing these sensing data to research communities have proliferated causing concerns over privacy. Traditional privacy solution is to apply comprehensive perturbation at feature level, which may results in significant utility loss.In this paper, we propose a learning-based generative model with designed classifiers to generate realistic synthetic mobility data with privacy protection at label/system property level. With two plug-in classifiers, i.e., privacy classifier and utility classifier, the trade-off between sensitive and non-sensitive mobility properties would be balanced with customization. To validate the performance, we implement our model based on the urban-scale cellphone data from the Shenzhen city as a case study. The resilience of smart attacks is tested and potential applications are pictured for this semi-supervised learning-based model.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {428–429},
numpages = {2},
keywords = {data synthesis, human mobility, privacy},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3361954,
author = {Kamboj, Priyanka and Pal, Sujata},
title = {QoS in software defined IoT network using blockchain based smart contract: poster abstract},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3361954},
doi = {10.1145/3356250.3361954},
abstract = {Internet of things (IoT) has made human life simpler through its numerous applications by interconnecting various devices and people together in a heterogeneous network. The number of users associated with IoT in different sectors such as media, intelligent transportation, and healthcare are increasing rapidly, therefore generating a large amount of data each day. Because of this, maintaining quality in the network has become an important aspect for end-to-end data delivery. Software defined networking thus provides flexibility and programmability to the network due to its global approach of network management. To adapt with the dynamic nature of QoS, blockchain technique with its encryption mechanism and distributed consensus algorithm can be deployed on SDN. In this paper, we propose the use of blockchain technology in Software-Defined Internet of Things to deliver high QoS at a low price.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {430–431},
numpages = {2},
keywords = {blockchain, internet of things, quality of service, software defined networking},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3361967,
author = {Asgarian, Farzad and Najafi, Khalil},
title = {Reducing synchronization error in wireless sensor nodes by using previous timing information as training data: poster abstract},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3361967},
doi = {10.1145/3356250.3361967},
abstract = {Many wireless sensor networks benefit from one type of time synchronization protocols. However, due to different crystal oscillators and clock sources in the nodes, even with drift-management techniques, synchronization error starts increasing fast after a few to tens of seconds. To keep the error low in minute-long sessions, typically the master transmits packets periodically to reset the error. These extra packets increase power consumption of all nodes especially in long sessions. Here, we propose AdaptSync that uses the timing/error information of a short period of time at the beginning of a session to reduce the error in the rest of the session independent of the master. Implementation steps are discussed and tested on sensor nodes that use Bluetooth as the wireless link. Experimental results show that in 10-minute long sessions, average synchronization error is reduced 7x compared to normal synchronization.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {432–433},
numpages = {2},
keywords = {adaptive, beacon, bluetooth, sensor networks, synchronization},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3361956,
author = {Farooq, Muhammad Omer and Kunz, Thomas},
title = {Revealing insights for improvements in LoRaWAN in multiple applications scenarios: poster abstract},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3361956},
doi = {10.1145/3356250.3361956},
abstract = {We study LoRaWAN's performance when multiple applications are concurrently running over the same LoRaWAN network. We consider applications that generate data packets using a Poisson process, a random distribution, and at periodic intervals. The LoRa PHY layer supports a number of communication settings. However, here we focus on two specific settings: the setting recommended by LoRaWAN and the setting that yields the highest possible data rate in LoRa. Our results demonstrate the following: (i) LoRaWAN favours applications that generate packets at a higher periodic rate, (ii) LoRAWAN does not favour applications that generate packets at a higher rate under Poisson and uniform random distribution, (iii) LoRaWAN's recommended PHY setting demonstrates poor performance, (iv) LoRa's fastest data rate setting outperforms the LoRaWAN recommended setting, and (v) LoRaWAN favours applications that generate packets using uniform random and Poisson distributions over application that generates packet at periodic interval. Our results also hint that using multi-hop communication along with the LoRa's fastest data rate setting can not only increase the setting's coverage, but it may still deliver better performance relative to the LoRaWAN's recommended setting.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {434–435},
numpages = {2},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3361939,
author = {Singh, Shirish and Shila, Devu Manikantan and Kaiser, Gail},
title = {Side channel attack on smartphone sensors to infer gender of the user: poster abstract},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3361939},
doi = {10.1145/3356250.3361939},
abstract = {Smartphones incorporate a plethora of diverse and powerful sensors that enhance user experience. Two such sensors are accelerometer and gyroscope, which measure acceleration in all three spatial dimensions and rotation along the three axes of the smartphone, respectively. These sensors are used primarily for screen rotations and advanced gaming applications. However, they can also be employed to gather information about the user's activity and phone positions. In this work, we investigate using accelerometer and gyroscope as a side-channel to learn highly sensitive information, such as the user's gender. We present an unobtrusive technique to determine the gender of a user by mining data from the smartphone sensors, which do not require explicit permissions from the user. A preliminary study conducted on 18 participants shows that we can detect the user's gender with an accuracy of 80\%.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {436–437},
numpages = {2},
keywords = {gender detection, privacy, sensors, smartphone},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3361943,
author = {Agarwal, Dhruv and Iyengar, Srinivasan and Swaminathan, Manohar},
title = {System for vehicle selection in drive-by sensing: poster abstract},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3361943},
doi = {10.1145/3356250.3361943},
abstract = {Drive-by sensing has emerged as a popular way to achieve fine-grained sensing of physical phenomena. However, for it to be effective at a city-scale, there is a need to optimally select a subset of vehicles from a larger available fleet. These chosen vehicles must maximize coverage of the entire city. Simultaneously, they must fulfill other deployment requirements specific to the sensing application such as reference-monitor colocation instances for gas sensors. In this paper, we describe a system to evaluate the coverage offered by different subsets of vehicles for sensor deployment based on historical vehicle mobility data. Our system allows evaluation of different vehicle selection algorithms, and also provides two in-built baselines --- i) Random-MP, and ii) MaxPoints --- for comparison. Finally, we provide visualizations showing coverage to gauge the efficacy of different vehicle selections.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {438–439},
numpages = {2},
keywords = {drive-by sensing, low-cost sensing, sensor deployment},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3361950,
author = {Wang, Guang and Zhang, Fan and Zhang, Desheng},
title = {tCharge - A fleet-oriented real-time charging scheduling system for electric taxi fleets: poster abstract},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3361950},
doi = {10.1145/3356250.3361950},
abstract = {With more and more advanced sensing and communication devices deployed on vehicles, ubiquitous vehicle location data (e.g., GPS) are available, which provide us great opportunities to enhance their mobility and energy performance (e.g., better scheduling decisions for operation or charging of taxis). In this paper, we design a real-time charging scheduling system called tCharge to address the charging problem of electric taxis with a fleet-oriented fashion. We leverage historical GPS data to estimate the travel time of electric taxis to different charging stations, and we utilize real-time GPS data of electric taxis to infer their waiting times at different charging stations. Then all these obtained information is fed to an online optimization for a fleet-oriented scheduling decision. We implement tCharge with real-world data from the Chinese city Shenzhen, including GPS data, taxi transaction data, road network data, and charging station data from more than 1,000 electric taxis and 28 charging stations. The results show our tCharge outperforms existing methods by 82\% of the queuing time reduction.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {440–441},
numpages = {2},
keywords = {charging scheduling, electric taxi, electric vehicle, fleet-oriented},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3361931,
author = {Chen, Kai and Chen, Lili and Han, Dianhe and Xiong, Jie and Lee, Sunghoon Ivan and Chen, Xiaojiang and Tang, Zhanyong and Fang, Dingyi and Wang, Fuwei and Wang, Zheng},
title = {Towards wide-area contactless human sensing: poster abstract},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3361931},
doi = {10.1145/3356250.3361931},
abstract = {Contactless wireless sensing without attaching a device to the target has achieved promising progress in recent years. However, one severe limitation in this field is the limited sensing range. This paper presents WideSee to realize wide-area sensing with only one transceiver pair. WideSee utilizes the LoRa signal to achieve a larger range of sensing and further incorporates drone's mobility to broaden the sensing area. We have developed a working prototype of WideSee for human target detection and localization that are especially useful in emergency scenarios like rescue and terrorist search. We also evaluated WideSee with field study in a high-rise building, which demonstrates the great potential of WideSee for supporting wide-area contactless sensing applications with a single LoRa transceiver pair hosted on a drone.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {442–443},
numpages = {2},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3361940,
author = {Altherwy, Youssef N and Elmallah, Ehab S. and McCann, Julie A.},
title = {Two-terminal connectivity in UWSN probabilistic graphs: A polynomial time algorithm: poster abstract},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3361940},
doi = {10.1145/3356250.3361940},
abstract = {We investigate the likelihood that two nodes are connected in an Underwater Wireless Sensor Network (UWSN) where nodes are floating freely with the underwater currents and the location of nodes at any given time can only be determined in a probabilistic fashion. This problem is #P-hard, thus, we propose HB-Conn2, an algorithm that returns an exact solution in polynomial time when applied on a set of node-disjoint (s, t)-paths.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {444–445},
numpages = {2},
keywords = {probabilistic graphs, two terminals connectivity, underwater sensor networks},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3361960,
author = {Vasconcelos, F\'{a}bio and Aguiar, Vitor and Pereira, Lucas},
title = {Ultrasonic waste monitoring in the future industrial kitchen: poster abstract},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3361960},
doi = {10.1145/3356250.3361960},
abstract = {This paper presents the initial results of ultrasonic waste monitoring during the operation of one industrial kitchen. Undifferentiated, paper and plastic waste bins were monitored, and a heuristic-based waste disposal event detector was developed and evaluated. The results show that it is possible to identify disposal events in the three waste bins. Furthermore, it is also possible to determine when paper and plastic are compressed to make additional room.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {446–447},
numpages = {2},
keywords = {industrial kitchen, sustainability, ultrasonic, waste monitoring},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3361952,
author = {Huang, Jiayi and Liu, Ning and Ma, Rui and Liu, Xinyu and Wang, Yue and Zhang, Lin},
title = {Understanding air pollution patterns in city based on minute-level event detection: poster abstract},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3361952},
doi = {10.1145/3356250.3361952},
abstract = {Air pollution is a serious urban problem that threatens human health. Therefore, fine-grained pollution events detection has become a concerned issue for environmental management. Algorithms in previous studies identify pollution events as uptrend intervals at hour level. However, a significant part of pollution events caused by traffic and industry can be brief but frequent, which may be neglected under traditional coarse-grained detection. In this paper, we propose a fine-grained analysis of air pollution pattern based on minute-level event detection. Over the real-world deployment in Foshan, these events are analyzed according to their geographical contexts and temporal features. Results show insightful findings and this case study provides a practical reference for government inspection and pollution control.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {448–449},
numpages = {2},
keywords = {air pollution pattern, event detection, government inspection},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3361958,
author = {Xie, Xiaoyang and Zhang, Fan and Zhang, Desheng},
title = {Understanding real-time interaction in heterogeneous vehicular sensing: poster abstract},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3361958},
doi = {10.1145/3356250.3361958},
abstract = {With the development of connected vehicles and autonomous vehicles, various innovative sensing services are proposed to improve the driving experience based on the vehicle to vehicle (V2V) communication. However, due to the heterogeneous mobility pattern of different vehicular fleets and the dynamic nature of the vehicles, it is hard to systematically measure the feasibility of those services in large scale before implementation. In this work, we design a measurement framework called mChat to measure the interactions among heterogeneous vehicle fleets, which is quantified by the topology of the urban vehicular fleets. mChat utilizes two key metrics, i.e., static connectivity feature and dynamic coexistence feature, to explore the feasibility of real-time vehicle to vehicle service on inner-fleet and inter-fleets. We apply mChat on one month of real-world data of a Chinese city, Shenzhen, from (i) a regular vehicle fleet with 3 thousand vehicles; (ii) a taxi fleet with 14 thousand vehicles; (iii) a bus fleet with 11 thousand vehicles; (iv) a truck fleet with 3 thousand vehicles. Our measurement results reveal interesting results related to the feasibility of V2V-based sensing service, leading to utility implications for the upcoming applications for connected vehicles and autonomous vehicles.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {450–451},
numpages = {2},
keywords = {vehicle to vehicle communication, vehicular sensing},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3361930,
author = {Fletcher, Jude and Wallom, David},
title = {Using machine learning to orchestrate cloud resources in a RAN enabled edge environment: poster abstract},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3361930},
doi = {10.1145/3356250.3361930},
abstract = {With the deployments of fifth generation mobile networks (5G), rapid development of mobile internet, continued growth in mobile traffic and increased adoption of the internet of things, Multi-access Edge Computing (MEC) remains an inevitable critical future radio access network artefact for an optimized network. Network operators as part of their optimization exercise are also adopting Cloud Radio Access Network (C-RAN) to further improve network operation performance. Combining MEC and C-RAN makes the economics (OPEX + CAPEX) of both technologies more attractive and also enables network operators to support key 5G applications. This research investigates a major challenge of combining MEC + C-RAN: how to efficiently orchestrate cloud resources as a user moves within the edge environment without compromising the overall quality of service. This ongoing research adopts a machine learning technique in orchestrating resources efficiently in order for applications and services to adhere to stringent performance requirements even at geographically dispersed "edge" locations.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {452–453},
numpages = {2},
keywords = {5G, bayesian networks, cloud radio access network (C-RAN), deep learning, energy reduction, machine learning, multi-access edge computing, resource orchestration},
location = {New York, New York},
series = {SenSys '19}
}
@inproceedings{10.1145/3356250.3361964,
author = {Yang, Yu and Zhang, Fan and Zhang, Desheng},
title = {Vehicular mobility modeling based on heterogeneous sensor networks: poster abstract},
year = {2019},
isbn = {9781450369503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356250.3361964},
doi = {10.1145/3356250.3361964},
abstract = {Urban mobility especially vehicular mobility is important for many real-world applications. A lot of work has been done based on data from mobile sensor networks and stationary sensor networks, where each of them has its only weakness in spatiotemporal coverage and penetration rates. In this paper, we aim to coordinate the strength of heterogeneous sensor networks to achieve mobility modeling. Specifically, we design a mobility prediction system called Mohen to predict the destination of all the vehicles after they leave the coverage of stationary sensors with (i) only small portion vehicles with mobile sensors (i.e., GPS devices); (ii) historical data from stationary sensors (i.e., toll stations). The key novelty of Mohen is that we utilize the complementary features from heterogeneous sensor networks that mobile sensor networks provide better spatiotemporal coverage whereas stationary sensor networks capture vehicles with high penetration rates. We implement and evaluate Mohen in Guangdong Province, China by two real-world datasets, an electric toll collection (ETC) data set of 2 million vehicles, and an insurance-based vehicular tracking system of 114 thousand vehicles and show the accuracy of 75\%.},
booktitle = {Proceedings of the 17th Conference on Embedded Networked Sensor Systems},
pages = {454–455},
numpages = {2},
keywords = {heterogeneous sensor networks, vehicular mobility},
location = {New York, New York},
series = {SenSys '19}
}